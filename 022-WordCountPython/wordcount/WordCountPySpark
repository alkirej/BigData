from pyspark import SparkContext
from pyspark.sql import SparkSession,DataFrame
from pyspark.rdd import RDD

# CONSTANTS
INPUT_FILE_NAME: str = "/home/jeff/BigData/004-WordCount/shakespeare-hamlet.txt"
OUTPUT_FILE_NAME: str = "/home/jeff/BigData/output/wc_results_scala.csv"
CSV_GENERATION_CLASS: str = "com.databricks.spark.csv"
OUTPUT_DIR_NAME: str = "hdfs://localhost:50700/output/wordcount_pyspark"

def read_and_clean(sc: SparkContext, fileName: str) -> RDD:
    return sc.textFile(INPUT_FILE_NAME) \
                .map(lambda line: line.lower()) \
                .flatMap(lambda w: w.split("\\W+")) \
                .filter(lambda w: w != "")

def open_spark_session(sc:SparkContext) -> SparkSession:
    return SparkSession(sc)   \
        .builder \
        .appName("WordCountPySpark") \
        .master("local[*]") \
        .getOrCreate()

def sum( x: int, y: int ):
    return x+y

if __name__ == "__main__":
    sc: SparkContext = SparkContext()
    spark: SparkSession = open_spark_session(sc)
#    sc: SparkContext = spark.sparkContext()

    lines: RDD = read_and_clean(sc, INPUT_FILE_NAME)
    results: RDD = lines.map(lambda w: (w, 1)).reduceByKey(lambda x,y: x+y).sortByKey()

    df: DataFrame = spark.createDataFrame(results)
    df.write.format(CSV_GENERATION_CLASS).save(OUTPUT_DIR_NAME)

