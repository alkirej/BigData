
RDBMS
Successfully support 99.99% database availability uptime within different LoBs of financial domain.
Manage and support for multiple large databases sized in multi terabits.
Cloud professional in Amazon Web Service(AWS) and Microsoft Azure infrastructure solutions.
Cloud Data Center Solutions
Cleaned and made necessary changes to data as part of data transformation.
Work with multiple customer teams and support teams to execute Hadoop engagements.
Defined procedures to simplify future upgrades.  Standardized all MySQL installs on all servers with custom configurations.
RDD, Spark Streaming, Pair RDD Operations, Check-pointing, and SBT.
MACHINE LEARNING
Austin, TX
Experience in Big Data security using Kerberos, LDAP, Active Directory, Apache Ranger, Apache KMS, SSL, and Apache Knox Gateway.
Installed and maintained the client’s first Elasticsearch (ELK) cluster on AWS hosting about 4TB of indexes used primarily to ingest log data with information dashboards created with Kibana.
Worked with mining tools such as RapidMiner and Orange to unlock trends and patterns and derive insights in complex datasets.
Cluster Monitoring
Used Spark Streaming API with Kafka to build live dashboards; Worked on Transformations & actions in
Chicago Transit Authority
Responsible for MySQL installations, upgrades, performance tuning, etc.
Documented and provided status, updates and technical information to project manager.
Handling Hive queries using Spark SQL that integrate with Spark environment implemented in Scala.
ETL Data Cleansing, Integration & Transformation using Pig: Managing data from disparate sources.
Hadoop clusters design, installation, configuration and monitoring, troubleshooting, security, backup, re-sizing, addition/deletion of nodes, performance monitoring, HDFS balancing and fine-tuning for MapReduce applications.
AWS Redshift, Data Pipeline, SQL, Loading, Compression/Encoding
Created Design documents, Architectural Documents and Technical documents for POC
Performed security audit of MySQL internal tables and user access.  Revoked access for unauthorized users.
September 2016 to Present
AremuKayode01@gmail.com 917-677-5790
Ambari v2.5
Developed PIG Latin scripts to extract the data from the web server output files to load into HDFS
Primarily responsible for architecting and administering the HDFS distributed file systems for over 15 petabytes of data.
Loaded data into the cluster from dynamically generated files using FLUME and from RDBMS using Sqoop
for benchmark testing and general functional comparison.
Good working knowledge of open source configuration management and deployment tools such as Puppet and Python.
Cloudera Hadoop (CDH) 4.1.1 – 5.13.0
Involved in complete Big Data flow of the application starting from data ingestion upstream to HDFS, processing the data in HDFS and analyzing the data and involved Low level design for MR, Hive, Impala, Shell scripts to process data.
Use of Puppet for node configuration, and cluster management.
Analyzing, profiling data for quality and reconciling data issues using SQL.
Search
Created and implemented database standards and procedures for management.
March 2014 to March 2015
Utilize new and latest Open Source tools for addressing business challenges.
HortonWorks HDP 2.6 & HDF 3.0
Performed performance tuning and troubleshooting by analyzing and reviewing Hadoop log files.
Expert in MapReduce counters tuning for faster and optimal data processing.
Google DataProc
Expert in Informatica for data analytics, data integration and management.
Importing the data from the MySql and Oracle into the HDFS using Sqoop
Use of Apache Oozie to schedule workflows for streamlined processes with efficiency and automation.
Optimized and Integrated Hive, SQOOP and Flume into existing ETL processes, accelerating the extraction, transformation, and loading of massive structured and unstructured data.
	
Built and supported several AWS, multi-server environment's using Amazon EC2, EBS and Redshift
Programming Languages
Involved in designing, installations and maintenance of KAFKA and Talend.
Rolled out new staging tier by repurposing existing hardware, integrating with Puppet and following the development team software and configuration specifications.
New York, NY
Big Data Tools
Kayode Hammed
Apache Nifi, Apache Camel, Flume, Apache Kafka,Talend, KETL, Pentaho, 
March 2012 to March 2014
Worked on Sequence and ORC files, bucketing, partitioning for Hive performance and storage improvement
March 2015 to August 2016
Certified expert in Hortonworks Hadoop Platform architecture, engineering and administration.
Apache Lucene, Elasticsearch, Elastic Cloud, Kibana, Apache SOLR
Performed upgrades, patches and fixes using either rolling or express method.
Security
Strong experience on Hadoop system administration (Hortonworks/Ambari) and Linux system administration (RHEL 7, Centos.)
Paraccel, Elasticsearch, Cassandra, Aurora
Mentored and worked with developers and analysts to review scripts and better querying.
Used Oozie scheduler to automate the pipeline workflow and orchestrate the map reduce jobs that Extract the data on a timely manner
Setup replication for disaster and point-in-time recovery.  Replication was used to segregate various types of queries and simplify backup procedures.
Directly supporting and managing clustered VMware ESXi \/5 with vCenter.
Strong experience in JVM, Hive, MapReduce, Operating Systems performance tuning.
Architected ETL pipelines on AWS cloud using Spark EMR.
Experience in working with Relational databases and design database schemas using SQL.
JR. DATA ANALYST
Used different mathematical and computational algorithms such as Neural network, k-Means, Association rules, Naïve Bayes to unlock various insights and make future forecasts.
AWS EC2 & EBS, S3
NoSQL Data Store
Hadoop Administrator
Kerberized the cluster for user authentication.
Installed and administered client’s first Hadoop cluster utilizing the Cloudera distribution.
9
Architecture
Provided Big Data and general distributed system technology, design/development and expertise. This was an AWS cloud implementation with heavy duty Redshift using the technologies below.
Hadoop (HDFS, YARN, MRv2), Flume, Sqoop, Zookeeper, Pig, Hive, HBase, Oozie, Apache NiFi, Kafka, Storm, Knox, Kerberos, Apache Ranger
MySQL Database Administrator
Lautech Group Technology
Requirements Gathering, Documentation, Data Modeling, Data System Design, Pipeline and ETL Architecture, Search Architecture, Dashboard Architecture, Toad Data Modeler, Lambda Functions, Data modelling, Data structures, algorithms, and problem solving
                          HDP Certified Administrator Certificate
Highly skilled in troubleshooting and finding root causes.
Designed and configured MySQL server cluster, and managing each node on the Cluster.
Worked on DB2 to extract data and transfer it to Hadoop.
March 2011 to March 2012
Hadoop Big Data Architect & Administrator
Hadoop Distributions
Skilled in performance tuning to provide robust systems for low latency and fast retrieval on cloud (AWS) and on-premise environments.
EDUCATION & CERTIFICATIONS
Redshift, queues, tuning, scaling, 100tb implementation
5 years of experience in administrating and managing Oracle database, Cloud and Big Data environment for multinational companies in financial, telecom and information technology.
Experienced on loading and transforming of large sets of structured and unstructured data
Prepare documentations and specifications.
RHEL(6 &7)/Centos/Ubuntu
Chicago Transit Authority (CTA) is the second largest public transportation system in the nation and covers the city of Chicago and surrounding suburbs. With two modes of transportation – Bus and Train – CTA provides 1.64 Million rides per day on average.
                          Chicago State University 
Pipeline & ETL
Helped design back-up and disaster recovery methodologies involving Hadoop clusters and related databases.
Operating Systems
Puppet
Translated a logical database design or data model into an actual physical database implementation.
Experienced and certified professional in Cloudera Distributed Apache Hadoop (CDAH) and Oracle Big Data Appliance (BDA) in development and production environment.
Perform tuning, firewall setup, monitor and troubleshooting cluster-wide as needed.
Setup Apache Ranger for cluster ACL’s and audits to meet compliance specifications.
Expert in troubleshooting, diagnosing, performance tuning, and solving Oracle database and Hadoop related issues in production and SDLC database environments.
Visualization
Strong experience in resource and workload management to configure YARN Capacity and Fair scheduler based on organizational needs.
Verizon Wireless
                          Bachelor of Science Degree in Computer Science
Chicago, IL
Warner Music Group
Developed database architectural strategies at the modeling, design and implementation stages.
Big Data Architect/Admin
Exported analyzed data to the relational databases using Sqoop for visualization & Report generation
Develop data migration plan for other data sources into the Hadoop system.
Worked both independently and in a team-oriented collaborative environment.
Regular database maintenance.
Hadoop /AWS Data Architect
Involved in Cassandra and Teradata for Nosql database.
Data Warehousing: Designed a data warehouse using Hive, created and managed Hive tables in Hadoop
Managed and reviewed Hadoop log files to identify issues when job fails
At CTA, we want to continue to offer clean, reliable and timely transit services to our customers and we believe putting to use all our data, not just some, plays critical roles toward our goals. We collect and analyze large amounts of data from sensors and tracking devices on our buses, rail vehicles, rail tracks and other equipment, as well as when our customers interact with our fare machines. Raw data from these sources could be structured, semi-structured or unstructured. Hadoop distributed platform allows us to store, process and analyze huge amount of data from these disparate sources in entirety like never, as oppose to traditional database systems, where we had to choose what data to keep and what to discard. Some of our use cases includes Planning and demand model, Predictive maintenance, Event response and personalized services for our Chicago Card and Chicago Card Plus loyal subscribers.
Worked with highly unstructured and structured data of 1.2 PB in raw size.
EXPERIENCE
Deployment of applications using AWS EC2
SKILLS MATRIX
Monitor production cluster by setting up alerts and notifications using metrics thresholds.
AWS administration, Hardware architecture and scaling
                          Hortonworks, Inc.
Pantaho, Power BI, Qlikview, Tableau, Informatica
Key role in migrating production and development Hortonworks Hadoop clusters to a new cloud based cluster solution.
Used Hive to simulate data warehouse for performing client based transit system analytics.
May 2010 to March 2011
Responsibilities
SUMMARY
Configuration Management
Expert in leveraging Apache Yarn for workload management and resource management.
Workflow Management: Developed workflow in Oozie to automate the tasks of loading the data into HDFS and pre-processing with Pig
Applied performance tuning to improve issues with a large, high-volume, multi-server MySQL installation for job applicant site of clients.
Suggested continuous improvements to data collection, organization and delivery processes.
Performed clusters capacity and growth planning; recommended nodes configuration for test, production and DR clusters based on business needs and workloads.
Integrated Zeppelin notebook with Spark EMR for developer use.
MapR
Integrated BI Tools for ad hoc queries, reporting and visualization using tools such as Tableau, and Qlikview.
Configure High Availability for Namenode, Resource manager, HiveServer2 and Metastore.
Modified database schema as needed.
Integrating on-premises cluster to better work with transient, cloud-based Hadoop clusters and storage.
Implemented POC to migrate map reduce jobs into Spark RDD transformation using Scala IDE for Eclipse
Sun Power Corp.
Leveraged pig for transformation and processing of unstructured and semi-structured data.
Experience in data migration from RDBMS for streaming or static data into the Hadoop cluster using Hive, Pig, Flume and Sqoop.
Collected and analyzed business requirements to derive conceptual and logical data models.
Skokie, IL
Apache Drill, Presto, ELK, Elasticsearch, Logstash, Apache Jelly 
Strong understanding of the internals and interrelationships of Hadoop components and Hardware/software infrastructure on which they are built.
VPC, S3, EC2, CloudWatch, RDS, Lambda, EMR
Kerberos, Knox Gateway, Apache Ranger
Transferred ETL workflow to processes from Hive to Redshift
Hadoop Ecosystem
Certified HDP Big Data Administrator and Systems Architect with combined 6 years of IT experience. Strong and demonstrable experience administering, managing and supporting industrial grade Hadoop solutions for a competitive edge.
Cluster consolidation saved administrative overhead cost and service contract cost and reduce technical debt.
Worked on highly available 120 nodes Production cluster running HDP 2.50
 Migrated multiple databases environments to Exadata machine using multiple migration approach.
Apache Hbase, DynamoDB,  Apache Cassandra, Datastax Cassandra, Apache Phoenix, MongoDB 
Amazon Redshift, Oracle, MySQL
Created Hive Internal and External tables and loaded the data in to tables and query data using HQL
Architected structured data on Redshift and unstructured on DynamoDB using Amazon AWS services.
SQL, BASH Scripting, Python
Irving, TX
