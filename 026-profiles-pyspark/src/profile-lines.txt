
hadoop big data architect & administrator
certified hdp big data administrator and systems architect with combined 6 years of it experience. strong and demonstrable experience administering, managing and supporting industrial grade hadoop solutions for a competitive edge.
use of puppet for node configuration, and cluster management.
cloud professional in amazon web service(aws) and microsoft azure infrastructure solutions.
skills matrix
pantaho, power bi, qlikview, tableau, informatica
apache nifi, apache camel, flume, apache kafka,talend, ketl, pentaho,
aws redshift, data pipeline, sql, loading, compression/encoding
architected etl pipelines on aws cloud using spark emr.
march 2015 to august 2016
primarily responsible for architecting and administering the hdfs distributed file systems for over 15 petabytes of data.
used oozie scheduler to automate the pipeline workflow and orchestrate the map reduce jobs that extract the data on a timely manner
develop data migration plan for other data sources into the hadoop system.
work with multiple customer teams and support teams to execute hadoop engagements.
march 2011 to march 2012
defined procedures to simplify future upgrades.  standardized all mysql installs on all servers with custom configurations.
created and implemented database standards and procedures for management.
suggested continuous improvements to data collection, organization and delivery processes.
hdp certified administrator certificate
prototyped analysis and joining of customer data using spark in scala and processed it to hdfs.
utilized spark structured streaming to update the data frame in real time and process it
data stores (repositories)
data pipelines/etl
marlabs, inc.
spark streaming implemented for real-time data processing with kafka.
developed multiple spark streaming and batch spark jobs using scala and python on aws.
worked with amazon aws iam console to create custom users and groups.hands-on work with aws emr and s3.
hive partitioning, bucketing, and joins on hive tables, utilizing hive serde’s.
loaded into hbase tables and hive tables consumption purposes.
bloomington, in
master of science in data science
manish kathait
hadoop ecosystem and big data tools and frameworks.
strong hands on experience in hadoop framework and its ecosystem including but not limited to hdfs architecture, mapreduce programming, hive, pig, sqoop, hbase, mongodb, cassandra, oozie, spark rdds, spark dataframes, spark datasets, spark mllib, etc.
well versed in using python and the packages like numpy, pandas, scikit-learn etc. for data modeling.
agile, continuous integration, test-driven development, unit testing, functional testing, gradle, git, svn, jenkins, travis, jira, puppet, maven
summary: we tested a lot of different models including latent semantic analysis(lsa), tf-idf and clustering, latent dirichlet allocation(lda), and correlated topic models(ctm). we ultimately decided to use guided/seededlda because it marries both exploration of new themes and mapping known themes. lda is a commonly used topic model that assumes that topics are made of a collection of words. it also assumes that documents are a collection of topics. so for each word in a document, there is a probability that it is drawn from a certain topic, and each topic has a probability of being associated with each document. the goals of the model are to understand which words are most associated with eac topic by calculating co-occurrence of words and also to understand which topics are most associated with each document.
data engineer	august 2017 – february 2019
performed streaming data ingestion to the spark distribution environment, using kafka.
built a prototype for real-time analysis using spark streaming and kafka.
implemented spark using scala, and utilized dataframes and spark sql api for faster processing of data.
deployed the application jar files into aws instances.
developed map reduce jobs for data cleaning and transformation
python certification
data scientist’s toolbox certification
10 years of experience in data engineering and analytics, working with
▪ hands on experience in installing, configuring cloudera's and horton
▪ extensively used apache flume to collect logs and error messages
|nessus, maltego, set, shodan, api,  |apache parquet, avro                |
|vsphere, appdynamics, confluence,   |snappy, gzip, orc                   |
|talend, talend, ketl, pentaho data  |misc                                |
|integration (kettle), jaspersoft,   |bigquery, jupyter notebooks, google |
redshift, as well as workflows to transfer aws glue log files to confluence
transformation, and reformat/load to kinesis as part of new analytics
for staging.
for airflow, redshift project.
linux (redhat),  spark
kudu/arcadia for bi analytics. creation of hive tables, loading with data,
• built row/column-count validation for workflows into shell script
tests
poland
for sales teams
• integrated disparate versions of python/spark to update older code
environment: windows 7/10, linux, ms azure, aws, cloudera, hue, apache
georgia pacific – atlanta, ga
georgia pacific uses hadoop big data analytics for audits of the consumer
products businesses which include brands such as angel soft, quilted
northern, dixie, brawny, sparkle, vanity fair, mardi gras and enmotion.
bucketing on all tables.
aggregations before hdfs.
• architected data system using apache cassandra for data storage and
modeling.
• data ingestion is done using flume with source as kafka source & sink
and care level needed can be ascertained to take proactive measures on
• used hive, spark sql connection to generate tableau bi reports.
• worked with amazon web services (aws) and involved in etl, data
queries using spark and aws emr.
and agile project methodology approach.
hadoop installed and running.
• worked with the client to reduce churn rate, read and translate data
and streaming.
• worked on streaming the analyzed data to hive tables using sqoop for
making it available for visualization and report generation by the bi
and pig jobs.
• consumed the data from kafka queue using storm
• used oozie to automate/schedule business workflows which invoke sqoop,
data has thus created a more cost-effective approach to hydrocarbon
• creating hive external tables to store the pig script output. working
• successfully loaded files to hdfs from teradata, and loaded from hdfs
environment: hadoop cluster, hdfs, hive, pig, sqoop, linux, hbase, shell
countries that have mentioned they will restrict pii processing in the
• an integrated solution that saves the company millions in data
• work in cooperation with our infragaurd partners to resolve fraud
sql,
• decreased the average scan time from ninety minutes to thirty-two
• change detection decreased downtime caused by undocumented changes
• migrated solarwinds monitoring to nagios network monitoring and
phone:
email: hamiltonblakejames@gmail.com
experience as a big data engineer and developer.
helped develop methodologies guided by the cto team for acquiring new open source libraries needed by data scientists and engineers.
helped in planning architecture to be used for autoscaling kinesis streams for increasing production workloads.
technologies:  aws, dynamodb, lambda, emr, ec2, pyspark, hive, thoughtspot, sqs, kinesis, kms, secretsmanager, iam, s3, python, pandas, cloudformation, amazon elasticsearch, lex, bitbucket, bamboo
integrated spark code into the sdlc with the ci/cd pipeline using jenkins ci with git versioning.
structured unstructured data with spark.
migrated data through sqoop and hive in hdfs platform
automated and defined spark and aws best practices for future deployment.
set-up aws lambda to process event-driven data to various aws resources.
hadoop developer
involved in implementing security on hdp hadoop clusters with kerberos for authentication and ranger for authorization and ldap integration for ambari, ranger
created hive tables, loading with data and writing hive queries.
hands-on work with databases like cassandra and writing sql, pl/sql for creating tables, views, indexes, stored procedures and functions.
elastic █████████
hadoop components █████████
used kinesis with spark streaming for high speed data processing.
played a key role in installation and configuration of the various big data ecosystem tools such as (elk) elastic search, logstash, kibana, kafka and cassandra.
i worked as an aws developer, were i leveraged various aws services to create a big data pipeline. the purpose of this pipeline is to take data sources from our stores to create custom displays and sales that target a geographical region’s overall interests. aws lambdas and api calls were used to ingest the data into apache kafka. the data is then put into s3 buckets via firehose. the data in these buckets is then consumed by emr to perform etl and machine learning. for hot data, the data is put in aws redshift. for long-term storage, data is ingested into hive external tables and stored in s3 buckets. elasticsearch and kibana are used to display data from redshift and hive via kibana dashboards for our end users. an elk stack is used to ingest logs from all pipeline pieces to gain insights into pipeline health and performance.
implemented data processing using hadoop cloudera distributions on aws.
migrated data from hortonworks cluster to aws emr cluster.
responsible for designing logical and physical data modelling for various data sources on amazon redshift.
in this position i worked extensively with large hadoop and spark clusters managing terabytes of data by creating a complex pipeline. sensor data is ingested through kafka and then into spark streaming. etl is performed, and then the data is stored within hive. legacy data is imported into hive using sqoop. this legacy data is then processed in batch jobs with hadoop. i was also responsible for performance optimizations for our clusters.
responsible for performance optimization of clusters.
set-up kerberos for more advanced security features for users and groups.
senior big data engineer
corvallis, or
big data development
cassandra
jenkins
aws elk
hdfs
hadoop
yarn
cluster security
created hive queries to spot trends by comparing hadoop data with historical metrics.
deep understanding and implementations of various methods to load hive tables from hdfs and local file system.
processed data in aws glue by reading data from mysql data stores and loading in to redshift data warehousing.
worked on deployment of code to aws code commit using git commands (pull, fetch, push and commit. etc) from aws cli.
cloudera manager running on cloudera hadoop clusters used to measure performance for optimization.
handled importing of data from various data sources, performed transformations and analysis using hive.
involved in complete big data flow of the application starting from data ingestion from upstream to hdfs, processing the data into hdfs using spark.
collaborated with security team to implement kerberos on hadoop to enhance privacy and security.
conducted exploratory data analysis and managed dashboard for weekly report, using tableau desktop connecting to hadoop hive tables.
worked on kafka cluster environment and zookeeper.
provide end-to-end data analytics solutions and support using hadoop systems and tools on cloud services as well a on premise nodes.
aws, kali, cisco
systems
snappy, gzip, orc
nessus, set, api, metasploit, wireshark, vmware, vsphere, appdynamics, confluence, jira, , rabbitmq, minsoft, nagios, cloudclock
agile scrum, pair programming
senior big data engineer	may 2019 - present
wells fargo	summit, nj
apply codebuild to package python libraries inside conda environments.
used cloudwatch event rules to create triggers upon the completion of training jobs.
created model catalogue across development, training, testing environments using mlflow.
created iam service roles and policies.
configured fair scheduler to allocate resources to all the applications across the cluster.
involved in complete big data flow of the application starting from data ingestion from upstream to hdfs, processing the data into hdfs using spark streaming.
performed maintenance, monitoring, deployments, and upgrades across infrastructure that supports all hadoop clusters.
responsible for developing data pipeline using sqoop, mr and hive to extract the data from weblogs and store the results for downstream consumption.
created the pipeline to scrape data from internal resources.
proposed a working poc and constructed the roadmap for prediction pipeline.
used etl to transfer the data from the target database to pentaho to send it to microstrategy reporting tool.
conducted poc for hadoop and spark as part of nextgen platform implementation. implemented recommendation engine using scala.
servicenow	santa clara, ca
developed mapreduce jobs using java for data transformations.
b.a. in computer science and physics
in combination with my master's degree allows me to deliver business solutions to maximize business value, create flexibility for change, and accelerate business outcomes to create and improve enterprise etl systems and custom pipelines.
apache frameworks
apache hadoop core, apache ant, apache flume, apache yarn, apache hcatalog, apache hive, apache kafka, apache maven, apache oozie, apache pig, apache spark, apache tez, apache zookeeper, apache airflow, apache camel, apache lucene, apache solr, apache drill, apache hue
my boeing team was responsible for the ingestion of confidential information from the autonomous airplanes.  data was derived from multiple data sources including multiple sensors along with old legacy data.   we successfully delivered a solid pipeline that will retain all the information coming from the autonomous services.
automated all jobs for pulling data from windows share servers to a hadoop big data lake.
experience in transferring streaming data from different data sources into hdfs and hbase using apache flume.
experienced on loading and transforming of large sets of structured and semi structured data from hdfs
navigation through various screens of application is implemented using web scrapers.
created shell scripts and pl/sql scripts that were executed daily to refresh data feeds from multiple systems.
autonomous university of queretaro
master’s degree in economic science and administration
responsible for moving and transforming massive datasets into valuable and insightful information.
amazon aws cloud
tools
hdfs, hive, zookeeper, sqoop, oozie, yarn
unix shell scripting, , pl/sql scripting, html, dhtml,html5, css3, javascript, vb script
etl pipeline
apache camel, flume, apache kafka, logstash
http, tcp/ip, ftp, pop3, smtp
hadoop big data architect
capital one, mclean, va
created multiple data visualization with reports and dashboards in kibana
developed a dashboard for errors and linked kibana dashboard to give a drill down effect.
made a dashboard to display business metrics.
developed multiple qa dashboard
created custom hadoop analytics solution integrating data analytics platform to pull data from decentralized assima systems for various analytical used such from marketing to actual use data.
evaluated proprietary software and current systems to determine gaps.
created cloudera hadoop system on aws consisting of multiple nodes with defined systems by use case.
worked closely with hdinsight production team for the optimization of the performance of spark jobs on the cluster.
created external hive tables on the blobs to showcase the data to the hive metastore.
april 2015
involved in creating hive tables, loading with data and writing hive queries, which will internally run a map reduce job.
generated and published reports regarding various predictive analysis on user comments. created reports and documented various retrieval times of them using the etl tools like qlikview and pentaho.
lawrenceville, ga
log files.
wrote sql queries to perform data validation and data integrity testing.
october 2006-
network administrator for company lan
setup and implemented jserv and apache web server.
recorded defect found in test director.
oracle crm fallout handling for bellsouth support.
shared 24/7 on call every 5 week
12
database technologies.
• hadoop, cloudera, hortonworks, cloud data analytic platforms, aws, azure
• hands-on use of spark and scala api's to compare the performance of spark
hadoop to/from rdbms.
apache spark, hive, kafka, storm, pig, sqoop
tableau, powerbi, kibana, pentaho, qlikview
apache ant, apache cassandra, apache flume, apache hadoop, apache hadoop
yarn, apache hbase, apache hcatalog, apache hive, apache kafka, apache
revenue.  the analysis looks at how to get music into the ears of
aws s3 for raw file storage, 3. aws ec2 for kafka)
• used aws lambda to perform data validation, filtering, sorting, or
data store
impressions, household tv impressions data processing.
• load terabytes of different level raw data into spark rdd for data
rdd.
data supplier name which further processed pulled by analytics service
• successfully loading files to hive and hdfs from oracle, sql server
aws emr, aws s3, aws ec2
mysql to hdfs on regular basis.
• worked with deep knowledge in incremental imports, partitioning and
transform the large volumes of data with respect to business
• etl (extract, transform, load) large sets of structured, semi-
on region.
to have a back-up of images.
reduced costs and ease of maintenance and scalability.  designed and
• designed and implemented incremental imports into hive tables.
• worked on creating the rdd's, df's for the required input data and
• archived the data to hadoop cluster and performed search, query and
• extensively worked on performance optimization of hive queries by
using map-side join, parallel execution, and cost-based optimization.
queries, implemented partitioning, dynamic partitions and buckets in
• developed workflow in oozie to automate the tasks of loading data into
all the components in the system. hands-on integrating spring with
• done poc on stratosphere (a big data analytics platform) to perform
lockheed martin uses big data analytics for everything from aircraft
maintenance to aircraft readiness and reliability.  data analytics can
logistics, transport of carriers and fuel.
data.
• used bteq and sql assistant (query man) front-end tools to issue sql
• created reusable transformations and mapplet and used with various
/windows 2000, teradata, rdbms v2r5, tuf (teradata utility foundation)
david dubois    703-659-4464  |  duboisdavid582@gmail.com
30 years – software engineering
excellent knowledge on hadoop architecture and ecosystems such as hdfs, configuration of nodes, yarn, sentry, spark, falcon, hbase, hive, pig, sentry, ranger.
working with large complex data sets, real-time/near real-time analytics, and distributed big data platforms.
ability to troubleshoot and tune relevant programming languages like sql, python, scala, pig, hive, rdds, dataframes. able to design elegant solutions through the use of problem statements.
experienced in the full software development life cycle from requirements gathering through architecture, development, testing and deployment.  expertise in the development of ui/ux design and implementation -  interfaces including graphical trees and other detailed data with data analysis to present the operator with red, green, yellow type statuses. software design, integration, productivity tool development.  database scripting and product count analysis to simplify the visualization of the data to an operator. developed scripts to read the delivery status from the database and developed a clear color-coded display and summary that because the product tracker application.
optimized memory-based io driven performance on the laurel and hardy platform using light cpu.
the team used jira for issue tracking and managing tasks and backlog, and confluence for documentation.
used boto3 to access aws s3 from an emr cluster.
managed change requests through change order logging process implement changes  into the production system.
may 2016	feb 2019
data ingestion is done using flume with source as kafka source & sink as hdfs.
for one of the use case, used spark streaming with kafka & hdfs & mongodb to build a continuous etl pipeline. this is used for real-time analytics performed on the data.
configured spark streaming to receive real-time data from kafka and store the stream data to hdfs.
used spark sql and dataframes api to load structured and semi-structured data into spark clusters.
worked with spark context, spark -sql, dataframe and pair rdds.
developed metrics, attributes, filters, reports, dashboards and also created advanced chart types, visualizations and complex calculations to manipulate the data.
techdata – clearwater fl
environment:  hadoop, spark, hdf, oozie, sqoop, mongodb, hive, pig, storm, kafka, sql, acro, rdd. sqs s3, cloud, mysql, informatica, dynamo db
involved in loading the created files into hbase for faster access of all the products in all the stores without taking performance hit.
implemented partitioning, bucketing in hive for better organization of the data.
developed image down-sampling software to output jpeg images at 100-megabyte vs the original size of 2 gigabytes allowing an operator to perform a quality check of the products in 5 minutes instead of 5 hours (python).
software development for the goes-r program. worked with the team of developers across the country to perform requirements analysis, uml design, develop, and test c++ and java code integrating goes-r science algorithms into pgim. also acting as the deputy lead for the l2 team.
software integration for the gmr program encryption/decryption algorithms, key management, certificate signing, and radio control software for the jtrs gmr radio.
bachelor of science, computer science (bscs)
5 years experience in big data and i.t.
modified existing and added new functionalities to financial and strategic summary dashboards.
project methods:
cloud database & tools:
file systems:
kerberos, ranger, blockchain
aws big data engineer
ingestion data through aws kinesis data stream and firehose from various sources to s3.
bachelor’s degree in management information system with computer science minor – university of georgia
hands-on experience with aws and azure.
dealing with multiple terabytes of mobile ad data stored in aws using elastic map reduce and redshift postgresql.
hands on programming using spark, scala, python, java to refine hadoop data analytics.
analyzed and documented existing customizations as well as the current sharepoint 2013 environment.
vpc, route 53, security groups, manage route, firewall policy, load balancer, dns setup.
as part of batch modernization initiative in ec2, analyzed existing batch ingestion developed in oracle data integrator and developed pyspark application as etl tool. this reduced the batch ingestion time from 3.5 hrs to 15 minutes.
setup cloud compute engine managed and unmanaged mode and ssh key management.
iam access control and policy creation, service account and access management.
philadelphia, pa
master of arts in linguistics
march 2019- present
capital one financial corporation – mclean, va
•	created proof of concept for (and later implemented) integrating existing application architecture with inner-sourced pipeline platform, to improve architecture efficiency and ensure compliance to security standards.
worked embedded with core team on-site and collaborated remotely with indian offshore teams.
streamlined unix shell and hive program scripts, decreasing program startup time by a factor of 10.
standardized team project management, source control, and documentation using the atlassian enterprise jira/confluence/bitbucket stack.
created cron-synchronized scheduler task to automatically refresh restful web services cached data every 30 minutes, eliminating the need for costly and error-prone manual refreshing.
august 2017- march 2018
responsible for project management and team management.  nvp embedded engineers on-site in separate teams with a handful of bank analysts. working in parallel, we worked to take raw financial data and conform it to a dimensional model. this dimensional data would then be used to stage in ibm datastage and load into ibm netezza database
applied agile principles to team work activities
implemented spark using scala and spark sql for faster analyzing and processing of data.
worked on importing the unstructured data into the hdfs using spark streaming & kafka.
wrote complex hive queries, spark sql queries and udfs.
assisted in designing, building, and maintaining database to analyze life cycle of checking and debit transactions.
hands-on experience of large database systems: rdbms oracle and sql.
march 2009- june 2010
accenture – irving, tx
proficient in extracting and generating analysis using business intelligence tool, tableau for better analysis of data.
spark architecture including spark core, spark sql, spark streaming, spark
experience in importing and exporting data using sqoop from oracle/mainframe db2 to hdfs and data lake.
technical security skills profile
maven, ant, flume, hdfs
hadoop administration
unit testing, functional testing, scenario testing, regression testing, object-oriented programming, functional programming
agile scrum, sprint planning, sprint retrospective, sprint grooming, backlog, daily scrums
aws cloud, hadoop on-prem, hadoop, cloudera (cdh),
involved in converting hive/sql queries into spark transformations using spark rdds, scala.
fine-tuned resources for long running spark applications to utilize better parallelism and executor memory for more caching.
automated the pipelines in spark for bulk loads as well as incremental loads of various datasets.
implemented hdfs access controls, directory and file permissions user authorization that facilitates stable, secure access for multiple users in a large multi-tenant clusterdfs
• data extraction, transformation and load in hive, pig and hbase
• designing and implementing of secure hadoop cluster using kerberos.
sas, c++, pl/sql, eclipse, sharepoint
database [pic][pic][pic][pic]
development, unit testing, functional testing, design thinking, lean, six
maven, apache oozie, apache pig, apache spark, spark, streaming, spark
teradata, netezza, nifi, oracle, and sas analytics and eminor, informatica
present     revionics – atlanta, ga
kafka for batcha and realtime streaming pipelines and analytics for this
identifying the attributes and to convert the business requirements
requirement.
value using md5 algorithm in java and used various udf's from
files and parse data into json format.
• created tables in teradata to export the data from hdfs using sqoop
after all the transformations and wrote bteq scripts to handle updates
• responsible for gathering requirements to determine needs and
quality standards and consistency.
• used spark sql to perform transformations and actions on data residing
• used spark streaming to divide streaming data into batches as an input
parquet format in hdfs.
• wrote spark applications for data validation, cleansing,
• created unix shell scripts to automate the build process, and to
environment: cdh 5.5.1, hadoop, map reduce, hdfs, nifi, hive, pig,
sqoop, ambari, spark, oozie, impala, sql, java (jdk 1.6), eclipse.
spring mvc, spring 3.0
importing various formats of flat files into hdfs.
• used pig as etl tool to do transformations, even joins and some pre-
to validate sales data.
functionality in apache nifi for some additional tasks.
message router), itcm (interoperable train control messaging), onboard
• used tableau for data visualization and generating reports.
• developed python scripts, udf's using both data frames/sql and rdd in
• developed spark code using scala and spark-sql for faster testing and
dec 2013    charles schwab – littleton, co
files
environment: cloudera distribution cdh 4.4.1, hadoop hdfs, mapreduce, hive,
• responsibilities include designing and delivering web-based j2ee
• involved in writing of release notes to deploy in various environments
developed custom j2ee and ejb for custom analytical platforms in the
• modified existing java apis on performance & fault management modules.
easy mock, and junit.
agile development methodology.
phone:  470-344-0250
senior hadoop data engineer
vicenteayala1888@gmail.com
phone: 703-936-4065
gmail.com
hands on experience in coding mapreduce/yarn programs using java, scala for analyzing big data.
apache airflow, apache camel, apache flink/stratosphere, hive, pig, sqoop, flume, scala, python
etl architecture, creation for various use cases
performed data analysis on transactions for visa and mastercard.
troubleshot spark applications in emr cluster.
designed and documented end-to-end workflows for the application and how we deploy the application along with the infrastructure.
contributed to projects using react and nodejs for web components.
hbase master and region servers, phoenix query servers working to store.
aggregate keyed elements in memory and be able to work with the state using valuestate and liststate.
ensured hippa compliance and security of sensitive data using hashing, md5 sql encryption, kerberos.
python
spark transformations
created hive tables and dynamic partitions, with buckets for sampling and working on them using hive ql.
june 2014 – august 2015
worked on importing and exporting tera bytes of data using sqoop from hdfs to relational database systems and vice-versa.
designed jobs using db2 udb, odbc, .net, join, merge, lookup, remove duplicate, copy, filter, funnel, dataset, lookup file set, change data capture, modify, row merger, aggregator and peek, row generator stages.
omnium financial services, chicago, il
migrated big data architecture to cloud created on aws using aws tools and database instances with hadoop hdfs to create a data lake in cloud.
designed a cost-effective archival platform for storing big data using hadoop and its related technologies. created data modeling and implemented redshift instance on amazon.
terasort
sql queries
automated the inventory for more than one thousand windows servers
microsoft hyper-v
shinray kuo
e-mail: shinraykuo1994@gmail.com
responsible for designing and deploying new elk clusters. (elasticsearch, logstash, kibana, zookeeper).
expertise implementing and managing jenkins continuous integration server.
documented the requirements including the available code which should be implemented w/ spark, hive, hdfs and elastic search.
oct 2016 – feb 2018
sept 2015 – oct 2016
bachelor of computer science
page 5hadoop consultant  912-421-0128 | georgeatarpeh@gmail.com
inform and recommend cluster upgrades and improvements to functionality and performance.
experience with large-scale hadoop deployments (40+ nodes; 3+ clusters).
hive, spark, kafka, sqoop, flume, camel, apatar, talend, tez
hive, pig, mapreduce, yarn, python, scala, spark, spark streaming, storm
jan 2016 – present
consisting of near real-time using spark sql; spark cluster.
st. louis, mo
worked to create a new system that would enable analysts to improve operational efficiencies.  after the necessary technology architecture was put in place, graybar identified initial big data and predictive analytics use cases that could produce immediate results. fleet management and asset utilization were some of the first beneficiaries of using open source software solutions to extract patterns and predict trends from existing data sets.
september 2013 – december 2014
developed dynamic parameter file and environment variables to run jobs in different environments.
eau claire, wi
analyzed mapreduce programs to moderate complexity.
created analytics system for marketing and inventory analysis for this retail grocery chain.  responsible for coding to develop custom pipelines and visualization reports and queries.
analyzed the data by performing hive queries (hiveql), impala and running pig latin scripts to study customer behavior.
natallia laurova
administration of hadoop cluster (cdm); review of log files of all daemons.
expertise in storm for reliable real-time data processing capabilities to enterprise hadoop.
apache airflow, apache camel, apache flink/stratosphere, hive, pig, sqoop, flume, scala, python,, flume, apache kafka, logstash,
big data frameworks
ide: jupyter notebooks, pycharm, intellij
installed and configured tableau desktop to connect to the hortonworks hive framework (database) which contains the bandwidth data from the locomotive through the hortonworks odbc connector for further analytics of the data.
automate ci/cd on qa/prod environments, helping developers/qa automation team to achieve their day to day goals.
handled large amounts of data utilizing spark dataframes/datasets api and case classes.
migrated long running hadoop applications from legacy clusters to spark applications running on amazon emr.
conducted qa procedures, compliance with the accepted concept, timing, acceptance of the implementation of etl-processes.
created aws lambda function for extracting the data from kinesis firehose and post the data to aws s3 bucket on a scheduled basis (every 4 hours) using aws cloud watch event.
experienced in deployment of hadoop cluster using puppet tool.
built the hive views on top of the source data tables and built a secured provisioning framework for users to access the data through hive based views.
phone: 999-999-9999
able to work with existing eds platforms and strategic initiatives that are built for future phases of eds/ebi.
unix shell scripting, object-oriented design, object-oriented programming, functional programming,  sql, java, hive ql, mapreduce, python, scala, xml, blueprint xml, ajax, rest api, spark api, json, avro, parquet, orc, jupyter notebooks, eclipse, intellij, pycharm, matlab, sas & r, cs i & ii, assembly & c programming
high-availability, fault tolerance, scalability, database concepts, system and software architecture, security, it infrastructure, virtualization, and internet technologies.
delta air lines, inc. is a major american airline with an extensive domestic and international network. a main concern for commercial airlines is knowing exactly where a passenger’s baggage may be at all time and prevention as well as finding of lost baggage. delta looked further into their data and created a solution that would remove the uncertainty of where a passenger’s bag might be.  this platform was constructed specifically for that purpose, and it employs data from internet input’s on customers including planes, flights and itineraries, and iot sensor data from various machines, computers and conveyor belt scanner located in various airports as well as hand held scanners used by baggage handlers.
november 2012 – november 2014
saml2.0 idp authentication & integration using tivoli federation identity manager for saas applications like sfdc, bmi, lithium community, cornerstone ondemand, perks, cid etc.
university of maryland: baltimore county: mathematics b.a.
vincent schneider   |   phone  999-999-9999   |  email@gmail.comheber fragoso / sharepoint / .net  /  phone: 413-301-9827    /    e-mail: fragosomezaheb@gmail.com
a motivated and experienced it professional with 5 years of experience in big data systems including engineering, development and administration on prem and on cloud.
experienced in working on cql (cassandra query language), for retrieving the data present in cassandra clusters by running queries in cql.
aarp
sept 2015 – dec 2016
novartis
infosys
loc    san francisco, ca
l
r
computer information systems
senior data engineer with strong hands on experience in hadoop framework and its ecosystem including but not limited to hdfs architecture, hive, pig, sqoop, hbase, mongodb, cassandra, oozie, spark rdds, spark dataframes, spark datasets, spark mllib, etc.
cloudera, hortonworks
aws, google cloud, azure cloud
imported data from aws s3 and into spark rdd and performed transformations and actions on rdd's.
used spark for interactive queries, processing of streaming data and integration with popular nosql database for huge volume of data.
wrote different pig scripts to clean up the ingested data and created partitions for the daily data.
used avro, parquet and orc data formats to store in to hdfs.
imported required tables from rdbms to hdfs using sqoop and also used storm and kafka to get real time streaming of data into hbase.
developed java code to generate, compare & merge avro schema files.
importing and exporting data into hdfs and hive using sqoop.
writing the hive queries to extract the data processed.
created hbase tables to store variable data formats of data coming from different legacy systems.
deployment and testing
hadoop engineer / january 2015 – april 2016
debugging and identifying issues reported by qa with the hadoop jobs by configuring to local file system.
experienced in running hadoop streaming jobs to process terabytes data.
worked with application teams to install operating system, hadoop updates, patches, version upgrades as required.
installed oozie workflow engine to run multiple hive and pig jobs.
data ingestion, extraction and transformation using etl processes developed using hive, sqoop, kafka, firehose, flume, kinesis and mapreduce.
implementation and management of data systems using cloudera hadoop, hortonworks hadoop, hadoop or aws cloud platform or on premise.
design and build big data architecture for unique projects, ensuring development and delivery of the highest quality, on-time and on budget.
good experience in working with cloud environment like amazon web services (aws) ec2 and s3.
spark api, rest api, soap api
security & forensics
aws, azure, anaconda cloud, elasticsearch, solr, lucene, cloudera, databricks, hortonworks, elastic.  cloud foundry, elastic cloud
monitor background operation in hortonworks ambari
spotify is an online music catalogue providing users with personalized recommendations based on their taste in music. spotify, the largest on-demand music service in the world, has a history of pushing technological boundaries and using big data, artificial intelligence and machine learning to drive success. the digital music company with more than 100 million users has been busy this year enhancing its service and tech capabilities through several acquisitions.
evaluated cloudera hadoop on aws cloud services to estimate the scalability/cost of the environment
configured sap hana smart data access (sda), sap data services to integrate with hadoop environments for reporting purposes
used amazon kinesis and amazon redshift to provide real-time analytics.
bachelor of arts in computer science
minor: communication study
dashboard
big data engineering with strong understanding of hadoop big data architectures, data movement technologies, database partitioning, database optimization, and building communication channels between structured and unstructured databases; cloud technologies and tools.
vincent.nwobodo888@gmail.com
leadership
transferred data from data lake comprised of aws buckets where the raw events are stored into datamart which is loaded via spark streaming pipelines.
utilized amazon web services (aws) cloud services like ec2, s3, ebd, rds and vpc.
experience in importing and exporting data using sqoop 1.99.7 from hdfs to rdbms and vice-versa.
worked on setting up high availability for major production cluster and designed for automatic failover.
designed and implemented a distributed network monitoring solution based on nagios and ganglia using puppet.
upgraded the hadoop cluster from cdh3 to cdh4. designed and allocated hdfs quotas for multiple groups.
search tools: apache lucene, elasticsearch, solr
misc software:  virtual box, visual studio, vim
big data experience
this project was to implement the existing hadoop data analytics system using spark streaming and spark.
identified and ingested source data from different systems into hadoop hdfs using sqoop, flume, creating hbase tables to store variable data formats for data analytics.
created hive and impala queries to spot emerging trends by comparing hadoop data with historical metrics.
jan 2015-feb 2016
python, unix, shell scripting, linux
prepared data for usage in predictive modeling and machine learning.
responsible for etl, source to target documentation, and bi and analytic solutions in a big data environment.
developed spark scripts by using scala as per the requirement.
responsible for importing and exporting data into hdfs and hive using sqoop.
hands-on  major components in hadoop echo systems like spark, hdfs, hive,hbase, zookeeper, sqoop, oozie, flume, kafka.
responsible for defining the data flow within hadoop eco system and direct the team in implement them.
troubleshooting of port opening issues along with firewall team for data transfer, kerberos configuration.
daily housekeeping of local file systems, hdfs and involved in scripts creation for automated housekeeping.
phone: (571) 934-4496
5 years of professional it experience in big data engineering, development and administration.
developed spark code for spark-sql/streaming in scala and pyspark.
kafka used for cluster handling in real time processing.
experience with multiple terabytes of data stored in aws using elastic map reduce (emr) and redshift postgresql.
configured airflow.cfg and added postgres credentials  to use postgres db for metadata
developed custom aggregate functions by spark sql and performed interactive querying.
dell computer
used aws redshift clusters to sync data as a data warehouse solution of our data pipeline in aws and used aws rds to store the data for retrieval to dashboard.
amazon web (aws)
met with tech team to discuss best practices for coding on python.
developed pyspark scripts for ingestion of structured and unstructured data.
found solutions to data mapping for unstructured data by applying schema inference
worked with hadoop admins to fix encoding issue on data files.
cleanse, aggregate, and organize hadoop hdfs data lake to build data science insights.
analyzed transaction event & history data and credit card holder profile data for fraud detection and risk assessment, using hadoop technologies including hive, impala, dataframes, sparksql.
worked in agile development approach.
developed a strategy for full load and incremental load using sqoop.
created over 10 dashboards, 20 reports written in hive for daily reports
answerlab | san francisco, ca	march 2013– may 2014
loaded data into hbase for online lookups to business using scala
partitioning and bucketing done for the log file data to differentiate data on a daily basis and aggregate based on business requirements.
email:  collinspenoh@gmail.com
hard-working
international paper
university of yaoundé
skilled in managing data analytics and data processing, database and data driven projects
expert in bucketing and partitioning
architecture of new data analytics pipelines and migration of data to cloud.
moved the data from hortonworks cluster to aws emr cluster.
managed all the bugs and changes into a production environment using the jira tracking tool
analysis of end user requirements and business rules based on given documentation and worked closely with tech leads and business analysts in understanding the current system.
monitoring hadoop scripts which take the input from hdfs and load the data into hive.
created pig scripts to process raw structured data and developed hive tables on top of it.
expertise in preparing the test cases, documenting and performing unit testing and integration.
accountable for the timely, efficient upgrades of patient health information databases for the behavioral health services and support services to end user terminals and physician practices requiring strong attention to detail, organizational skill, and the ability to maintain tight delivery schedules.
wsw health partners, inc., olympia fields, illinois
services coordinator
responsible for updating inter-university research databases and implementing regulatory compliance mandates through the coordination of research accreditations;
diploma in supply chain management
management if client technical supportshiva teja
6 years hadoop - big data engineering
large scale hadoop environments build and support including design, configuration, installation, performance tuning and monitoring.
programming and scripting
spark, python, scala, hive, pig, kafka, sql
cloudera cdh 4/5, hortonworks hdp 2.3/2.4, amazon web services (aws)
at&t
june 2016 – present
provide mentorship and guidance to other architects to help them become independent.
provide review and feedback for existing physical architecture, data architecture and individual code. moved transformed data to spark cluster where the data is set to go live on the application using kafka.
worked on analyzing hadoop cluster and different big data analytic tools including pig, hbase amazon redshift, apache cassandra, sql, and sqoop.
proficiency with mentoring and on-boarding new engineers who are not proficient in hadoop and getting them up to speed quickly.
experience with being a technical lead of a team of engineers.
extensive experience with hadoop and hbase and redshift, including multiple public presentations about these technologies.
intuit, inc.
worked with application teams to install operating system, hadoop updates, patches, version upgrades as required. worked with flume to load the log data from multiple sources directly into hdfs.
hands- on experience in developing web applications using python on linux and unix platform.
responsible for managing data from multiple sources.
worked on installing cluster, commissioning & decommissioning of data node, name node recovery, capacity
epic systems
collaborated with customers on a frequent basis to define and accomplish long-term objectives for organization’s success
worked on internal project to prevent downtime across the network.
writes complex sql queries with databases like db2, mysql, sql server and ms sql server.
kafka messaging system to implement real-time streaming solutions using spark streaming.
aws, cloud foundry, elastic cloud,
kibana, tableau, splunk
i was involved in migrating a big data environment from an on-premise system to an aws cloud platform.  this data environment was for a credit line increase program to manage credit limit offers to customers according to rules based on expense, income and other criteria. capital one collects data from different sources and runs inclusion and exclusion rules, model calculation, segmentation, and ability to pay processes to determine which customers are eligible for an increase.
prepared use cases, mock data and error scenarios to test workflows execution in an emr cluster in qa environment.
created a kafka consumer that reads from a hard-coded kafka topic, creates a dataset and delivers parsed outputs to the stream data platform to send data for fulfillment.
ran the database migration assistant to upgrade the existing sql server to azure sql database.
key technologies:  azure hdinsights, statestage python, sqoop, spark, hadoop, hdfs, spark data frame, azure hdconnect, azure internet of things, stream analytics,cosmosdb ,spark,cloud service,iot ,hadoop,big data,,data analytics,virtualization,cloud automation, hive, kafka.
ppg is working on developing products that are geared toward sustainability, with solutions such as sigma air pure, a revolutionary bio-based product that protects indoor air quality while it beautifies.  the company gathers data to analyze impact on environment, contribution to waste and greenhouse gas emissions, and impact on health and wellness.
key technologies:  hadoop, hdfs, spark, kafka, hive, pig, spark streaming, apache flume, kafka topics, apache zookeeper, hbase,
schreiber foods, greenbay, wi
created reports and documented various retrieval times of them using the etl tools like qlikview and pentaho.
data engineer	12.2012 – 02.2014
logs that are stored on hdfs were preprocessed using pig and the processed data is imported into hive warehouse which enabled business analysts to write hive queries.
performed transformations of data using hive and pig to hdfs for aggregations.
created and developed the unix shell scripts for creating the reports from hive data.
key technologies:  hdfs, hive, pig, mapreduce, udf, python, unix, rdd, udf, aws, s3, data lake
prior experience
skill using open source software like spark, flume, and kafka.
c++, java, sql, javascript, python, oracle
siemens	alpharetta, ga
energy future holdings corporation is an electric utility company. the majority of the company’s power generation is through coal- and nuclear-power plants. the company used big data to install smart meters. the smart meters allows the provider to read the meter once every 15 minutes rather than one month.
data engineer	september 2014 - january 2016
chris peng
big data compute
visualization:  kibana, tableau
kafka for data ingestion and extraction with move into hdfs big data system.
spark sql to create real-time processing of structured data with spark streaming processed through structured streaming.
worked on aws cloud formation templates for data pipeline in aws
built aws kinesis for processing real time data invoking lambda functions and loading it to dynamodb tables.
decision lens - arlington, va
8+ years iot and information technology
experience with streaming analysis using spark streaming, kafka streams and kinesis firehose, storm, flink, nifi
spark mllib to run machine learning algorithms on spark clusters
web services (soap+get+post), wcf, wpf, web api 4.0
cloudera impala, apache lucene, elasticsearch, apache solr
hbase, cassandra, dynamo
hive bucketing and partitioning
spark performance tuning
implemented new data pipelines for new sponsor projects.
worked closely with stakeholders and data scientists/data analysts to gather requirements and create an engineering project plan.
set-up virtual environment on aws ec2 and implemented hadoop on ec2 with amazon redshift and iot aws on aws s3.
utilized spark with aws cloud services for iot data pipeline.
technologies: hive, data warehouse, teradata, hdfs, etl, kafka, sqoop, aws, haoop, hive, pig,
american society for engineering education (asee)
provide end-to-end data analytics solutions and support using hadoop systems and tools on cloud services as well as on premise nodes.
apache cassandra, amazon redshift, amazonrds, sql, apache hbase, hive, mongodb
versioning: git, github
cluster security & authentication
test-driven development, continuous integration, unit testing, functional testing, scenario testing, regression testing
amazon aws big data engineer
kahler slater, milwaukee, wi                                          	september 2013-february 2015
secured the kafka cluster with kerberos.
managing hadoop clusters via command line, and hortonworks ambari agent.belay goytom
uses expert skills across several platforms and tools and working with multiple teams in high visibility roles.
use of databases and file systems  in hadoop big data environments such as sql and nosql databases, apache cassandra, apache hbase, mongodb, oracle, sql server, hdfs
for hadoop data processing, familiar with amazon aws, microsoft azure, anaconda cloud, elasticsearch, apache solr, lucene, cloudera hadoop, databricks, hortonworks hadoop, or hadoop environments.
wrote mapreduce and spark codes in java to run a sorting application on the data stored on aws.
texas tech university
my team is in charge of coding policies for 4 upmarket capital one credit card products: the product owners write the rules in a confluence page called ‘intent’ which contains all the features and variables that need to be extracted from the application and the flow in which the rules need to be evaluated by the rules engine. as a result of the evaluation, some things like the amount of the assigned credit line and the apr for the customer will be determined.
the project was divided in two main parts:
read avro files from s3 and generate a dataframe with spark using scala
documented and managed the project using jfrog and atlassian confluence.
environment: hdfs, pig, hive, sqoop, oozie, hbase, zoo keeper, cloudera manager, java, ambari, oracle, mysql, cassandra, sentry, falcon, spark, yarn, mapreduce
may 2015 	hadoop data architect/engineer
utilized java and mysql from day to day to debug and fix issues with client processes.
rich experience of database design and hands-on experience of large database systems: oracle 8i and oracle 9i, db2, pl, sql.
administered hadoop cluster(cdm) and reviewed log files of all daemons.
developed functions, stored procedures in transact-sql and triggers for sql server 2008. i participated in the development of the system of maintenance logs to federal police aircraft using java with hibernate, spring and zk frameworks.
diploma of administration of databases
data systems, data lakes, data warehouse and other repositories to
• ability to troubleshoot and tune relevant programming languages like
real-time analytics, and distributed big data platforms.
• in-depth knowledge of real-time etl/spark analytics using spark sql
perform the mapreduce jobs/hive queries.
mailer.
versa.
beans, spring framework, mvc and hibernate.
• experience with front-end technologies like html5, css3, javascript
jboss, weblogic, websphere, tomcasthouse, das, nas, san
used to create and manage data lakes to collect and store a massive
system, and triggering real-time actions from statistical data.
involved storm, spark, hive, kafka, python, and scala in addition to
• involved in migrating hive queries into spark transformations using
• developed shell scripts to periodically perform incremental import of
enhancements and with data center teams on testing and deployment
database on a 4 node hadoop cluster
and map reduce and wrote mapreduce job using scala
• evaluated various data processing techniques available in hadoop from
records, wrote udfs, and generated the output data using pig and hive
• creating mapreduce jobs for performing etl transformations on the
between multiple data sets using mapreduce, pig, and hive
spark streaming, and storm to enable analysis of roi on pr campaigns
using scoop.
• loaded home mortgage data from the existing dwh tables (sql server) to
nodes, cluster setup, cluster performance and monitoring on daily
continuous integration).
• supported the team using talend as etl tool to transform and load the
aug 2011    big data developer
and hive
structured and unstructured data and store in managed and external
• performed a test run of the module components to understand the
hive, sqoop, hbase, and pig in team
involved in building our data warehousing solutions for the banking
requirement specification document.
apache tomcat server
• involved in api development using core java concepts
design.
• used development environment integrated with eclipse with team support
by git
november 2007    database administrator
years.
developed stored procedures and used ssis 2005 to transform the data from enterprise data warehouse to data mart to build dimensions and load data to fact tables.
database migration from on premise to sqlazure vm servers and azure databases.
reduced the backlog of pending requests for reports in the organization. the company had no bi area to attend the requests and that caused a backlog of more than 140 pending requests. after joining the project, the number of requests got reduced to a constant of 35.
enhancing of the existing data warehouse solution
created ssis packages to extract data from ups quantum view for shipment information.
client discontinued proclarity as the analytical tool and adopted tableau as the new one. the migration of the most important workbooks for most users should be done before retiring proclarity.
coordinate 8 people team for creating ssis packages for demand management datamart implementing partitioning strategy in sql tables for data retention.
involved in designing and implementing database applications using ms sql server 2008/2012/2014 and
also involved in creating crm reports using visual studio 2010/2013.
created parameterized, drill down and aggregation reports in ms sql environment using ssrs 2012r2.
written complex t-sql queries and created stored procedures, triggers, functions and views.
designed, developed, and implemented etl process in ssis to collect daily, weekly and monthly information from sql data warehouse and create specific data marts for different olap models for supply chain area.
maintenance of olap cubes across supply chain area and micron.
designed and implemented olap cubes and dimensions for the tactical system information model using ssas.
detailed a plan for capacity and sizing of bi infrastructure in order to achieve high availability and optimum performance.
designed and developed a time tracking portal using access database and aspx pages to track project’s timeline and deliverables.
involved in designing and implementing database applications using ms sql server 2008/2012 and
created ssis packages using ssis designer for export heterogeneous data from ole db source (oracle),
data warehousing using data transformation services package (dts).
diplomat on information security engineering
phone:  1 (999) 999-9999  |  email:  consultant@gmail.com
pulled files from aws cluster and stored in elastic search using lambda functions and populating the data in kibana.
cicd consisted of jenkins server for continuous integration used with test-driven development (tdd) methodology.
arcelormittal - burns harbor, in 	jan 2015-jan 2016
created oozie workflow for various tasks like similarity matching and consolidation
driven, detail-oriented, with a passion for learning new skills and
time analytics, and distributed big data platforms.
|functional testing, design thinking,|linux, macos, microsoft windows     |
|lean, six sigma                     |                                    |
|                                    |mllib, graphx, scipy, pandas, rdds, |
|elasticsearch, apache solr, lucene, |camel, apache lucene, elasticsearch,|
|cloudera hadoop, cloudera impala,   |apache solr, apache drill, presto,  |
mastercard – miami, fl
chevron is using data in every aspect of communications, to predict actions
bachelor of science in engineering
university of central florida, orlando, fl
performed on the data.
methodology approach.
analyzed hadoop cluster using big data analytic tools including kafka, pig,
in hdfs & mongodb.
installed and running.
node, namenode recovery, capacity planning, and slots configuration.
carlos sturdivant  phone: (999) 999-9999  /    e-mail: consultant@gmail.com
netsuite, great plains, salesforce, sap, oracle, daceasy, ms office suite, quickbooks, windows linux
page 7
apache hive, apache kafka, apache maven, apache oozie, apache pig, apache
yarn, rdbms, dataframes, apache spark, apache oozie, apache kafka, hdfs,
pig, sqoop, oozie, zookeeper, cloudera manager, sentry, kerberos
• employed apache spark, spark api, and spark streaming, creating
queries in hadoop.
• import/export of data using sqoop between hadoop distributed file
• integrated hadoop with microsoft active directory and enabled kerberos
• implemented yarn resource pools to share resources of cluster for yarn
store the stream data to hadoop distributed files system (hdfs).
hadoop distributed file system (hdfs) data. from data lakes and
databases.
• worked with clients to better understand their reporting and dashboard
• import/export data into hadoop distributed file system (hdfs) and hive
integration and migration.
cassandra db.
• moving data from oracle to hadoop distributed file system (hdfs)  and
and staging data in hadoop distributed file system (hdfs) for further
from servers.
structured data to propel hr reporting and analytics capabilities
responsibilities.
pl, sql.
• supported data analysts by turning business requirements into
bachelor of science in economics, virginia state university -
• importing real-time logs to hadoop distributed file system (hdfs)
native, cloudera and hortonworks, spark, and hive.
• handling of large datasets using partitions, spark in-memory
analytics systems design, implementation and administration.  of this
← capable of building data tools to performance tune and optimize end-
← responsible for building data expertise and data quality for the
transfer pipelines for transformation and moving of data using flume,
hive, pig, sentry, ranger.
omnigrafflefilesavro, parquet
xmlstorage
optimization of the data storage in hive using partitioning and bucketing mechanisms on both the managed and external tables.
i managed, implemented and administered data flow of financial information flowing between the usa and mexico via banks and credit cards for paybook which handles us companies doing business with mexico.
maintained and backed up meta-data
dec 2013 bb&t – winston-salem, n.c.
i worked on optimization and building data pipelines as well as administration and workflows for bb&t.  the objective here was secure systems for processing data to inform risk management and guide lending decisions.
environment: hadoop cluster, hdfs, hive, pig, sqoop, linux, hbase, oozie, oracle, hdfs, mysql, teradata, zookeeper, beautifulsoup.
designed and developed ssis etl processes, test, deploy and quality assure the content
expert working in virtual environment using vmware for sql server.
hadoop developer/engineer
development of big data projects using hadoop, hive, hdp, pig, flume, storm, and apache open source tools/technologies.
hands-on experience installing, configuring and using hadoop ecosystem components like hdfs, hbase, avro, zookeeper, oozie, hive, hdp, cassandra, sqoop, pig, and flume.
experience in web-based languages such as html, css, php, xml and other web methodologies including web services and soap.
extensive knowledge of nosql databases such as hbase.
experienced in working with different hadoop ecosystem components such as hdfs, hbase, spark, yarn, kafka, zookeeper, pig, hive, sqoop, storm, oozie, impala, and flume.
expert with extraction of data from structured, semi-structured and unstructured data sets to store in hdfs.
rdbms and nosql; hbase, cassandra, realm, mongodb
use of jira/atlassian for project management. integration with gitlab.
created and populated tables in hive from s3 data sources stored as avro and parquet.
working with queries on sql server provided by the data team to build our fact and dimension tables from their data warehouse.
use of gitlab for ci/cd creating pipelines for automated testing and deployment of hive scripts.
mentored and supported team members on sdlc best practices and technical solutions.
environment: aws (emr, s3, iam, redshift, aurora, redshift spectrum, cluster steps, aws marketplace), hive, sql server, hadoop, hue, docker, spark, tez, java, scala, avro, parquet, ganglia, oozie, gitlab (ci / cd, pipeline schedules)
understanding of business rules, business logic, and use cases to be implemented.
experience in handling vsam files in mainframe to move them to hadoop using sftp.
created the shell script to ingestion the files from edge node to hdfs.
experience in writing hive join queries.
experience in writing the business logic for defining the dat, csv files.
experience in managing hadoop jobs and logs of all the scripts.
data engineer & hadoop architect/ september 2014 – september 2015
imported bulk data into hbase.
worked with avro data serialization system to work with json data formats.
worked with the data science team to gather requirements for various data mining projects
big data engineer & hadoop administrator, july 2013 – august 2014
stored non-relational data on hbase, worked extensively on that.
deployed the big data hadoop application using talend on cloud aws.
mapping the input web server data with informatica 9.5.1 and informatica 9.6.1 big data edition.
after the transformation of data is done, this transformed data is then moved to spark cluster where the data is set to go live on to the application using kafka.
the meter data and billing data was pooled into a warehouse storage system. an etl process was written to prepare the data for an application where the process was run to create a batch process and prepare data for analysis.
knowledge in installing and configuring hive, pig, sqoop, flume, and oozie on the hadoop cluster.
bi analyst and sharepoint administrator, july 2010 – december 2011
multitasking between various projects and ensuring the timelines are not being missed. so far have successfully delivered 7 major projects (including migration and rebranding) in a short span of 2 years
customized dashboards in power bi to make reports for stakeholders.
environment: powerbi, sharepoint, sql, data warehouse,
provide systems of geographical inquiry for citizenship in general.
excellent knowledge in understanding big data infrastructure, distributed file systems -hdfs, parallel processing - mapreduce framework and complete hadoop ecosystem - hive, hue, pig, hbase, zookeeper, sqoop, kafka-storm, spark, flume, and oozie.  in-depth understanding/knowledge of hadoop architecture and various components such as hdfs, job tracker, task tracker, name node, data node, and mapreduce concepts and experience in working with mapreduce programs using apache hadoop for working with big data to analyze large datasets efficiently.  in-depth knowledge of real-time etl/spark analytics using spark sql with visualization  hands-on experience on yarn (mapreduce 2.0) architecture and components such as resource manager, node manager, container and application master and execution of a mapreduce job.
linux, macos, microsoft windows
strong understanding of nosql databases and hands on work experience in writing applications on nosql databases like hbase, cassandra and elasticsearch.
kafka for streaming data ingestion to the spark distribution environment.
used jenkins with git for cicd integration.
hbase, cassandra
cloudera (cdh)
administered aws with iam privileges for aws security.
implemented modifications involving pyspark within a python application framework.
data was brought in a pyspark application running on an emr cluster for processing, to be output to aws s3.
technologies:  python, pyspark, aws s3, aws lambda, aws emr, google cloud storage
huntsville, al
bachelor of arts in economics
• data governance, security & operations experience.
spark for data processing.
sentry, ranger.
promotions.  the system analyzed erp, crm, conversions, social media, and
various disparate data sources.
•     experience in optimizing the data storage in hive using partitioning
•     performed import and export of dataset transfer between traditional
•     implemented yarn resource pools to share resources of cluster for
•     data ingestion is done using flume with source as kafka source & sink
may 2016    prism data systems – san antonio, tx
streams like advanced windowing, event correlation, event clustering,
•     built continuous spark streaming etl pipeline with spark, kafka,
•     worked with amazon web services (aws) and involved in etl, data
•     import/export data into hdfs and hive using sqoop and kafka.
involved in a project to build-out managed cloud infrastructure, with
pocs for inventory management, and optimization suing a variety of
jul 2012    prism data system – san antonio, tx
necessary.
education.
itt technical institute
sa community college, san antonio tx
cloudera and mapr.
experience in installation and configuration on spark.
good experience in understanding the client's big data business requirements and transform it into hadoop centric technologies.
hadoop components (hive, pig, sqoop, oozie, flume, hcatalog, hbase & zookeeper).
strong skill in hadoop hdfs architecture.
sqoop to import/export data from rdbms, databases, mysql, oracle into hdfs, and hive.
skilled in converting business requirements into concrete deliverables
sql server development (query optimization, stored procedures, etc.) and administration (fail over using high availability, mirroring techniques, jobs, database backups, etc.).
technical qualifications
big data etl /pipeline:  talend, informatica, flume, kafka, camel, apatar, atom, ketl, kettle, clover
configured deployed and maintained multi-node dev and test kafka clusters.
designed, developed and did maintenance of data integration programs in a hadoop and rdbms environment with both traditional and non-traditional source systems as well as rdbms and nosql data stores for data access and analysis.
developed hive queries to process the data and generate the data cubes for visualizing.
analyzed the sql scripts and designed the solution to implement using scala.
design and develop data ingestion component.
apache hadoop, hdfs, hive, pig, sqoop, spark, cassandra, hbase, sql, scala, vmware, eclipse, pig,  flume.
working knowledge in nosql databases like hbase and cassandra.
log file management; i.e., remove logs greater than 7 days old from log folder and load into hdfs.
led multiple edw projects, prototyped and evaluated the performance on azure cloud, aws amazon cloud, massively parallel processing (mpp) data warehouse appliance.
involved in requirement gathering, design, coding, testing, and deployment phase of the project.
created content types and defined the term sets using the managed metadata service.
interpreted business requirements and provided interpretation as technical requirements
developed custom web parts for individual project-based tasks leveraging javascript and css
5 years of experience in the it field
spark, scala, kafka, hdfs, hive
experience in distributed big data systems with hadoop, aws emr hadoop clusters
skilled in hadoop big data using cloudera (cdh) and hortonworks (hdp)
in depth understanding/knowledge of hadoop architecture and various components such as hdfs, job tracker, task tracker, name node, data node and mapreduce concepts and experience in working with
java, scala, python, unix shell scripting
big data etl tools & frameworks
hadoop, cloudera hadoop (cdh), hortonworks hadoop data platform (hdp)
hadoop cluster security
kibana dashboard designed over elasticsearch for visualizing the data.
complex sql, pl/sql
• preparation of test cases, documenting and performing unit testing and
|c++, c#, asp.net                    |sql, rdbms, nosql, hbase, cassandra |
|spark api, rest api, soap api       |cloudera, databricks, hortonworks,  |
present     ga power– atlanta, ga
• key role in migrating production and development hortonworks hadoop
• cloudera cluster using existing hardware and a short outage window.
my work for this bank was the underlying pipelines, etl system and
• wrote shell scripts to execute scripts (pig, hive, and move the data
• handled 20 tb of data volume with 120-node cluster in the production
industry.  i was placed on the project to work on an etl design and
platform (hadoop loader framework, big data spark framework etc.)
data related.  i was able to provide insights on data platform strategies
• successfully loaded files to hive and hdfs from oracle, sql server
aug 2011    honeywell – morris plains, nj
• managed third-shift tac employees, to ensure that nightly monitors and
issues, including application outages, system issues, and customer
corrective action as needed.
mar 2005    network engineer
apr 2007    computer sciences corporation – virginia beach, va
decommission navy legacy networks.
facilitate enterprise-wide solutions.
troubleshooting when related to department of defense policy.
such as analysis of cyber security/ ia issues and impact mitigation
design in order to facilitate peo fielding of material solutions to
• generated department of defense (dod) information assurance
project matters.
army national guard 67n/10 uh-1 huey mechanic, 68f/10 aircraft electrician
printers using a sophisticated computer system as a training vehicle.
structural specialist, you'll be trained to use specialized materials,
tools, and equipment, and you'll gain knowledge that will easily transfer
into a civilian career in the construction field.
faultfinding and the maintenance and repair of electronic equipment and
experience in the area of network managers/system administrators for the
master of science in information technology
the university of central missouri, warrensburg, missouri.
bachelor of science in electronics; minor in computer information systems
knowledge of financial services, electronic communications, finra/sec
file system (hdfs), and specialization in hadoop cyber security, kerberos,
• strong communication and collaboration skills, team lead and team player.
• researching and recommending machine learning algorithms and data sets
for advancing the state-of-the-art techniques for a variety of analytics,
e.g. word2vec, glove
• command of data science principles e.g. regression, bayes, time series,
sql server, hdfs
oozie, apache pig, apache spark, spark streaming, spark mllib, graphx,
google analytic, weka -software, microsoft excel vba, project and access,
|             |economic research service, usda, washington, dc             |
extracts, parameters, filters, calculations, context filters, data
dashboards.
• optimized hadoop with hive data storage partitioning and bucketing on
managed & external tables.
• in hadoop ecosystem, created hive external tables and hive data
hadoop pipeline.
balancing and glacier for our qa and uat environments as well as
(hdfs) & mongodb to build a continuous etl pipeline for real time data
• configured elastic lod balancers with ec2 auto scaling groups.
scala, hadoop distributed file system (hdfs).
• wrote shell scripts to execute pig and hive scripts and move the data
data integration and migration.
hive using sqoop and kafka.
•  built a prototype for real-time analysis using spark streaming and
unstructured data using hadoop, spark, hive for etl, pipeline and
elasticsearch, logstash, kibana, kinesis, cloudwatch.
•  streamed analyzed data to hive tables using sqoop, making available
•  connected various data centers and transferred data using sqoop and
•  imported data from disparate sources into spark rdd for data
the weinberg group specializes in fda regulatory marketing strategies.  big
data analysis is used to get a picture of the regulatory environments, use
configuration in hadoop.
• used linux shell scripts to automate the build process, and regular
• involved in loading data from linux file system to hadoop distributed
navigator.
benefits:
daemon services and respond accordingly to any warning or failure
• database design and development of large database systems: oracle 8i
and oracle 9i, db2, pl, sql.
• gathered and finalized requirements and data profiling analysis.
the applications.
spark mllib, spark streaming, etc.
and ideal precision/scale determination
studios to automate tasks.
• proficient in manual, functional and automation testing.
|languages:  , hive ql, python,      |apache cassandra, apache hbase,     |
|scala, pig latin,                   |mapr-db, mongodb, oracle, sql       |
|scripting: unix shell scripting     |compression utilities: snappy, gzip |
|aws, cloud foundry                                                        |
• gathered and documented detailed business requirements to identify,
during data processing, cleaning, and analysis.
quality standards.
• developed aws data pipeline, sns for automating the dunning process on
• wrote pig latin programs to manipulate data.
• loaded data into hbase for online lookups.
sqoop, and spark.  transforming and cleansing data for bi analysis.
failure conditions.
environment: maven, sql language, oracle, xml.
sep 2011    bi developer
masters degree in business administration
latin american university, mexico
hadoop big data including  azure, hdinsight, cloudera, hortonworks, impala
visualization: pentaho, qlikview, tableau, informatica, power bi
nosql database: apache cassandra, hbase, redshift, mariadb
proficient with extraction and transformation and storage of data in various file formats in azure data lake, azure warehouse and azure blob/azure blob store such as orc, parquet, json, as well as semi-structured data.
upgraded sql servers and data warehouse, and set-up virtual server farm using azure computer to establish a migration test and migration server.
microsoft corporation in seattle, wa
implemented real time processing using hdinsight and azure stream analytics.
worked on creating comprehensive mongodb api and document db api using azure stream analytics and azure data factory into azure cosmosdb.
created custom business rules to manage users and data transfer capabilities.
worked on cosmodb to map multiple values to either an update operation or an insert.
provided designed, implemented and configuring topics and partitions in new kafka cluster in all environments.
architected hadoop data lake for ingestion of batch data including logs, and data from structured databases as well as unstructured nosql sources with kafka.
successful in setting up, a no authentication kafka listener in parallel with kerberos (sasl) listener.
installed hdp in all environments.
involved working in production ambari views.
performed pre-migration activities such as cleaning up sites, removed unused elements, validated ad users, etc.
created proper documentation including visio diagrams for every major change in the configuration of services or new development to get a high level understanding of the approach taken.
designed a front-desk application that would printout client contracts, maps, and technical checklists.leon li
5 years i.t.
sql, hiveql, spark sql, cql
apache cassandra & hbase, aws redshift
cloudera, hortonworks, hadoop
hartford, ct
" i was in charge of the development team for spark, building the skeleton for spark jobs using technologies like s3, to merge datasets using complex transformations. in charge of hadoop administrative tasks like security configuration, rack awareness, integration with multiple technologies to secure the hadoop cluster, also creation of containerized applications using technologies to automate the process of instantiation.”
work using agile methodology to utilize tasks and delegated between team members
big data/aws engineer 	05/2017 - 06/2018
wrote spark sql queries and optimized the spark queries with spark sql.
etl to hadoop file system (hdfs) and wrote spark udfs.
hadoop developer	11/2014 – 02/2016
monitored workload, job performance and capacity planning using ambari.
ethan karlson
mgm las vegas
10 years in data related technologies. 5-8 years in a management position
scripts
waterfall, agile, budget forecast, resource allocation, documentation, scheduling, strategic planning, deliverables, dependencies, project plan, managing expectations, project charter, project scope document, test plans, risk management, governance, daily scrum, sprint planning, sprint review, sprint retrospective, sprint backlog, prioritizing tasks, time management, project metrics, reporting, staff management, onboarding, mentoring
drill down the data and maps.
developed visualizations using parameters, calculated fields, dynamic sorting, filtering, parameter driven analysis.
created various visualizations to analyze broker, claims and customer details.
lead analyst/bi project team lead
developed strategy for implementing data cleaning, data profiling, and data quality.
migrated existing cognos reports to tableau and ms power bi embedded in applications by replacing semantic layer.
expert evaluation of data models to design tableau solutions.
expedia | bellevue, wa	august 2011 – september 2013
developed numerous ad-hoc queries and reports to collect and present information that satisfied clients and contributed to achieve organizational goals.
business analyst
syntel, inc. – phoenix, arizona	may 2008 – july 2011
more than 6 years big data experience
managed project backlog through jira.
evaluated a possible switch to faust from kstreams and made recommendations
successfully integrated kstreams to consume/produce to rest api
the project follows a strategic initiative in chemical pricing strategy for this dow spinoff company to improve market share and profitability.  chemours manufactures and sells performance chemicals falling within three segments: titanium technologies (titanium dioxide); fluoroproducts (refrigerants and industrial fluoropolymer resins and derivatives including freon, teflon and viton); and chemical solutions (cyanide, sulfuric acid, aniline, methylamines, and reactive metals).
loaded rdbms of large datasets to big data by using sqoop.
developed mapreduce programs from scratch of moderate complexity.
zookeepertarball, configured zookeeper ensemble of 3 nodes in standalone and multi-node cluster
bachelor of science computer science
5+ years specializing in hadoop/big data
java, python, scala, c, c++, c#, vb.net, asp.net
mapreduce, hive, hive ql, rdds, dataframes, datasets
agile team consisted of associate, junior and senior developers, team lead, business analyst, scrum master and technical product owner.
heavily paired with qa teams to integrate testing frameworks and suites into the application and ci/cd pipeline to ensure high quality deliverables that were thoroughly tested.
used google kubernetes engine to host and deploy our application as a kubernetes pod running our containerized application.
created a robust workflow management system that allowed users to both bring their own airflow dag/workflow files or use our specially made file templating system as to enable custom workflows.
cummins inc., columbus, in
cummins designs, manufactures, and distributes engines, filtration, and power generation products.  this project focused on cummins application of big data, statistical analysis and iot to analyze existing component or system reliability.
iot device analytics platform; researched and codified the kafka consumer using kafka consumer api 0.10 and kafka producer api 0.10(java); designed the spark streaming and kafka producer interfaces - for multithreaded partitions and multiple topics by smartphone manufacturer device type; competitive analysis of storm, spark, flink, samza for processing messages, replay and lost message management, horizontal scalability, security, message sequencing;
custom kafka broker design to reduce message retention from default 7-day retention to 30- minute retention - architected a light weight kafka broker.
s3, lambda, kinesis
wrote documentation for legacy support of finished projects.
worked with a rapidly growing team that had members in remote, international locations.
communicated with the team daily to provide updates on current projects.
worked in an agile scrum environment, participating in daily scrums, sprints, sprint planning, etc.
participated in various phases of data processing (collecting, aggregating, moving from various sources) using apache spark
aar corporation - wood dale, il
a+ #comp001021317420
• the last 5 years as big data architect and engineer
hortonworks, cloudera, apache distribution hadoop & cassandra.
for 'expectations management' & 'perceptions management'.
components
apache pig, apache spark, apache tez, apache zookeeper, cloudera impala,
|operating systems                 |server                            |
✓ created an enterprise data lake program leveraging apache hadoop
✓ prepared the solution design document and get it reviewed.
✓ scheduled daily jobs in oozie, job dependency manager and airflow.
✓ involved in writing the mr script to deposit the intake data on
✓ near real-time streaming and analytics using kafka, spark streaming
(hot-storage) database.
✓ load and transform large sets of structured, semi structured and
✓ involved in loading data from unix file system to hdfs.
✓ job management using fair scheduler.
✓ responsible for defect management includes, defect logging, defect
✓ maintained versions for system testing, data driven and regression
and regression testing.
✓ maintained and improved existing hp alm scripts in scale with
and existing application.
✓ performed functional, integration, regression and end-to-end testing
✓ coordinated the uat testing by guiding the users during uat.
helpful for management to understand overall testing status of the
email:  consultant@gmail.commckinley kareem
s
hands on experience in writing pig latin scripts, working with grunt shells and job scheduling with oozie.
analytics & visualization
datastage, apache sqoop, apache camel, flume, apache kafka, apatar, atom, talend, pentaho
it infrastructure security protocol, matching security threats, virtualization, dns, tcp/ip, cifs, smb,  infrastructure planning, system monitoring tools, iis, scom, san, nas,  security auditing, active directory, anti-virus systems/malware removal, remote desktop assistance, application security and firewalls
infrastructure:  cloudera hadoop, hdfs, impala, hive, pig, spark, spark streaming, sql, python, sqoop, kafka, flume, tableau.
involved in full life cycle of the project including logical and physical architecture modeling, implementation, testing, launch and training.
created variable data formats for data storage in online repositories using hadoop data lake, hbase, cassandra and redshift.
infrastructure: hadoop, aws iot, spark, aws s3, redshift, athena, neo4j, dynamodb, aurora, qlikview, elk (elasticsearch, logstash, and kibana on ec2), python
installed and configured various components of the hadoop ecosystem, and added apache hadoop clusters and configured etl pipelines and tools on the cloud to capture data from various iot systems.
implementation of hadoop data lake on amazon aes cloud service.
worked on developing user defined functions (udfs) in hive to transform the large volumes of data with respect to business requirement. involved in creating udfs in hive like - simple udf, udtf, udaf.
etl (extract, transform, load) large sets of structured, semi-structured and unstructured data and analyzed them by running hive queries and pig scripts.
infrastructure:
•	responsible for building scalable distributed data solutions using hadoop.
•	used hive, spark sql connection to generate tableau bi reports.
•	created hive generic udf's to process business logic that varies based on policy.
•	developed metrics, attributes, filters, reports, dashboards and also created advanced chart types, visualizations and complex calculations to manipulate the data.
•	used sqoop to efficiently transfer data between databases and hdfs and used flume to stream the log data from servers.
hill, inc., atlanta, ga
configured tcp/ip, dns, cifs, smb.
preformed successful migration from vsphere 5.1 to 5.5 migrating all vms to temporary locations and then back to new 5.5 environment
perform scheduled configuration changes to network. create and maintain standard operating procedure documentation for systems and network policies and procedures.
monitored event viewer for errors and problems. kept all workstations and servers up-to-date with all security patches. upgraded workstations and servers as needed. maintained ntfs security access.
5 years of experience in administrating and managing oracle database, cloud and big data environment for multinational companies in financial, telecom and information technology.
mapr
hadoop (hdfs, yarn, mrv2), flume, sqoop, zookeeper, pig, hive, hbase, oozie, apache nifi, kafka, storm, knox, kerberos, apache ranger
cloud data center solutions
puppet
amazon redshift, oracle, mysql
google dataproc
experience
responsibilities
integrated bi tools for ad hoc queries, reporting and visualization using tools such as tableau, and qlikview.
transferred etl workflow to processes from hive to redshift
developed pig latin scripts to extract the data from the web server output files to load into hdfs
integrated zeppelin notebook with spark emr for developer use.
performed performance tuning and troubleshooting by analyzing and reviewing hadoop log files.
etl data cleansing, integration & transformation using pig: managing data from disparate sources.
for benchmark testing and general functional comparison.
march 2012 to march 2014
chicago transit authority (cta) is the second largest public transportation system in the nation and covers the city of chicago and surrounding suburbs. with two modes of transportation – bus and train – cta provides 1.64 million rides per day on average.
strong experience in resource and workload management to configure yarn capacity and fair scheduler based on organizational needs.
good working knowledge of open source configuration management and deployment tools such as puppet and python.
leveraged pig for transformation and processing of unstructured and semi-structured data.
monitor production cluster by setting up alerts and notifications using metrics thresholds.
highly skilled in troubleshooting and finding root causes.
kerberized the cluster for user authentication.
utilize new and latest open source tools for addressing business challenges.
prepare documentations and specifications.
lautech group technology
may 2010 to march 2011
worked both independently and in a team-oriented collaborative environment.
used different mathematical and computational algorithms such as neural network, k-means, association rules, naïve bayes to unlock various insights and make future forecasts.
developed aws strategy, planning, and configuration of s3, security groups, iam, ec2, emr and redshift.
involved in continuous integration of application using jenkins.
experienced in amazon web services (aws), and cloud services such as emr, ec2, s3, rds and iam entities, roles, and users.
r-programming, matlab, c++, c#
elasticsearch, logstash, kibana
cloudera cdh, hortonworks hdp, amazon web services (aws)/amazon cloud
aws iam formation, aws redshift, aws rds, aws emr, aws s3, ec2,  aws lambda, aws kinesis, aws elk, aws cloud
tableau, power bi, excel, kibana
professional experience
implement ci/cd tools upgrade, backup and restore
versioning with git and set-up a jenkins ci to manage cicd practices.
built jenkins jobs for ci/cd infrastructure from github repos
atlanta, ga
ingestion data through aws kinesis data from various sources to s3.
implemented spark in emr for processing big data across our data lake in aws system.
aws kinesis used for real time data processing.
experienced in importing real-time logs to hdfs using flume.
transferred data between a hadoop ecosystem and structured data storage in a rdbms such as mysql using sqoop.
managed cluster using ambari
overall gpa: 3.95/4.0
accessing hadoop data using hive (ibm)
oozie, cloudera manager, ambari, zookeeper, active directory, powershell
capital one
summary: the email tool was designed to take in a list of people to send an email to, and send it to them individually instead of doing a mass mail. the business  reason for this was that mass emails are easier to ignore, and they frequently result in reply-all chains that serve to annoy users.
developed custom aggregate functions using spark sql and performed interactive querying.
involved in converting hiveql/sql queries into spark transformations using spark rdds, python and scala.
iot sensors in the vehicles stream a wealth of data to the cloud and to smartphones.  it is available to users and the maintenance shops.   engine controls and microprocessors that were around regulating fuel delivery, and things like oxygen sensors for fuel economy have been upgraded to the  sync3 system.  with the ability to update the sync system in the vehicle remotely with software that can download from the cloud. so there’s all kinds of flexibility and capability that you get when you move to off-board computing and storage capabilities.  through big data systems in the cloud that store and allow real-time processing of data ford is able to identify key factors in predictive maintenance, input triggers through a custom algorithm to enable up-to-date and timely maintenance before a breakdown can occur.
developed a task execution framework on ec2 instances using sql and dynamodb.
us xpress
involved in migrating mapreduce jobs to spark, using spark sql and dataframes api to load structured data into spark clusters
created custom test, design and production spark clusters for the veruca - verizon universal communications
created variation of the lambda architecture consisting of near real-time using spark sql; spark cluster 1.4 consisting of 25 nodes running with 200gb ram/24 tb,
implemented all scd types using server and parallel jobs. extensively implemented error handling concepts, testing, debugging skills and performance tuning of targets, source, transformation logics and version control to promote the jobs.
involved in loading and transforming large sets of structured, semi-structured and unstructured data.
institute for technology & management, chennai
cycle, apache camel, apache flume, kafka, apatar, clover, and others.
types such as sql, rdbms, data lake, data storage, data mining.
hdfs for analytic purposes.
▪ involved in building a multi-tenant cluster.
▪ skilled in data analysis and forensic analysis using hadoop tools,
|css, bash                           |hadoop ecosystem                    |
data using aws lambda, kinesis, and streamsets.used tendo cli to export
• built aws lambda functions for raw data extraction, schema
• developed kinesis-to-redshift projects via jdk and maven builds.
cases. these included provisioning new, cloud-based data hubs and creating
automated etl workflows from older databases, etl workflows for information
• worked with sales analysts and 4 data engineers to create a data hub
• created joiner tables for mapping between unconnected primary keys
• loaded datasets from google analytics to bigquery for export
• creation of hive tables, loading with data, and running jobs in the
hdfs.
• worked with engineers to configure apache spark & spark streaming.
• used spark sql and dataframes api to load structured and semi-
structured data into spark clusters.
cluster handling.
• responsible for maintenance and upgrades to the cloudera hadoop
• managed hadoop authentication by integrating hadoop with active
• designed and presented a poc on introducing impala in project
• worked on importing the unstructured data into the hdfs using spark
• scheduled and executed workflows in oozie to run hive and pig jobs
• created partitions, buckets based on state to further process using
appropriate bucket permissions.
• apache kafka to transform live streaming with the batch processing to
scala, hdfs, and mongodb.
• developed metrics, attributes, filters, reports, dashboards and also
created advanced chart types, visualizations and complex calculations
storm, cloudera, impala
sealedair is a global corporation involved in food safety, security,
dynamodb.
• used the hive jdbc to verify the data stored in the hadoop cluster.
and business critical decision processes in the oil and gas industry.  big
exploration across the industry.  the industry relies heavily on oils and
used flume to stream the log data from servers.
determine standards.
workflows.
• the application obfuscates and creates a hash code to allow personal
processing and labor cost while decreasing the quote to cash time.
• developed a next-generation threat intelligence application to
• utilized machine learning to map normal member behavior.
• decreased labor cost needed to investigate by eighty percent.
systems and applications.
• company a significant amount of money.
• standardized active directory roles and groups for all titles allowing
seconds.
scanners did not detect.
propriety application workarounds, changes, and deployments.
• wrote ansible scripts to deploy nagios agents to the infrastructure.
taught a spark class at a p&g “data engineering university” in warsaw,
various applications of spark, and how to use it in the big data
candidate:  gcfe – giac forensic examiner certification
[pic]
experience with data ingestion from various sources using apache flume and kafka.
work closely with management and team to understand the existing systems and recommend for cicd automation.
adept at high performant data processing and pipelines using spark, spark sql, spark streaming, spark structured streaming, kafka
installation, maintenance and monitoring of hadoop clusters, kafka clusters, online and on premise; experienced in use of ambari.
created build and deploy plans for bamboo
used bitbucket for git versioning
coordinated with 3rd party vendors and other teams to ensure data integrity.
dupont
hands-on work with aws emr and s3.
omaha, ne
imported/exported from various sources to hdfs to build data lake.
used unix shell scripting to automate common tasks.
performance tuning of spark, issued by data skewness.
experience in a variety of industries including healthcare and finance; familiar with pci and pii regulations.
spark, spark streaming, spark, spark sql, scala, python, pyspark, pytorch, java, javascript, sql, hive ql, mapreduce, shell scripting, ruby, groovy, c#, c++, c
aws rds, aws emr, aws redshift, aws s3, aws lambda, aws kinesis, aws elk, aws ec2, aws iam, aws cloudformation
spark sql to create real-time processing of structured data with spark streaming processed through micro-batching.
responsible for continuous monitoring and managing elastic mapreduce (emr) cluster through aws console.
worked on multiple aws instances, set the security groups, elastic load balancer and amis, auto scaling to design cost effective, fault tolerant and highly available systems on aws.
data engineer 	atlanta, georgia
performed hive partitioning, bucketing, performing joins on hive tables.
hands-on experience in working with job scheduling with oozie.
created unix shell scripts to automate the build/test/deploy process, and to perform regular jobs like file transfers.
digital world’s institute	may 2014 - june 2016
university of florida
oregon state university
list certs here
services
experience with nosql databases including hbase, cassandra and elasticsearch.
maintained elk (elastic search, kibana) and wrote spark scripts using scala shell.
spark
spark streaming
cicd
ambari
wrote and automated scripts for elasticsearch, logstash and kibana
developed new flume agents to extract data from kafka and more into hadoop file system (hdfs).
developed end to end hive queries to parse the raw data, populated external & internal tables and store the refined data in partitioned external tables
migrated the data using sqoop from hdfs to relational database system.
configure and ensure connection to aws rds database running on mysql engines.
responsible for building servers on aws, and have ansible configure, security management
extensively worked on creating terraform files for building aws sqs queues, ecs clusters, auto scaling groups, sns, lambda, dynamo db and cloud watch event as infrastructure.
5 years in hadoop/big data & 5 years in information technology
familiarity with profiling and tuning sql execution plans.
works with cluster subscribers to ensure efficient resource usage in the cluster and alleviate multi-tenancy concerns.
apache cassandra, aws dynamob, mongodb, arangodb, auroradb, redshift, amazonrds, sql, mysql, nosql, oracle, db2.
open source distributions
data mining
datacleaner, winpure data cleaning tool, patnab, openrefine, drake
__________________________________________________________________________________________
applied cloudwatch boto3 api to create dashboard that shows sagemaker training job metrics.
filebeat setup & configuration, devops activity.
transferred streaming data from different data sources into hdfs and hbase using apache flume.
hadoop big data engineer		january 2016 – aug 2017
real time/stream processing apache storm, apache spark
involved in running hadoop jobs for processing millions of records and data which was updated daily/weekly.
created modules for spark streaming in data into data lake using storm and spark.
designed and implemented test environment on aws.
created a poc involved in loading data from linux file system to aws s3 and hdfs.
performed sentiment analysis using text mining algorithms to find out the sentiment/emotions & opinion of the company/product in the social circle.
university of colorado, boulder
performance tuning of spark jobs in hadoop for setting batch interval time, level of parallelism, and memory tuning, and changing the configuration properties, and using broadcast variables.
apache kibana, tableau, microsoft power bi, cognos, sap
hadoop data engineer
extracted data from different locations and scheduled oozie workflows to execute the task from a web application.
active in the full software development lifecycle (sdlc) including requirements gathering, data analysis, development, and testing.
iexperienced in running hadoop streaming jobs to process terabytes of xml format data.
imported real-time logs to hdfs using flume and spark.
hands on experience on fetching the live stream data from db2 into hbase table using spark streaming and apache kafka.
may 2015 to october 2016
experience in configuring zookeeper to coordinate the servers in clusters to maintain the data consistency and to monitor services.
used kafka producer to ingest the raw data into kafka topics run the spark streaming app to process clickstream events.
involved in a project focused on insurance risk analysis and sales, creating pipelines and dashboards or actuaries, and data scientists to use for predictive analytics.
august 2012 to may 2014
general electric iq
transformed the logs data into data model using pig and written udf functions to format the logs data.
loaded and transformed large sets of structured, semi-structured, and unstructured data.
developed job processing scripts using oozie workflow to run multiple spark jobs in sequence for processing data
big data 101
lead hadoop data engineer
provide end-to-end data analytics solutions and support using hadoop systems and tools on cloud services as well as on premise.
design and develop cloud-based data solutions
apache cassandra, dynamodb
apache hbase, amazon redshift
protocols
implemented spark quantum set-up including preparation of json workflows, spark set-up, aws deployment, and workflow demo creation.
aws rds creation using cloud formation.
deployed and tested rds database in aws.
conducted kibana training for the team.
april 2015 -
used spark as an etl tool to remove the duplicates from the input data, apply certain joins and aggregate the data which intern is provided as an input to the twitter package to calculate the time series for anomaly detection.
developed a pipeline that runs once a day which does a copy job.
used pentaho to showcase the hive tables in an interactive way of pie charts and graphs.
worked on putty and jupyter note book to run the spark sql commands.
implemented a data analytics system for collection and analysis of global marketing data to be used by sales.  by utilizing big ata analytics the company was able to reduce wasted time and effort by sales to get better marketing intelligence and create a more efficient sales process.
architected hadoop system pulling data from linux systems and rdbms database on a regular basis in order to ingest data using sqoop.
analyzed the data by performing hive queries (hiveql), impala and running pig latin scripts.
involved in writing pig scripts for cleansing the data and implemented hive tables for the processed data in tabular format.
duluth, ga
implemented logistic regression in mapreduce to find the customer's claim probability and k-means clustering in mahout to group customers with similar behavior.
september 2007-
pc and printer support and repair
managed weekly client meetings
accenture technology solutions.
october 2006
maintained and developed price lists for bellsouth.
trained, managed and supported 2 offshore recourses.
developed validation pl/sql scripts to verify setups.
wrote test scripts to verify changes made meet customer requirements.
defect fixing
analyzed peoplesoft rem tickets to help determine fall out issue.
pushed the order to completion in workflow.
analyzed identified, and fixed adsl accounts issues so customers were billed and surfed correct
• expertise in python and scala, user-defined functions (udf) for hive and
apache, solr, lucene, databricks, drill, presto
hadoop ecosystem components
from streaming services.  streaming royalties are the future of artists'
• maintained end to end ownership for analyzed data, developed
spark streaming app to process clickstream events.
• performed data analysis and predictive data modeling.
• used pig and hive, and imported data using sqoop to load data from
bucketing concepts in hive needed for optimization.
pig latin scripts to study customer behavior.
parsed data into hbase and hive using hbase-hive integration.
• used hbase to store a majority of data which needs to be divided based
distribution, and data processing.
• involved in creating hive tables, loading with data and writing hive
hdfs using flume.
output directory into hdfs.
format to facilitate effective querying on the log data.
involved in building data warehousing/storage solutions along with data
formats into hadoop hdfs in preparation to cleansing and analysis.
• designed an archival platform, which provided a cost-effective
• analyzed the data originating from various xerox devices and stored it
on user comments. created reports and documented various retrieval
• worked with phoenix, a sql layer on top of hbase to provide sql
interface on top of no-sql database.
environment: jquery, javascript, css, bootstrap, hadoop, hdfs, hive,
oracle, oozie, qlikview, pentaho
sept 2008   big data developer
aug 2012    lockheed martin – new orleans, la
• collected stats every week on the tables to improve performance.
session log files and bad files to
check the data.
david
apache ant, apache cassandra, apache flume, apache hadoop, apache hadoop yarn, apache hbase, apache hcatalog, apache hive, apache kafka, apache maven, apache oozie, apache pig, apache spark, spark streaming, spark mllib, graphx, scipy, pandas, rdds, dataframes, datasets, mesos, apache tez, apache zookeeper, cloudera impala, hdfs, hortonworks, apache airflow and camel, apache lucene, elasticsearch, elastic cloud, kibana, x-pack, apache solr, apache drill, presto, apache hue, sqoop, kibana, tableau, aws, cloud foundry, aws, azure, anaconda cloud, elasticsearch, solr, lucene, databricks, mapreduce
soft skills
hadoop data architect/engineer
credit card fraud at capital one. joined data from fraud reports, transaction reports, authorization reports to produce output finding other transactions that are potentially fraudulent also.
worked in an aws cloud environment hosting hadoop clusters along with redshift and cassandra clusters, and aws s3.
use spark ‘cache and coalesce’ to persist data frames through multiple iterations without need of saving to disc.
performance tuned spark jobs according to the cluster size used.
used aws file mover lambdas to transfer files from one zone to another.
worked on disaster management with hadoop cluster.
zurich n.a. – schaumburg, il
migrated complex programs into apache spark rdd operations.
implemented partitioning, dynamic partitions and buckets in hive for increasing performance benefit and helping in organizing data in a logical fashion.
scheduled and executed workflows in oozie to run hive and pig jobs
used hive, spark sql connection to generate tableau bi reports.
collected the business requirements from the subject matter experts like data scientists and business partners.
involved in design and development of technical specifications using hadoop technologies.
used shell scripts to dump the data from mysql to hdfs.
consumed the data from kafka queue using storm
involved in loading data from linux file system to hdfs.
imported data using sqoop to load data from mysql and oracle to hdfs on regular basis.
used zookeeper for providing coordinating services to the cluster.
software engineer iii
harris corporation – annapolis junction, md
harris corporation – greenbelt, md
developed 3d modeling software (realsite); utilizing this software and digitalglobe imagery developed a 3d model of baghdad and other areas for the army.
developed software for raptormist to exploit weaknesses in waveforms. developed a demo which was conducted over a several hundred-mile-area to show our results.
jan 1985	feb 1990
developed software for signal processing and analysis systems including some embedded devices where there was no os. a number of these applications where developed using table driven software to enhance maintainability.
hadoop training
security clearance
expert with bi tools like tableau and powerbi, data interpretation, modeling, data analysis, and reporting with the ability to assist in directing planning based on insights.
knowledgeable of database technologies and frameworks involving structured data, unstructured data, and semi-structured data as well as various storage formats such as rdms and data lakes.
agile, kanban, scrum, devops, continuous integration, test-driven development, unit testing, functional testing, design thinking, lean, six sigma
hadoop, cloudera hadoop, hortonworks hadoop
cloud services:
programming methodologies:
configured access for inbound and outbound traffic rds db services, dynamodb tables, ebs volumes to set alarms for notifications or automated actions on aws.
created multiple batch spark jobs using java.
experience with data visualization tools, data analysis, and business recommendations (cost-benefit, invest-divest, forecasting, impact analysis).
database skills
data lake, data warehouse, sql database, rdbms, nosql database, amazon redshift, apache cassandra, mongodb, sql, mysql, oracle, and more
apache lucene, elasticsearch, kibana, apache solr
cloudera cdh 4/5, hortonworks hdp 2.5/2.6, amazon web services (aws)
mckesson
april 2016 – august 2017
aws cloud services planning, designing and devops support like
sept 2013 – march 2015
automated all the jobs for pulling data from ftp server to load data into hive tables, using oozie workflows.
used log4j for the logging the output to the files.
hadoop enterprise architect
dedicated and seasoned big data professional with skill in implementing and improving big data ecosystems using hadoop, spark, microsoft azure, amazon aws, cloudera, hortonworks, mapr, anaconda, jupyter notebooks, and elastic.  proficient in etl and data pipeline methods and tools.
developed scripts and automated data management from end to end and sync up between all the clusters.
served as senior engineer in team of four engineers. also worked with various engineering and business teams in other card segments to advise them on credit policy creation.
worked on-site working across several teams with senior and junior engineers
implemented acceptance test-driven development (atdd) test suites using cucumber
created hands-on training labs to train other teams on how to create upmarket credit policies for the inner-sourced knowledge engine
•	created automated tools to improve the team workflow, eliminating costly and error-prone menial work.
•	published extensive team-facing and client-facing documentation for applications on confluence.
implemented owasp security standards to combat common vulnerabilities, such as injection, cross-site scripting (xss), and sensitive data exposure.
citizen’s bank/new vantage partners – bridgeport, ct
assisted analysts in creation of sql and hiveql queries to test workflows and verify input and output data
resolved project issues stemming from missed deadlines, changing business requirements and project scope, analyst absences, and missing or incomplete data
wrote detailed technical documentation for project systems and supporting processes
created and presented slides to business personnel that explained complex technical details in an easy-to-understand manner
comcast xfinity – philadelphia, pa
analyzed hadoop cluster using big data analytic tools including kafka, pig, hive, spark
loading data from diff servers to aws s3 bucket and setting appropriate bucket permissions.
worked with clients to better understand their reporting and dash boarding needs and present solutions using structured waterfall and agile project methodology approach.
july 2011- july 2012
worked on hive for exposing data for further analysis and for generating transforming files from different analytical formats to text files.
utilized big data (nosql/hadoop) technologies to create in-house support applications.
william a thorndike
mllib, spark graphx.
experience in importing and exporting data using sqoop and sftp for hadoop to/from rdbms.
expertise with the tools in hadoop ecosystem including hdfs, pig, hive, sqoop, storm, spark, kafka, yarn, oozie, zookeeper etc.
hive, pig, zookeeper
continuous integration (ci cd):  jenkins
hortonworks data platform (hdp)
aws lambda, aws s3, aws rds
developed spark applications for the entire batch processing by using scala.
experienced in implementing spark rdd transformations, actions to implement business analysis.
expertise in aws data migration between different database platforms like sql server to amazon aurora using rds tool.
experience in working on aws kinesis for processing huge amounts of real time data.
application development using hadoop ecosystems such as spark, kafka, hdfs, hive, oozie and sqoop.
used spark-sql to load parquet data and created datasets defined by case classes and handled structured data using spark sql which were finally stored into hive tables for downstream consumption.
monitored hadoop cluster using tools like nagios, ganglia, ambari.
master’s of computer science
macomb, illinois
• experience on importing and exporting data using flume and kafka.
• processing this data using spark streaming api with scala.
integrating apache nifi and apache kafka.
• extensively worked on build tools like maven, log4j, junit and ant.
high-level documentation.
• created hive, phoenix, hbase tables and hbase integrated hive tables
• imported required tables from rdbms to hdfs using sqoop and also used
queue)
• started using apache nifi to copy the data from local file system to
implement business analysis and worked with spark accumulators and
supporting documents for proper deployment.
worked on the project for verisign using big data analytics to manage
more.
• analyzed large sets of structures, semi-structured and unstructured
custom udfs, and hive-based exception processing.
flume and set destination as hdfs.
perform regular jobs like file transfers between different hosts.
division.
• uploaded and processed more than 30 terabytes of data from various
• experience working on solr to develop search engine on unstructured
• created ssis packages to extract data from oltp and transformed to
data processing.
nifi, hive, hbase, pig, oozie, sqoop, flume, hue, tableau, scala, spark,
connecting to hadoop cluster and cassandra ring and executing sample
implementation
utilizes struts, spring, jsp and oracle database.
status reports.
healthcare industry.
implemented the dao and service classes.
• wrote spa (single page web applications) using restful web services
six sigma
storm, hive, kafka, spark to customize big data analytics solutions.
• microsoft certified solution associate windows server 2012
research and present potential solutions for current eds platform in relation to data integration and visualization and reporting.
worked with various file formats (delimited text files, click stream log files, apache log files, parquet files, avro files, json files, xml files)
apache cassandra, datastax cassandra, apache hbase, apache phoenix, bigsql, couchbase, db2, mariadb, mongodb, ms access, oracle, rdbms, sql, sql server, apache toad
talend, scriptella, ketl, pentaho kettle, jaspersoft, geokettle, cloveretl, hpcc systems, jedox, apatar
in spark batch processing with scala, i was responsible to calculating the interchange amount between capitalone, visa and mastercard.  this was accomplished by developing a spark application to consume a set of very large files of various formats(avro, parquet and csv), and process them in accordance with quality and security standards such as unit testing, atdds, licensing and vulnerabilities.
disaster recovery
evaluated systems and provided enhancements by designing, documenting and collaborating with teams across the organization.
started flink yarn-session to provide enough resources for all elements being process.
created a poc involved in loading data from linux on premises ecosystem to amazon s3 using redshift, dynamodb, and hdfs using mongodb and cassandradb (hortonworks hadoop).
process automation
created hbase tables to store variable data formats of data coming from different portfolios.
used sqoop job to import the data from rdbms using incremental import.
wrote spark codes to run a sorting application on the data stored on aws.
used spark sql and dataframes api to load structured data into spark clusters
developed spark scripts by using scala shell commands as per the requirement.
executed tasks for upgrading clusters on the staging platform before doing it on production cluster.
windows and vmware administrator
windows server 2000, 2003, 2008, 2008 r2, 2012 and 2012 r2
resume:
phone: 949-273-2822
experience data ingestion from various sources using apache flume and kafka.
 elasticsearch, logstash, kibana, kafka, zookeeper etc.
 worked on streaming the processed data to dynamodb using spark for making it available for visualization and report generation by the bi team.
 spark sql to create real-time processing of structured data with spark streaming processed through structured streaming.
 log monitoring and generating visual representations of logs using elk stack. implement ci/cd tools upgrade, backup and restore
cloudera hadoop upgrades and patches and installation of ecosystem products through cloudera manager along with cloudera manager upgrade.
implemented kafka messaging consumer
broadcast variables in spark, effective & efficient joins, transformations.
may 2014 - sept 2015
support for the clusters, topics on the kafka manager and kafka/hadoop upgrades on the environment.
5+ hadoop engineering and 7 yrears total information technology /database
managed architecture and  integration of real-time systems processing and near real-time processing using apache spark, sprk streaming and apache storm.
pentaho, qlikview, tableau, informatica, power bi
query engines
hive, pig, spark, spark streaming, storm,
fetched live stream data from iot data sources and streamed to hadoop data lake using spark streaming and apache kafka
created materialized views, partitions, tables, views and indexes.
created analytics system for marketing and inventory analysis for this home improvement store chain.  having experience in this area, i was brought onto this project to assist in designing and implementing an analytics system.
analyzed large sets of structures, semi-structured and unstructured data by running hive queries and pig scripts.
certification: aws solutions architect
email:  natallialaurova009@gmail.com
experience working in hadoop-as-a-service (haas) environments, subversion (svn), and sql and nosql databases
clearly documents big data systems, procedures, governance and policies.
architecture of big data systems:
hive, pig, zookeeper, sqoop, oozie, yarn, maven,  flume, hdfs, apache airflows
spark framework
interfaced with engineers, product managers and product analysts to understand product goals and data needs.
configured elasticsearch, log stash and kibana (elk) for log analytics, full text search, application monitoring in integration with aws lambda and cloud watch.
customink, tysons corner, va   	dec 2013-jan 2015
able to architect and build new data models that provide intuitive analytics to customers.
wells fargo bank – san francisco, ca
heber fragoso / sharepoint / .net  /  phone: 413-301-9827    /    e-mail: fragosomezaheb@gmail.com
spark used in optimizing etl jobs to reduce memory and storage consumption.
programming/scripting
used kibana for dashboards and reporting to provide visualization of log data and streaming data.
developed etl pipeline to process data from sequence file and saved to hive tables in orc format.
worked on spark sql to clean up data tables.
worked on tickets related to various hadoop/big data services which include hdfs, yarn, hive, sqoop, spark, kafka, hbase, kerberos, ranger, knox.
the primary analyst of a 2 person team responsible for repairing databases for the company’s #1 selling software package in north america.
“being in the military, i learned a number of skills including time management, problem solving, and teamwork.  traveling to different duty stations across the world introduced me to many cultures that i would never have experienced.”
deployed routers, switches, and servers in various locations throughout the world.
performed troubleshooting of computers, networks, and software.
strong knowledge in upgrading mapr, cdh and hdp cluster.
linux/unix shell scripting, sql, mysql, nosql, html5, css3, visual basic, hive ql, python, scala, cobol, xml, blueprint xml, ajax, rest api, spark api, json, avro, parquet, orc, jupyter notebooks, eclipse, intellij, pycharm, c#, r, java ,r, angular, javascript, html/css, hibernate, spring,  .net core with c#, asp.net, mvc
used the json and xml serde's for serialization and de-serialization to load json and xml data into hive tables.
environment: hadoop, hdfs, spark, hive, sqoop, kafka, hbase, oozie, flume, scala, aws, python, java, json, sql scripting and linux shell scripting, avro, parquet, hortonworks.
in depth understanding/knowledge of hadoop architecture and various components such as hdfs, application master. node manager, resource manager, namenode, datanode concepts.
used hive to analyze the partitioned and bucketed data and compute various metrics for reporting.
used hive optimization techniques during joins and best practices in writing hive scripts using hiveql.
involved in evaluation and analysis of hadoop cluster and different big data analytic tools including pig, hbase database and sqoop.
install security using kerberos on cluster for aaa (authentication, authorization and auditing).
importing of data from various data sources, parse into structured data region wise and date wise. analysed the data by performing hive queries and running pig scripts to study customer behaviour.
professional highlights
provides clear and effective testing procedures and systems to ensure an efficient and effective process.
communication, collaboration, customer service, help desk, mentoring, reviewing
rdds, udfs, data frames, datasets, pipelines, data lakes, data warehouse, data analysis
hadoop, hdfs, hadoop yarn, hortonworks, cloudera, impala
redshift, arangodb, cassandra, hbase, mongodb, sql, nosql, mysql, rdbms, access, oracle
connecticut	october 2018 – present
worked in an environment consisting of linux rhel 6/7 + hortonworks 2.6/3.1 + dell s3 + aws s3 + windows + scala + kubernetes + docker.
define spark data schema and set up of development environment inside the cluster
use scala to connect to ec2 and push files to aws s3
kellogg’s	hadoop engineer
moved the on-premises rdbms databased to amazon web services (aws) where the company could use various software solutions specific to the industry.
implemented amazon virtual private cloud (amazon vpc), connected directly to the kellogg data centers to allow access to sap tpm directly for employees who are on the company network.
used amazon cloudwatch for monitoring, to allocate costs to each department based on their individual infrastructure use.
the music streaming services is using a ‘data lake’ based on the hortonworks distribution of hadoop to calculate royalties, recommend tracks to users and measure audience response to new features and functions.  hadoop plays a vital role, for example, in helping spotify to recommend particular music tracks to an individual user on the basis of their established listening habits, using collaborative filtering techniques. it also helps spotify staff to curate playlists, based on their insights into what users want to listen to at certain times of day or during particular activities, from making supper to working out.  it’s also increasingly used for a/b testing, says baer, when new features and functions are rolled out on the spotify service.
implemented authentication using kerberos and integrated with active directory (ad)
“by running more responsive queries, we hoped to better understand how our customers were using qnap products and harness these insights to make plans for our future development,” says paul chu, qnap cloud services director. in early 2014, qnap’s web operation team began developing a new analytics platform to deliver the required customer insights. qnap aimed to complete the project and have a functioning analytics platform within six months.
the company had the problem of needing to search data logs manually and run data queries one at a time to generate statistics and business reports—an inefficient and error-prone process. depending on a query’s complexity, this could take hours or even days to get results.  the new platform using hadoop and aws the company can run queries in a matter of minutes.  we can easily generate a wide range of business reports using the gathered data, providing a degree of business insight it previously couldn’t achieve.
connected different branch offices and datacenters across the globe with consistent upload and download speeds using aws.
reporting data on different dimensions and generated dashboards with both numeric and graphic details on performance (oracle db, google sheet, excel, datastudio, tableau).
associate of computer & information technology
transforming
home depot // atlanta, ga
big data / hadoop engineer
master degree in computer information systems
american university
communication
microsoft office suite, peoplesoft, weka, pandadoc
followed agile scrum processes for software development lifecycle (sdlc) with 2-week sprints and daily 30-minute standups (scrums).
building zero-down-time spark scala pipelines that ingest data from mcgraw-hill software products.
developed complete end-to-end big data processing in hadoop eco system.
performed various configurations, which includes, networking and iptable, resolving hostnames, user accounts and file permissions, http, ftp, ssh keyless login. moved data from hdfs to rdbms and vice-versa using sqoop.
university of maryland, college park
web development and programming
proficient in data processing using hadoop cloudera and hadoop hortonworks distributions.
file formats:  avro, parquet, orc
union pacific railway | omaha, ne
responsible for architecting and implementation of custom hadoop big data services environment, etl pipelines and analytics platforms
worked with scalable solutions for business problems using hadoop, spark, spark streaming, kafka, and hive.
custom architecture provided superior performance and scalability, as well as the usable data derived from hadoop data lakes and hadoop clusters for predictive analytics.
imported data from web service into hadoop file system (hdfs) and transformed data.
spark used in optimizing etl jobs to reduce memory and storage consumption, and optimization of hive tables and large sets of structured, semi structured, and unstructured data for easier loading and transformation.
developed, monitored, and deployed ssis packages.
pierrejosh647@gmail.com
5 years’ experience constructing data systems and pipelines on cloud platforms
florida international university
apache flume, apache hadoop, apache hive, apache kafka, apache oozie, apache spark, apache tez
database & data structures, apache cassandra, amazon redshift, dynamodb, apache hbase, elasticsearch
valero was ahead of the curve many years ago in using data analytics and bi to transform its business strategy which has made it one of the most profitable oil producers today. as big data consultant, i was responsible for delivery of batch data integration and automation in a client consulting environment.
managed hadoop batch jobs.
this project required work across a variety of functional stakeholders (marketing, category, supply chain, etc.) to translate business needs into data and visualization requirements, and then back to pipelines to deliver to data analyst and bi teams.  there was a strong focus on data governance processes with an objective to transform manual reporting tools dependent on a refined and optimized data cleaning and delivery system.
also worked on analyzing hadoop cluster and different bigdata analytic tools including hbase and sqoop.
responsible to manage data coming from various sources.
exported the analyzed data to the relational databases using sqoop for visualization and to generate reports for the bi team.
responsible for creating hive tables and working on them using hiveql.
involved in creating hive tables, loading with data and writing hive queries which will run internally in map reduce way.
hands-on processing of data using spark streaming api with scala.
importing the unstructured data into the hdfs using flume.
importing and exporting data from mysql/oracle to hiveql using sqoop.
responsible for building hadoop cluster using hortonworks distribution with namenode and resource manager
continuous monitoring and managing the hadoop cluster, hdfs health check through ambari.
210-960-3326  |  pierrejosh647@gmail.comemail: ibrahimamir213@gmail.com   phone: (571) 934-4496
repartition datasets after after loading gzip files into dataframes and  improved the process time from 1.1 hours to under 28 minutes.
setup and configured development environment and qa environment
integrated kafka and spark with avro for serializing and deserializing data, and for kafka producer and consumer.
designed and developed etl jobs to extract data from aws s3 and load it in data mart in amazon redshift.
worked on s3 bucket integration for web application and development projects.
developed etl pipelines w/ spark and hive for business-specific transformations.
wrote custom hive udfs and hooked udf's into larger spark applications.
bachelor’s degree in information systems
st. paul, mnjermaine felder
phone:	 219-315-0311
systems software
windows
met with architects to discuss planning of the migration.
peer reviewed scripts based on project criteria.
documented how to execute ingestion process thorugh pyspark.
worked in an hadoop environment with coding in java, python and use of pyspark and unix shell scripting.
hands on programming using spark, scala, python to refine hadoop data analytics.
understands user requirements for use cases involving analytics using hadoop, spark, hive.
created hive queries to spot emerging trends by comparing data with historical metrics.
worked with business analysts and data scientists in leverage business rules and ingesting data into hadoop environment
created the estimates and defined the sprint stages.
collins enoh
dependable
experience in a variety of industries including healthcare and finance; familiar with hippa compliance and finra regulations.
tracker, task tracker, name node, data node and mapreduce concepts and experience in working with
proficient in writing technical reports and documentation
unix/linux, windows 10, windows8, windows xp, ubuntu, apple os x (yosemite, mavericks);
microsoft project, primavera p6, vmware, microsoft word, excel, outlook, power point; technical documentation skills
design roles and groups for users and resources using aws identity access management (iam) and managed network security using security groups, and iam.
utilized cloud watch to monitor resources such as ec2, cpu memory, amazon to design high availability applications on aws across availability zones.
created and maintained continuous build and continuous integration environments in scrum and agile projects.
architected big data systems on aws using aws tools and redshift database.
involved in running hadoop jobs for processing millions of records and data gets updated on daily and weekly basis.
continuous data integration from mainframe systems to amazon s3 which is connected via attunity an etl tool.
worked on aws to create, manage ec2 instances and hadoop clusters.
partitioning in db2, oracle and datastage.
database specialist
jan 2000 – dec 2009
certification and training
microsoft sql server database administration
shiva reddy
extremely large-scale big data, databases and data warehouses such as amazon redshift, apache cassandra, oracle, mondo db, nosql, and more.
large scale distributed systems, knowledge of all aspects of database technology from hardware to tuning to modeling.
experienced in spark data frames. spark sql,and spark streaming apis.system architecture, and infrastructure planning
implemented hadoop based data warehouses, integrated hadoop with enterprise data warehouse systems
developed free text search solution with hadoop and apache solr. analyzing emails for compliance and ediscovery.
transformation
focused on results and team centric performance
installed oozie workflow engine to run multiple map-reduce programs which run independently with time and data.
implemented best income logic using pig scripts.
supported in setting up qa environment and updating configurations for implementing scripts with pig and sqoop.
worked on tuning the performance pig queries.
crawled some websites using python and collected information about users, questions asked and the answers posted.
exported data from db2 to hdfs using sqoop.
troubleshot issues regarding windows server and networking, working alongside customer and outside healthcare organizations.
designed and developed code to enhance and support the product.
6+   years data engineering
apache cassandra, apache hbase, mapr-db, mongodb, oracle, sql server, db2, sybase, rdbms
data storage
big data processing
testing
bi and data visualization
data processing
developed workflows to read parquet files from s3 buckets and apply transformations, joins, filters, and sql queries to different dataframes and create output datasets.
use of jenkins as ci/cd methodology to create daily builds, automatically run unit tests and deploy builds in our qa or production environment.
hadoop cloud architect 	11.2016 – 09.2018
deployed hadoop clusters of hdfs, spark clusters and kafka clusters on virtual servers in azure environment.
use azure hdinsights as interface and to manage the online clusters.
the company uses data to make its work environment safer for employees and analyzes the roi of wellness programs it has implemented, and the impact of $10.5 million invested in hundreds of community organizations across 29 countries.  i was responsible for constructing a new data pipeline to study environmental impact on air quality.
design of kibana dashboard over elasticsearch for log monitoring.
this large dairy supplier uses big data to collect and analyze the results of food science testing.  they derive data from their r&d labs and aggregate data from external sources involving market research and packaging.  i was involved in constructing pipeline for a new food science testing project.
implemented tableau connected to hive query for ad hoc reporting and tableau for scheduled reports.
logstash configuration, setup multiple pipeline, managing worker and batch size and devops support.
this company specializes in motion and control technologies. motors and other machines have specific and distinctive signatures created by the vibration of their moving parts. a change in these patterns can indicate impending failure or long-term wear and tear that could eventually take the device off line. assessing these signatures for signs of trouble is a long-established discipline.
analyzed data by performing hiveql queries,  and running pig scripts to study data patterns.
coca-cola 	07.2007 – 02.2012
coordination of projects in the areas of systems, market intelligence and marketing. • management and analysis of information in sql databases. • acquisition of computer equipment, servers and software for management. • programming of systems and macros with databases. • planning and elaboration of work plans
eric bannavti suiseka
https://www.siemens.com/innovation/en/home/pictures-of-the-future/digitalization-and-software/from-big-data-to-smart-data-project-icewater.html
walmart inventory and merchandising
big data allows banks to create new levels of security. enhanced information protection and cyber security allows ken to make banking transactions that are faster, easier and safer, from any location in the world. analyzing transactions for fraud across multiple channels, including online and mobile banking, and in real time, means security protections that were previously unimaginable.
degree in computer science - incomplete
hive, yarn, spark, spark streaming, kafka
troy, new york
spark streaming to divide streaming data into batches as an input to spark engine for data processing.
highly available, scalable and fault tolerant big data systems using amazon web services (aws).
arbitron - columbia, md
deep knowledge in incremental imports of sqoop
917-338-9629
email
experience in designing and architecting big data pipelines with apache kafka, spark, elastic search, yarn, hdfs, nifi, hive, sqoop, flume, oozie, hbase and zookeeper, marathon, aurora
data migrations from relational databases like oracle, mysql, postgresql, mariadb, teradata sqlite to nosql databases like cassandra and hbase
data migration from on premise to cloud services like aws, azure, google cloud
ansi sql, tsql, pl/sql
zookeeper, tex, oozie, maven, hcatalog
native hadoop, hortonworks hadoop, cloudera hadoop, mapr
viacom cable network uses data scientists and big data to calculate ways to help marketers place their commercials with more precision. once known as “project gemini,” the service, known as vantage, is offered to a broad array of the company’s sponsors.
worked with the data science team to gather requirements for various data mining projects.
implemented analysis pipelines for internet of things in order to gather sensor data from the slomin shields at consumer locations.
altice usa
southern company
worked on moving some on-prem data repositories to cloud using amazon aws to make use of reduced cost as well as scalability.
edupristine (mumbai) - big data and hadoop
big data and hadoop
chingiz khalifazada
professional introduction
spark, scala, pyspark, pytorch, java, shell script language
flume, apache kafka,  logstash
log analysis
elastic stack (elasticsearch, logstash, and kibana)
apache solr/lucene, elasticsearch
developed aws cloud formation templates for redshift.
delta micro systems - rockville, md                                              	february 2015-february 2016
worked with users to ensure efficient resource usage in the hortonworks hadoop clusters and alleviate multi-tenancy concerns.
apache ant, , apache flume, apache hadoop, apache hadoop yarn, apache hbase, apache hcatalog, apache hive, apache kafka, apache maven, apache oozie, apache pig, apache spark, spark streaming, spark mllib, graphx, scipy, pandas, rdds, data frames, datasets, mesos, apache tez, apache zookeeper, cloudera impala, hdfs, hortonworks, apache airflow and camel, apache lucene, elasticsearch, elastic cloud, kibana, x-pack, apache solr, apache drill, presto, apache hue, sqoop, kibana, tableau, aws, cloud foundry, github, bit bucket, microsoft power bi, microsoft visio, tableau, google analytic, weka -software, microsoft excel vba, project and access, sas, others microsoft cain & abel, microsoft, microsoft baseline security analyzer (mbsa) aws (configuring/deploying software)
created variation of the lambda architecture consisting of near real-time using spark sql; spark cluster 1.4 consisting of 25 nodes running with 200gb ram/24 tb.
like other big box retailers, costco tracks what you buy and when. the information they collect could prevent you from getting very, very sick.  case story:  a california fruit packing company warned costco about the possibility of listeria contamination in its stone fruits (peaches, plums, nectarines). rather than send out a blanket warning to everyone who shopped at costco recently, costco was able to notify the specific customers that purchased those items.  costco was able to help the centers for disease control pinpoint the source of a salmonella outbreak back in 2010. this project pertained to work on data pipelines to capture supply chain data and correlate with purchase data.
b.s., mathematics; minor in actuarial science
aldo angulo
files provisioning (java program in an ecs cluster)
files processing: scala program in  emr cluster coded in scala 2.11.0, implementing spark 2.4, resulting in an average execution time is between 20 to 30 minutes.
october 2018	comcast xfinity – philadelphia, pa
involved in creating hive tables, loading with data and writing hive queries, which will invoke and run tez jobs in the backend.
may 2016 	city of long beach – long beach, ca
strong experience of software and system development using jsp, servlet, java server face, ejb, jdbc, jndi, struts, maven, git, junit, sql language.
dec 2013	blue cross blue shield of north carolina – charlotte, nc
degree in computer engineering
java programmer, and expert on big data with j2ee, apache spark,
components such as resource manager, node manager, container and
cloudera's hadoop platform along with cdh4 & cdh5 clusters, hdp 2.2
with kafka-storm and ec2 platform, ibm's big insight hadoop ecosystem.
• experience in importing and exporting data using sqoop from hdfs to
• having experience of applications development on tools eclipse, sts
like ibm's bluemix using inbuilt services.
java, hive ql, mapreduce, python, scala, sql, html, php, unix shell
airflow and camel, apache hue, yarn, apache hive, apache kafka, apache
daemons.
• used spark api over hadoop yarn to perform analytics on data in hive.
• apache storm, with redis, kafka and netty for real-time data analytic
• implemented workflows using apache oozie framework to automate tasks.
infrastructure, and mesos.
• involved in the process of designing cassandra architecture including
yarn, kerberos, redis, netty
custom java application architecture and development in addition to
sql server, oracle, mysql) and columnar databases (e.g. amazon
• worked with open source communities to commit code, review code, drive
of apache hadoop framework
from different portfolios
developed workflow in oozie to automate the tasks of loading the data
• cleaned up the input data, specified the schema, processed the
patterns from historical data and then monitoring customer activity to
between rdbms and hdfs.
aug 2013    hadoop data engineer
• responsible for operating system and hadoop cluster monitoring using
hadoop 1.x & 2.x for multiple projects.
functionality of cloudera hadoop.
tools including kafka, pig, hive and map reduce.
• used hive to analyze the partitioned and bucketed data and compute
• responsible for software installation, configuration, software
• developed pig scripts for replacing the existing home loans legacy
joins, filter both traffic and some pre-aggregations before storing
• developed mapreduce programs to write data with headers and footers
and shell scripts to convert the data to the fixed-length format
• agile methodology was used for development using xp practices (tdd,
we were able to move large volumes of data from individual servers to
cassandra, java, eclipse juno
• the objective of this project is to design a system to keep track of
jpa, xml, weblogic, unit case, junit, uml
eclipse
francisco torres (281) 853-8897  franciscogranados06@gmail.com
etl custom data pipeline and process development using ssis to build and implement data warehouse solutions and data modeling.
during migration process, rewritten the packages and cutdown the execution time to 60%.
programming technologies
support for existing and new reports in power bi
sr. business intelligence analyst                                                 dec 2014 – mar 2018
demand management cube took 8 hours daily on end to end process (etl's to cube).
perform scrum master tasks during sprints in agile environment.
developed and implemented etl process in ssis to collect daily information from sybase iq into sql data warehouse for different olap models and reporting services portal.
designed and implemented the ssis etl process and development of stored procedures for the tactical system information, which is the source platform of information for the organization.
involved in giving demo of the created reports to the concerned department, making necessary changes, or getting the reports signed off and deploy on to the server.
sr. business intelligence specialist                                               apr 2011 - dec 2012
designed and developed olap cubes to be the main source of information for the board of directors using ssas. this tool was used monthly to make decisions based on trends and projections.
collaborated with the hr department to analyze expenses, cost, and recurrence in employee’s illness by building an olap cube using ssas.
consolidated previous cubes of academic, teacher, and student models into one school hypercube model using ssas.
excel spreadsheet to sql server 2008.
migrated dts packages to sql server integration services (ssis) and modified the packages accordingly using the advanced features of sql server integration service.
used package configurations to pass parent package variable values to child package variable.
build unix shell scripts to automate the etl processes.
fime – uanl
itesm campus monterreybig data engineer
5 years of experience in development of custom hadoop big data solutions, platforms, pipelines, data migration, and data visualizations.
apache spark, spark streaming, spark api, spark sql
authentication
query language
query processing
worked on multi clustered environment and setting up cloudera and hortonworks hadoop echo-system.
developed scala scripts on spark to perform operations as data inspection, cleaning, loading and transforms the large sets of json data to parquet format.
deployed mapreduce and spark jobs on amazon elastic mapreduce using datasets stored on s3.
made and oversaw cloud vms with aws ec2 command line clients and aws administration reassure.
configured zookeeper to coordinate the servers in clusters to maintain the data consistency and to monitor services
involved in installation and configuration of ldap server and integrated with kerberos on cluster.
install and configuration of hive, pig, sqoop, flume, oozie and hbase on the hadoop cluster with latest patches.
implemented capacity schedulers on the yarn resource manager to share the resources of the cluster for the jobs given by the users.
technologies.  i am capable of grasping new concepts and executing multiple
resolving conflict and fostering strong teamwork.
|project management                  |files                               |
|database                            |hadoop ecoystem components & tools  |
hadoop data engineer   mar 2014 - dec 2015
processing mechanism using hadoop ecosystem, and on improving data quality
software engineer      may 2008 - apr 2010
experience in optimizing the data storage in hive using partitioning and
performed import and export of dataset transfer between traditional
used impala where possible to achieve faster results compared to hive
based hive joins.
worked with clients to better understand their reporting and dash boarding
manipulate the data.
built continuous spark streaming etl pipeline with spark, kafka, scala,
worked on importing the unstructured data into the hdfs using spark
handled 20 tb of data volume with 120-node cluster in production
apache kafka to transform live streaming with the batch processing to
worked on streaming the analyzed data to hive tables using sqoop for making
configured oozie workflow engine scheduler to run multiple hive, sqoop and
with regular expression for data cleaning.
used oozie scheduler system to automate the pipeline workflow and
implemented partitioning, bucketing in hive for better organization of the
worked on installing cluster, commissioning and decommissioning of data
code changes done by other team members.
united states marine corps.
focused in the medical and financial sectors.  i have a bachelor’s degree
data ecosystem design and development exclusively for the last 5 year.
hadoop big data ecosystems.
spark, apache tez, apache zookeeper, apache airflow, apache camel, apache
lucene, apache solr, apache drill, apache hue
• high availability of name node, resource manager on the hadoop
for authentication.
• dataset etl between databases and hadoop distributed file system
• spark sql and dataframes for faster execution of hive queries using
• design and develop etl workflows in hadoop using python and scala for
• worked on importing the unstructured data into the hadoop distributed
pig, hive, spark, spark streaming.
store to hadoop distributed file system (hdfs).
queries in the hadoop system.
management of finances in care and insurance strategies.  used hadoop,
sql, acro, rdd. aws, cloud, mysql, informatica, cassandra
• connected various hadoop data centers and transferred data between
them using sqoop and various etl tools.
loaded data from various data sources into hadoop distributed file
hadoop data engineer   aug 2012 - dec 2013
ntt data    plano, tx
distributed file system (hdfs), hive, pig, sqoop, linux, hbase, shell
all the products in all the stores without taking performance hit.
distributed file system (hdfs) on regular basis.
• scheduled workflows using zookeeper and oozie.
aetna health     hartford, ct
• worked with business functional lead to review and finalize
• designed structured multi-source data solution to deliver the
• performed impact analysis of hr data warehouse and existing reports as
existing data sources to continually improve data quality.
implemented to ensure most reporting flexibility while minimizing
performance and communicating functional and technical issues.
develop a strategic roadmap for data warehousing and bi.
• experienced in amazon web services (aws), and cloud services such as
value we can extract from data.  data has become crucial to our society and
our economy, and i am excited to be a part of that.
budget.
that are built for future phases of eds/ebi.
it into hdfs using flume; staging data in hdfs for further analysis.
to-end big data analytics systems.
using spark for data processing.
virtualpoc
lucidchart
sql alchemysql
provided a pro and con evaluation of airflow and azkaban workflow schedulers.
installed azkaban on a cluster.
managed commits to apple repository to ensure quality; due to changes they made to the zeppelin notebooks source code they have to re-deploy every time they make a small change.
dec 2017 predictix – atlanta, ga
implemented authentication using kerberos and authentication using apache sentry.
source system analysis, data analysis, data modeling to etl (extract, transform and load) and hiveql.
load log data into hdfs using flume. worked with developers on creating search automation and aggregation.
worked closely with developers to develop and test new jobs
ovation is a large corporate and professional firm travel agency.  i worked on data systems designed to analyze the way business travelers do business to drive efficiencies in the travel business.
aug 2012 big data engineer
jul 2012 walmart – bentonville, ar
created tables with indexes, entity relationship model, applied constraints and referential integrity checks.
developed sql queries to fetch complex data from different tables in remote databases using joins, database links and formatted the results into reports and maintained logs.
developed materialized views for data replication in distributed environments.
used powershell script for sql server maintenance activities.
bachelor’s degree in entrepreneurial and small business operations
computer science, database administration, web development, design
an eager and dedicated data engineer who can work in any environment collectively as part of a group or independently. i strive for new challenges that can sharpen and enhance my current skills and abilities. my experience in programming includes c#, asp.net rest, soap, sql, html, css, javascript, and other client-side technologies.
-summary-
experience in big data analytics with hands-on experience in data extraction, transformation, loading and data analysis, data visualization using cloudera platform (hdfs, hive, pig, sqoop, flume, hbase, oozie).
background with traditional databases such as oracle, teradata, netezza, sql server, etl tools/processes and data warehousing architectures.
experienced in using zookeeper and oozie operational services for coordinating the cluster and scheduling workflows.
experience working with cloudera & hortonworks distribution of hadoop.
sql server 2008 r2/2012/ 2014, mysql
sql server reporting services (ssrs)
sql server analysis services (ssas)
sql server management studio.
working closely with the data and analytics team to understand, extract and transform the data generated by user apps to build our solution.
created tableau dashboards for analytics and marketing requirements, using data stored in s3.
development of shell scripts to perform file operations like moving, copying and sending files over ftp and email.
sunnyvale, ca
the solution is intended to be highly customizable by the final user, enabling the end user to create their own data pipelines, transformations and final loading destinations.
users can configure data format, data schema, transformations, joins, filters, and sql queries.  users can load transformed data to their own platforms, using default cassandra or netezza, hdfs, teradata, oracle, etc.
maven used for the managing the project lifecycle, splunk for log reporting, hubble for metrics management, and jenkins for continuous integration.
responsible for the development of a sample spark application to read protobuf input from an encoded proto file, using the result classes from protoc compilation to create an rdd and a dataframe.
having good experience on all flavors of hadoop (cloudera, hortonworks).
data integration (pdi) server.
using hive join queries to join multiple tables of a source system and load them into elastic search tables.
importing and exporting big data in cdh into every data analytics ecosystem.
analyzing hbase database and compare it with other open-source nosql databases to find which one of
using oozie to schedule the workflows to perform shell action and hive actions.
involved in the implementation of spring mvc pattern and developed persistence layer using hibernate framework.
implemented orm through hibernate and involved in preparing the database model for the project.
extracted the data from various sources into hdfs using sqoop and ran pig scripts on the huge chunks of data.
performed transformations and aggregation to build data model and persists the data into hbase.
developed pig scripts for the analysis of semi-structured data.
managed and reviewed the hadoop log files using shell scripts.
used hadoop hdfs to store the information for easy accessibility.
responsible for preparing technical specifications, analyzing functional specs, development, and maintenance of code.
partitioning and bucketing done for the log file data to differentiate data on daily basis and aggregating based on business requirements.
intensively worked on documentation of the project, maintained technical documentation for hive queries and pig scripts we created.
environment: hadoop, hdfs, hive, pig, sqoop, hbase, oozie, my sql, svn, my sql, zookeeper, unix, shell scripting, tableau
development of spatial analysis and director level dashboards with microstrategy business intelligence and visual crossing (mapping interface) to show business information and projections, like sales per region or productivity gauges on thematic maps and charts.
worked with stakeholders to gather the requirements directly from the business owners
interacting with subject matter experts of different source systems and technical streams to understand the dynamics of the data and existing application
working closely with the business users and proving support during user acceptance testing.
designed and developed rich internet applications for the web using silverlight
bachelor’s degree in computer systems
abdulrahman alqahtani
10 years of experience in the field of data analytics, data processing and database technologies.
apache ant, apache cassandra, apache flume, apache hadoop, apache hadoop yarn, apache hbase, apache hcatalog, apache hive, apache kafka, apache maven, apache oozie, apache pig, apache spark, spark streaming, spark mllib, graphx, scipy, pandas, rdds, dataframes, datasets, mesos, apache tez, apache zookeeper, airflow and camel, apache lucene, elasticsearch, apache solr, apache drill, presto, apache hue, sqoop, kibana,
company name		city, st
infrastructure design for elk clusters.
coordinated kafka operation and monitoring with dev ops personnel.
hands-on experience with spark streaming to receive real time data from kafka.
etl
stacks
flattened dataset rows to produce single rows for each user id.
managed network clicks data contained about 10 million rows, and network impressions contained about 8 billion rows in total.
apr 2014 – oct 2015
aws, cloudera, hortonworks, mapr, anaconda, jupyter notebooks, and elastic.
frameworks.
hortonworks.
• creation of udf functions in python or scala.
• hands on experience in installing, configuring cloudera's and horton
the cluster.
customer data derived from pos systems, including loyalty programs and
•     for one of the use case, used spark streaming with kafka & hdfs &
•     used spark api over hadoop yarn to perform analytics on data in hive.
•     migrated etl jobs to pig scripts for transformations, joins,
•     •     performed storage capacity management, performance tuning and
•     performed both major and minor upgrades to the existing cloudera
•     implemented high availability of name node, resource manager on the
to capture realtime customer transaction data for predictive analytics and
data node, configuring slots, and on name node high availability, and
for version control, and maven as a build tool for deploying the code.
•     worked with spark context, spark -sql, dataframe and pair rdds.
•     design and develop etl workflows using python and scala for
•     developed various data connections from data sourced to ssis, and
•     created partitions, buckets based on state to further process using
environment: hadoop, hdfs, hive, spark, yarn, kafka, pig, mongodb, sqoop,
storm, cloudera, impala, zookeeper, oozie
improved systems and analytics capability.  the project involved a lot of
and project plans.
• designed and set-up pocs to test various tools, technologies and
• used oozie to automate/schedule business workflows which invoke sqoop
to evaluate portfolio. this project involved implementing tools and
development of its entire portfolio.  the evaluation of opportunities
which to gather data for statistical analytics.
apache hadoop ecosystems along with various tools.
execute jobs in a timely manner.
• installed veritas volume manager 4.0 and configuring disk groups,
over 6 years of experience in systems engineering with expertise in big data, data analysis, and .net architecture.
experience in installing hadoop cluster using different distributions of apache hadoop, hortonworks,
design big data solutions for traditional enterprise businesses.
led team in architecting and designing backend database model, corporate web application and various front-end applications for windows ce touch based application
worked with horton works support to resolve the issues.
implemented elk (elastic search, log stash, kibana) stack to collect and analyze the logs produced by the spark cluster.
analyzed the sql scripts and designed the solution to implement using spark.
involved in converting hive/sql queries into spark transformations using spark rdds and scala.
3/2013 – 12/2014
expert in implementing advanced procedures like text analytics and processing using the in-memory.
experienced in defining job flows using oozie.
developed power enter mappings to extract data from various databases, flat files, and load into datamart.
i architected, worked and helped to develop the solr/lucene search deployed to azure. the indexing was done directly on top of the metadata extracted from various files.
developed a customized solr indexing scheduler in c# which would run periodically to do delta indexing.
sharepoint /.net developer
created custom master pages and page layouts for consistent branding.
developed custom kpis and integrated custom code with performancepoint to provide bi reporting for ecm activity
dish – mexico
analyzed and tuned cassandra data model for multiple internal projects/customers.
houston, tx
configuring access for inbound and outbound traffic rds db services, dynamodb tables, ebs volumes to set alarms for notifications or automated actions on aws.
systems from networks and infrastructure to data pipelines and processing
functions.
• hadoop ecosystem components such as flume, oozie, hive, and pig.
stage job levels using indexes, hints, and partitioning.
• knowledge in impala, spark/scala, shark, storm, ganglia.
adhere to strict deadlines.
|flume, apache hadoop, apache hbase, |hadoop                              |
|reporting/bi/visualization          |hive                                |
|file types                          |cloud services & distributions      |
representatives to properties each month to read meters.  meters are read
points relating to system functioning.  processing data from meters in real-
clusters to a new multi-tenant.
• cluster consolidation saved administrative overhead cost and service
based google hadoop clusters and storage.
feb 2015    sbc solutions – reston, va
this consulting firm focuses on big data analytics in the financial
for raw file storage, aws ec2 for kafka, aws lambda for data
usaa has a strong focus on the best way to use analytics and insights to
linux, hadoop, hbase, shell scripting, eclipse, oozie, navigator.
of commonly-used concepts, practices, and procedures within the adhar
resolve production outages.
documented.
cac/windows authentication.
• data center power and hvac requirements, cable management, hardware
system integration, testing and evaluation, and system architecture
• remediated system vulnerabilities iaw stigs and dod policies.
framework (rmf) packages, and collaborate with staff agencies to
• worked with redhat and windows operating systems. environment:
mar 2005    electronic data systems corporation – norfolk, va
repair wiring on aircraft on small aircraft army national guard (march 1990-
computer and network technologies to facilitate collaboration needs of the
customer network and computer systems on the marine corp network. trouble
cloudera hadoop (cdh) /elastisearch analyst
data systems engineering, data pipelines and etl with hadoop, hive, and
• extensive experience with a number of machine learning modelling
•  familiarity with sql and nosql databases e.g. mysql, postgresql,
map/reduce, spark, flink
unix shell scripting, sql, hive ql, spark, spark streaming, spark mllib,
for hadoop data processing, familiar with amazon aws, microsoft azure,
anaconda cloud, elasticsearch, apache solr, lucene, cloudera hadoop,
food safety, food and nutrition assistance programs, natural resources and
hortonworks hadoop distribution, and hadoop spark, hive, kerberos,
data processing using kafka.
• migrated hadoop etl jobs to pig scripts before hadoop distributed file
• expert with bi tools like tableau and powerbi, data interpretation,
• performance tuning the data heavy dashboards and reports for
partitioning in data source etc.
• imported unstructured data into hadoop distributed file system (hdfs)
using sqoop and kafka. created kafka topics and distributed to
• worked with hadoop on amazon web services (aws) and involved in etl,
analytics of big data.
•  used different file formats like text files, sequence files, avro for
system (hdfs). using kafka.
and spark streaming.
•  used the hive jdbc to verify the data stored in the hadoop cluster.
case, implications, and forecasts for global deployment of a marketing
• used zookeeper for providing coordinating services to the hadoop
jobs like etl.
environment: hadoop cluster, hdfs, hive, pig, sqoop, linux, oozie,
|[pic]          |sep 2010 - jul 2012                                       |
|               |bi developer                                              |
life cycle of checking and debit transactions.
effective, parallel compute platform
server, and j2ee application deployment technology.
|               |information specialist                                    |
• responsible for gathering the requirements, designing and developing
• worked with csv data for applications.
master of science in information technology:  database systems
excellent knowledge on hadoop architecture and hadoop ecosystems such as
and mapr.
development of custom large-scale enterprise hadoop data processing
hadoop framework and hadoop ecosystem, hadoop distributed file system
cjojqjajhýƒh ·cjojqjajhýƒh±[,cjojqjajarchitecture, hive, pig, sqoop, hbase,
mapr.
experience deploying large multiple nodes of a hadoop cluster and spark
mapr, s3), hadoop framework and parallel processing implementation.
• strong knowledge in overall hadoop eco-system. hands on experience in
hdfs, pig/hive, hbase.
• highly experienced in writing test cases and executing in hp
(qtp).
• application of structured methods to project scoping and planning,
• knowledge in dimensional modeling, data migration, data cleansing,
• worked with business team on requirements gathering and to prepare the
• developed programs to cleanse the data in hdfs obtained from
• wrote hive ql scripts and complex hive and sql queries for data
• designing and creation of hbase tables.
structures and queries for a seamless end-to-end highly efficient system
aug 2014    hadoop data engineer
• utilized mysql from day to day to debug and fix issues with client
processes.
bachelors in computer science
hadoop: hdfs, map reduce, hive, pig, sqoop, flume, amari, zookeeper, avro, orc, parquet, oozie, mahout, apache spark, spark streaming, spark sql, storm, kafka, tez, ant, toad, apache lucene, elasticsearch, elastic cloud, kibana, apache solr
skilled in management of azure storage including azure blob storage, azure blob store and azure data lake store.
use of azure stream analytics for realtime processing of data from iot systems including devices and sensors.
architecture of azure data pipelines extracting data from azure sql database and azure data warehouse.
excellent working in team environments and great individual performer.
created hadoop clusters and hadoop data lake with azure blob storage as the azure form of hdfs file storage.
used azure blob storage and azure data lake store to receive data migration and migrated data using azure data factory.
architected a streaming data source using azure stream analytics to receive real time data from ibm mq and store the stream data to azure blobs.
duplicated sql server to azure cloud-based test server and secured data in preparation for move to cloud with encryption using powershell for server-side encryption.
created event trigger data pipeline for analysis of player data to tie performance to trigger rewards and incentives.
implemented business logic with udfs automated jobs for pulling data from ftp server to load data into azure data lake stores, azure blob storage or distributed databases.
utilized mercurial to manage branches, commits, build and version control.
performed maintenance, monitoring, deployments, and upgrades across infrastructure.
automated all the jobs for pulling data from ftp server to load data into tables, using workflows.
jan 2012 to mar 2013
successfully secured kafka cluster with kerberos.
implemented kafka security features using ssl and without kerberos, with more grain-fines security to have users and groups to enable advanced security features.
tested non-authenticated user (anonymous user) in parallel with kerberos user.
created bash script with awk formatted text to send metrics to influxdb.
installed kafka manager for consumer lags and for monitoring kafka metrices by adding topics, partitions etc.
worked on maintenance of elasticsearch cluster by adding more partitioned disks. this will increase disk writing throughput and enable elasticsearch to write to multiple disk in same time and a segment of given shard is written to the same disk.
sep 2008 to may 2010
designed and developed functional and technical specifications using design patterns and object oriented methodology using uml.
agile, continuous integration, test-driven development, unit testing, functional testing, gradle, git, svn, jenkins, travis, jira, maven
worked on architecting serverless design using aws api, lambda, s3 and dynamo db with optimized design with auto scaling performance.
keycorp
installed and configured hortonworks hadoop (hdp) cluster, using hortonworks ambari for easy management of existing cluster.
bachelor of science in computer engineering
experian
responsible for presenting information and insights in plain english and facilitating discussion to generate possible options for practical application.
created hive and impala queries to spot emerging trends by comparing data with historical metrics.
methodologies: pmi, project portfolio management (ppm), agile/scrum
effectively communicated project scope, schedule, and budget with both consumers and contractors.
automated the tableau server extract refreshes of the data sources hosted on sql server.
implementation and transition of reporting solution based on ms power bi/tableau/cognos to all level of stakeholders.
designed and developed interactive tableau dashboards and reports that portray business intelligence.
planning the activities of the team, monitoring and reporting on progress.
prepared financial and operational analytics reports and visualizations at the direction of senior financial
management personnel.
maricopa county sheriff’s office – phoenix, arizona	august 2005 – may 2008
c, java, sql, javascript, python
july 2019 – present
implemented a daily incremental load from salesforce to stage tables & stage tables to dimension and fact tables
backfilled aggregate tables using matillion
developed okta to looker integration in matillion to sync deactivated user account security access
allstate, irving, tx
custom etl solution
build poc of java kstreams to benchmark against python faust.
kstreams integration with internal rest service to consume and produce records
documentation of kafka-connect to implement configuration in allstate environment.
understanding of insurance data privacy requirements and use case logic
environment: cloudera, confluent, windows, outlook
worked with sql and no sql databases: oracle, mongo db, cassandra, db2, mysql, along with installation and infrastructure design for the same, locking, transactions, indexes, replication and schema design.
extracted real-time feed using kafka and spark streaming and convert it to rdd and process data in the form of data frame and save the data as parquet format in hdfs.
implementation of data lake on hadoop clusters on amazon web service (aws).
esurance, san francisco, ca
https://www.forbes.com/sites/kathleenchaykowski/2016/02/08/how-esurance-engineered-its-way-to-winning-the-hashtag-bowl/#310883f12783
performed continuous data integration from mainframe systems to amazon s3, connected using attunity etl tool.
implemented custom logs for zab zookeeper atomic broadcast; implemented a zookeeper watcher interface(java api); installed, configured ganglia - gmond, gmetad, gweb, set up multicast/udp topologies and designed rdd files for high io demand; set up the web interface for grid/cluster/physical/host and node views; set up ganglia advanced metric monitoring and debugging
web scripting:
i designed a cloud-based flask web application that served as the main integration between a java spring boot & angular front-end and our managed apache airflow instance.
worked with networking team to modify the enterprise proxy and firewall settings to enable proper access and controls on our application and google kubernetes engine (gke).
paired with data science and data analyst teams to gather feature and enhancement requirements and had regular meetings for general feedback
trammo, new york, ny
zookeeper tarball, configured zookeeper ensemble of 3 nodes in standalone and multi-node cluster.
key technologies:  hortonworks hadoop, hive, pig, kafka, spark, spark streaming, google cloud platform, dataframes, amazon aws, s3, sqoop, b+db2, hbase, rdd, yarn
partitioned and bucketed hive tables; maintained and aggregated daily accretions of data.
created a general outline for future data pipelines to follow for ease of use and automation.
jan 2018 to may 2019
applied the latest development approaches including applications in spark using scala.
nov 2016 to dec 2017
developed etl pipeline to process log data from kafka/hdfs
intelligence, and configuring systems in cloud environment (i.e.
• significant experience working with / for a variety of domains /
primary skills
partitioning, spark performance tuning, optimization, spark streaming,
apache hbase, apache hcatalog, apache hive, apache kafka, nifi, apache
✓ prepared the detailed design document and get it approved.
✓ developed automated sqoop scripts for incremental and fullloads.
✓ developed complex etl's in hive.
✓ worked with tableau team for reports performance.
reporting.
tools including pig, hbase database and sqoop.
✓ responsible for building scalable distributed data solutions using
machine learning techniques and approaches.
✓ developed simple to complex jobs using hive and pig
updates, patches, version upgrades as required.
graphs for comparison, analyzed the results and found bottlenecks.
testing.
reusable functions, modules and better test data collection using
database connection.
✓ wrote sql queries for getting right test data from right testing
of the application against user stories with both uft.
la
apache oozie, apache zookeeper, logstash
very strong technical written and oral communication skills
hive, pig, spark, spark streaming, storm
architected custom hadoop etl using hive, pig, kafka, spark, spark streaming and storm to construction various data processing pipelines, data queries for data collection, ingestion, access, and visualization of data from cloud-based clusters.
created an hadoop architecture that could provide performance and scalability, as well as adaptability for new pipelines and use cases.
created hive and impala queries for use by staff to access data for reporting.
batch processing and realtime processing of sensor data.
used pig and hive, and imported data using sqoop to load data from mysql to hdfs on regular basis.
hadoop data engineer	february 2014 - june 2015
hadoop, hdfs, apache solr, lucene search, microsoft azure, data warehouse, data governance, mdm, azure cloud, aws, amazon cloud,
•	implemented partitioning, bucketing in hive for better organization of the data.
systems security engineer	october 2009 - february 2013
managed log files, logged security events and incidents, reviewed security logs, investigated incidents; provided risk assessment and recommended measures to mitigate exposure to risks.
i learned and implemented several technologies that were new to me and used them to complete projects.
systems administrator	sept 2006 - october 2009
university of phoenix, atlanta, ga
manage and support for multiple large databases sized in multi terabits.
september 2016 to present
redshift, queues, tuning, scaling, 100tb implementation
integrating on-premises cluster to better work with transient, cloud-based hadoop clusters and storage.
big data architect/admin
involved in complete big data flow of the application starting from data ingestion upstream to hdfs, processing the data in hdfs and analyzing the data and involved low level design for mr, hive, impala, shell scripts to process data.
sun power corp.
installed and administered client’s first hadoop cluster utilizing the cloudera distribution.
expert in informatica for data analytics, data integration and management.
chicago, il
designed and configured mysql server cluster, and managing each node on the cluster.
setup replication for disaster and point-in-time recovery.  replication was used to segregate various types of queries and simplify backup procedures.
applied performance tuning to improve issues with a large, high-volume, multi-server mysql installation for job applicant site of clients.
documented and provided status, updates and technical information to project manager.
professional summary
experienced in working on cql (cassandra query language), for retrieving the data present in cassandra cluster by running queries in cql.
data lake, hdfs, data warehouse, s3
software development
spark, scala, hive, pig, java, pyspark, keras and tensorflow
learn, boto3, psycopg2, beautifulsoup, geopandas, rasterio)
created a kafka producer to connect to different external sources and bring the data to a kafka broker.
kibana dashboard designed over elasticsearch for visualizing the data
pultegroup, inc.
st. paul, mn
managed hadoop clusters and check the status of clusters using ambari.
developed scripts to automate the workflow processes and generate reports.
involved in writing incremental imports into hive tables.
worked on tickets related to various hadoop/big data services which include hdfs, yarn, hive, oozie, spark, kafka.
used spark sql and udfs to perform transformations and actions on data residing in hive.
saint peter’s university
neural networks (udemy)
6  years  of  professional  experience  in  providing  analytics-based  business  solutions  using hadoop ecosystem (hadoop, cloudera, impala, hortonworks) and cloud computing environments such as amazon web services (aws), google cloud and microsoft azure cloud.
deep knowledge in incremental imports, partitioning and bucketing concepts in hive and spark sql needed for optimization.
experience deploying large multiple nodes of a hadoop and spark cluster.
professional skills
admin tools
mclean, va
team: reporting
environment: python, jupyter notebooks
technologies: gitpython, django, jenkins, and proprietary ci/cd tools.
candidate guru
data engineer	january 2015 – january 2016
configured auto-scaling for guaranteed for real-time streaming analytics via kinesis.
used spark streaming to receive real time data using kafka
configured apache zeppelin binaries/conf for spark web clients; integrated zeppelin daemon with spark master node, tested and configured web server with spark cluster; tested zeppelin with sparksql and python clients(pluggable interpreters); tested screen sharing functionalities websockets, zeppelin views from spark notebooks
replaced existing mapreduce jobs and hive scripts with spark dataframe transformation and actions for the faster analysis of the data.
migrated complex mapreduce programs into apache spark rdd operations like transformations and actions.
implemented spark rdd transformations to map business analysis and apply actions on top of transformations.
involved in loading data from unix file system to hdfs.
ibm – big data 101
hadoop and rdbms data pipelines, transformation, and cleansing.  skilled in
system configuration and processing using data processing tools hadoop
▪ forensic analysis with large complex data sets, using real-time
systems.
▪ deep knowledge in incremental imports, partitioning and bucketing
social media data from facebook and twitter using flume, and storing in
|open stack                          |hadoop, hive, spark, maven, ant,    |
|windows active directory, windows   |tez, zookeeper, apache airflows     |
|logstash, scriptella, stitch,       |powerbi                             |
led pocs to test spark solutions for production tasks involving kinesis and
spark, python, and kinesis.
and running jobs in the backend.
• created ad-hoc, concatenated views for new dbs by analyst request
tolerance against that platform using impala/spark (querying) and
• presented kudu findings to management, who approved further, practical
• translated complex knime data transformations into sql
• wrote python scripts in jupyter to loop through daily, dynamic
• wrote python scripts in jupyter to pull information security logs,
senior hadoop data engineer       november 2016- december 2017
• implemented and administered cloudera hadoop cluster(cdh); reviewed
log files of all daemons.
• migrated complex programs into apache spark rdd operations.
• implemented high availability of name node and resource manager on the
benchmarking of clusters.
• performance tuning of hive service for better query performance on ad-
• for one of the use case, used spark streaming with kafka & hdfs &
mongodb to build a continuous etl pipeline. this is used for real-time
• configured spark streaming to receive real-time data from kafka and
hadoop data engineer   july 2015- october 2016
analyzing patient data from discrete sources, insights into patient risk
behalf of those identified.
queries.
• design and develop etl workflows using python and scala for processing
streaming & kafka.
• implemented partitioning, dynamic partitions and buckets in hive for
increasing performance benefit and helping in organizing data in a
on policy
• loading data from diff servers to aws s3 bucket and setting
generate reports
• implemented spark using scala and spark sql for faster analyzing and
tableau server for report and dashboard development.
environment: hadoop hdfs, hive, spark, yarn, kafka, pig, mongodb, sqoop,
hadoop data engineer   march 2014- june 2015
sealedair corp. – charlotte, n.c.
a data analysis system specific to cleaning and the internet of clean (ioc)
to analyze metrics of cleaning runtimes, labor, to hone in on labor
hadoop and its related technologies.
sqoop and various etl tools.
• built a prototype for real-time analysis using spark streaming and
• used different file formats like text files, sequence files, avro.
the focus of this texas oil analytics company is to accelerate workflows
• • responsible for building scalable distributed data solutions using
• imported data using sqoop to load data from mysql and oracle to hdfs
on regular basis.
• used sqoop to efficiently transfer data between databases and hdfs and
error logs, and foreseeing and preventing potential issues, and
• used zookeeper and oozie for coordinating the cluster and scheduling
• business to business data.
• decreased fraud loss by thirty-five percent.
• provisioning and deauthorization.
• developed an automated employee access audit with powershell, python,
• salesforce/docusign api, and bash.
interim lead/application support engineer    may 2008- august 2010
o decreased labor costs and time to resolution for technology by
python, batch scripts.
monitoring solutions to replace the current proprietary application.
• created mysql events for table maintenance.
• decrease incident resolution by forty-eight percent.
clustered applications worldwide.
• troubleshot incidents through oracle sql, red hat linux, appdynamics,
modifications, and release management tasks.
blake hamilton
analyze and tune cassandra data model for multiple internal projects/customers.
pull files from aws cluster and store in elastic search by lambda functions and kibana to visualize.
hands-on experience processing data using spark streaming api with scala.
effectively tune hive scripts and use of spark to optimize systems.
spark used in optimizing etl jobs to reduce memory and storage consumption and wrote spark codes in to run a sorting application on the data stored on aws.
knowledge of multiple big data cloud platforms using aws elk, emr, s3, lambda, kinesis, cloud formation, iam
malvern, pa
transformed company-wide internal data using pyspark and pyspark sql
adhered to data compliance standards for storing and processing sensitive data
worked on deployment of ec2 infrastructure to support a thoughtspot cluster in the cloud.
created documentation including architecture diagrams and code documentation for easy knowledge transfer.
extract/ import log data from source to hdfs using kafka.
made incremental imports to hive with sqoop.
partitioning and bucketing to optimize hive app.
loaded data into hbase tables and hive tables consumption purposes.
developed oozie workflows for scheduling and orchestrating the etl process.
worked with different teams to install operating system, hadoop updates, patches, version upgrades of hortonworks
secure some of important hadoop application of the cluster from apache ranger.
used avro for serializing and deserializing data, and for kafka producer and consumer.
luke t richter
hands-on extracting and generating statistical analysis using business intelligence tools and data visualization tools like tableau for data analysis.
aws, ec2, ebs, softlayer, postgresql, mysql, sql server, dns, flask, restful, lamp, node.js, grails, ruby on rails, git, redis, rabbitmq, chef, jenkins, gitlab, cron jobs, bash shell, asp.net
data visualization █████████
hadoop distributions █████████
hadoop, cloudera hadoop (cdh), hortonworks hadoop (hdp)
worked on streaming the processed data to redshift using emr/spark to make it available for visualization and report generation by the bi team.
implemented hadoop cluster automation using docker containers.
installed, configured, and tested aws lambda function workflows in python
optimization of hive tables and large sets of structured, semi structured, and unstructured data.
configure yarn capacity scheduler to support various business sla's.
used ambari to monitor workload, job performance and capacity planning.
profile
performed streaming data ingestion from kafka to spark consumer.
integration of kafka with spark for real time data processing; streaming data ingestion w/ kafka; handling schema changes in data stream using kafka.
kibana
scala
aws emr
aws lambda
aws cloud formation
hive
hortonwork
hdp
work experience
automated systems with cicd using jeknins continuous integration server.
worked with different teams to install operating system, hadoop updates, patches, version upgrades of hortonworks as required.
configuring spark streaming to receive real time data from cassandra and store the stream data to hdfs.
implemented spark structured streaming for highly scalable, fault tolerant data processing.
prototyped analysis and joining of customer data using spark and processed it to hdfs.
added support for amazon aws s3 and rds to host static/media files and the database into amazon cloud.
kafka cluster maintenance, trouble shooting, monitoring, commissioning and decommissioning data nodes, troubleshooting, manage and review data backups, manage & review log files.senior data engineer
skilled in hadoop big data architectures, data movement technologies, database partitioning, database optimization, and building communication channels between structured and unstructured databases.
worked with various file-formats (parquet, avro & json) and compressions (snappy& gzip)
programming
c++, java, javascript, python
spark, scala, sql
codebase management and testing
database
windows active directory, windows server 2003, 2008, 2008r2, 2012/2012r2, red hat linux 6-7, red hat, ibm aix, hp, centos, ubuntu
implement cicd pipeline using codepipeline.
write cloudformation scripts to provision aws resources on demand.
senior big data engineer		august 2017 – april 2019
amazon ec2, amazon s3, amazon simpledb, amazon rds, amazon elastic load balancing, amazon sqs, and other services of the aws family.
logstash configuration, setup multiple pipeline, managing worker and batch size and devops support
ec2 instance creation and auto scaling, snapshot backup and managing template.
architecting and data engineering for aws cloud services including aws cloud services planning, designing and devops support like iam user, group, roles & policy management. aws access key management.
optimized data storage in kafka brokers within the kafka cluster by partitioning kafka topics.
transfered streaming data from different data sources into hdfs and hbase using apache flume.
used different file formats like text files, sequence files, and avro.
configured spark streaming to receive real time data and store the stream data to hdfs.
extract real time feed using kafka and spark streaming and convert it to rdd and process data in the form of data frame and save the data as parquet format in hdfs.
involved in transforming the relational database to legacy labels to hdfs, and hbase tables using sqoop and vice versa.
involved in transforming data from legacy tables to hdfs, and hbase tables using sqoop.
connected various data centers and transferred data between them using sqoop and various etl tools.
jsamano092@gmail.com  (206)-257-3849
experience in hadoop ecosystems with database and etl.
technical summary
cloud computing
apache hadoop, hortonworks hadoop, cloudera hadoop
hands-on experience on setting a kerberos tgt for different environment and different user accounts.
peoria, il
social seo
help to implement different components on the cloud for the kafka application messaging
configuring spark streaming to receive real time data from ibm mq and store the stream data to hdfs.
worked on aws to create, manage ec2ugh instances, and hadoop clusters.
san francisco, ca
implemented the model view control (mvc) structure using struts.
queretaro institute of technology
20 years total i.t. experience.
have worked with over 100 terabytes of data from data warehouse and over 1 petabyte of data from hadoop cluster.
uses flume, kafka, nifi, and hiveql scripts to extract, transform, and load the data into database.
ambari, yarn, workflows, zookeeper, oozie, cluster management, cluster security
nosql database
kinesis, kinesis stream, firehose, kafka, sqoop, flume
parquet, avro, json, orc, snappy, gzip
hdfs, data lake, data warehouse, database
project environment based on amazon cloud (aws), elk, grafana, new relic.
poc of redshift based on postgres (aws rds) for qa and dev environments.
created rds postgres instances to substitute and test redshift, creating databases, tables and inserting data.
technologies used:  spark, quantum, jira, jenkins, cicd pipelines, aws s3, aws lambda, aws redshift, aws rds, postgress, aws security groups, im roles, cloud formation, aws cli, docker. elk, kibana, logstash, elasticsearch, influxdb, new relic, grafana, data dog. file formats parquet, csv, avro, jira, git, linux, bash shell scripting.
assima, inc., atlanta, ga
created architecture schematics and implementation plan.
developed a jdbc connection to get the data from sql and feed it to a spark job.
involved in reverse engineering to obtain the business rules from the current commerce platform.
parsed data from various sources and store parsed data into hbase and hive using hbase-hive integration.
used spark codes to run a sorting application on the data stored on aws.
designed a cost-effective archival platform for storing big data using hadoop and its related technologies.
involved in designing web interfaces using html/ jsp as per user requirements. improved the look and feel of these screens.
used the hive jdbc to verify the data stored in the hadoop cluster.
america’s value channel,
software installation, configuration and troubleshooting
implemented new promotions and discounts for services and hardware in database.
developed pl/sql scripts that inputted most of the data for release.
adsl speed
devry university, atlanta, ga
certifications pending
ccna / server 2008 classes – lanier technical college, oakwood, ga
hadoop, data lakes, data warehousing, and database. proficient in
summary of experience
sqoop, storm, spark, kafka, yarn, oozie, zookeeper etc.
• mllib, spark graphx.
with hive and sql, and spark sql to manipulate data frames in scala.
streaming solutions using spark streaming.
• excellent understanding of hadoop architecture and its components such as
• extensive experience with databases such as mysql, oracle 11g.
methodologies in all the phases of data warehousing life cycle.
sql, mysql, rdbms, nosql, apache cassandra, apache hbase, mongodb, db2,
aws, s3, ec2, emr, lambda services, microsoft azure, adobe cloud, amazon
consumers, and how to create sustainable revenue for artists.  predictive
analysis of revenue models and analytics of consumer purchasing habits
reveals declining market shares as subscription models replace classic pay
models like cds and itunes.
and used erwin to design the business process, dimensions, and
• developed informatica design mappings using various transformations.
• change in a dynamodb table and load the transformed data to another
• used kafka producer to ingest the raw data into kafka topics run the
• created dataframes from different data sources like existing rdds,
• used hive context which provides a superset of the functionality
data processing and followed hive best practices for tuning.
spark sql, mongodb etl, oracle, informatica 9.6, sql, sqoop, zookeeper,
• ran jobs on yarn and hadoop clusters to produce daily and monthly
reports.
• analyzed data by performing hive queries (hiveql), impala and running
architected data storage and processing capabilities in the cloud for
implemented etl to redshift on amazon, as well as employing other data
• used rest api to access hbase data to perform analytics.
• perform analytics on time series data exists in cassandra using
build the common learner data model and persists the data into hbase.
performed the data transformations using spark python.
from data sources, perform
sep 2012    data analytics developer
platform for storing big data using hadoop and its related
in hadoop warehouse. used pig as etl tool to do transformations, joins
requirements. improved the look and feel of these screens using css,
bootstrap, jquery, javascript, jstl.
• used spring framework for developing business objects and integrating
• considering both the business requirements and the factors to create
• creating test cases for unit test, system integration test and uat to
cron jobs through pmcmd commands,
• created unix shell scripts and called them as pre-session and post-
worked on disaster management with hadoop cluster, and involved in building a multi-tenant cluster.
strong hands-on experience in hadoop; hdfs architecture, hive, pig, sqoop, hbase, mongodb, cassandra, oozie, spark rdds, spark dataframes, spark datasets,
programming & scripting
natural leader, involved in the community, mentoring and advocacy for others, written and verbal communication, presentation and reporting highly motivated, self-starter, strong sense of ethics and helping others, strong interpersonal skills, communication skills, highly effective time management and productivity., hard-working, generous and fair-minded.  skilled in analysis, critical thinking, evaluation and creative, custom solutions.
using sparkui and ganglia.
used github as the cm repository with versioning control.
optimized performance of data cycles to clear the data frames, and reread the data.
created hive external tables and designed data models in hive.
performed import and export of dataset transfer between traditional databases and hdfs using sqoop.
implemented yarn resource pools to share resources of cluster for yarn jobs submitted by users.
aug 2010	dec 2013
creating hive external tables to store the pig script output. working on them for data analysis in order to meet the business requirements.
analyzed the feasibility of using the existing tools to monitor the ground system was untenable, sought new tools to fill in the gaps and investigated the database for information that could supply more information to an operator (mysql).
developed scripts to read the delivery status from the database and display the results to an operator in a more concise, clear, color coded manner. this effort turned into the product tracker which provides the operator with both a graphical summary and delivery details that the operator uses to verify the delivery of products to all of its external interfaces (python and bash).
july 1995	oct 2000
march 1990	july 1995
harris corporation – melbourne, fl
developed software for an ir&d to monitor and control a large control system and performed automatic fault analysis on the results.
developed image processing software for both met and origin.
worked with a team in the development of automatic extraction of detailed 3d data from 2d images which resulted in a new patent.
cloudera, columbia, md
track record of results as a project manager in an agile methodology using data-driven analytics.
expert in project management tools such as microsoft team foundation server (tfs), jira, and ms project.
amazon aws - ec2, sqs, s3, mapr, elastic cloud
solr cloud, databricks, datastax
redshift, dynamodb, cassandra, apache hbase, sql
continuous integration (ci cd):
object-oriented programming, functional programming
file format and compression:
search tools:
security:
usaa, san antonio, texas	september 2017-present
hands-on experience with spark core, spark sql and data frames/data sets/rdd api.
spark jobs, spark sql and data frames api to load structured data into spark clusters.
wrote incremental imports into hive tables.
esl federal credit union, rochester, ny	november 2013-january 2015
used cloudera manager for installation and management of single-node and multi-node hadoop cluster.
dimancheolson@gmail.com
understands and articulates the overall value of big data; works effectively and proactively with internal and external partners.
.
tech skills
managed and reviewed hadoop log files.
university of floridawilliam m. siglerwilliam m. sigler  |   (703) 659-4419 |   wsmclemore1@gmail.com
8 years of experience in the field of data analytics, data processing and database technologies.
█    █    █    █    █    █    █    █    █    █
founding member of four-engineer team focused on creation of targeted credit policies for upmarket customer segment. policies were targeted across multiple online and offline channels. developed credit policy implementations across various upmarket card products for approvals, credit line adjustment, and counteroffers. the counteroffer policies are estimated to approve up to 50% of upmarket customers who otherwise would be ineligible for a card product, increasing revenues by tens of millions of dollars.
performed production support and post-release monitoring with logging and incident management software such as splunk and pagerduty
managed binary artifacts using jfrog artifactory to ensure reliable and secure builds with convenient access to snapshot and release versions.
•	developed and maintained spark/scala application which calculates interchange between visa, mastercard, and capital one, grossing hundreds of millions of dollars in revenue.
oversaw big data engineering tasks for team of eleven on-site engineers and other offshore teams.
hadoop data engineer / project manager
involved in testing mobile wifi hotspot technology by gathering data from comcast modems which are used for wi-fi free to subscribers, like a public utility. in exchange, comcast gets an enormous amount of data - with the idea being that the data can be used to offset the cost of the wi-fi in other areas. for example, sensors tell comcast about utilization, price optimization, and geographical variants.
city of long beach – long beach, ca
handled 20 tb of data volume with 120-node cluster in production environment.
technologies:  hadoop, hdfs, hive, spark, yarn, kafka, pig, mongodb, sqoop, storm, cloudera, impala
aug 2012- dec 2013
designed and oversaw implementation of large-scale parallel relation-learning system.
bank of america – charlotte, nc
provided technical support to colleagues to accomplish their project goals
brigham young university, idaho (gpa 3.8)william a thorndike
big data engineer & developer
seasoned hadoop/big data engineer skilled in the use of spark/spark streaming, spark data frames.  experience working with hadoop components, kafka, kibana and pyspark.  design and implement on-prem and cloud big data ecosystems and pipelines using hadoop and spark.
experience in implementing user defined functions for pig and hive.
extensive knowledge in development, analysis and design of etl methodologies in all the phases of data warehousing life cycle.
pig latin, hiveql, mapreduce, shell scripting, sql, spark sql
sql, nosql
elasticsearch
maintained elk (elasticsearch, kibana) and wrote spark scripts using scala shell.
integration of kafka with spark for real time data processing.
worked on continuous integration with jenkins and automated jar files at end of day.
cloudera implementation of several applications, highly distributive, scalable and large in nature using cloudera hadoop.
automated workflows using shell scripts pull data from various databases into hadoop.
western illinois university
• experience in apache nifi which is a hadoop technology and also
data processing integrated with functional programming language scala.
tableau.
• hands on etl, data integration, migration, informatica etl.
unix shell scripting, object-oriented design, object-oriented programming,
project management [pic][pic][pic]
agile, kanban, scrum, devops, continuous integration, test-driven
cloud services & distributions [pic][pic][pic]
aws, azure, anaconda cloud, elasticsearch, solr, lucene, cloudera,
unix, spotfie, ms office, teradata,
depot, lowes and staples.
• involved in preparing the s2tm document as per the business
partitions for the daily data.
before moving data into hdfs.
workflow.
schema
setting up these jobs in tidal/control m for incremental data
• optimized hive analytics, sql queries, created tables, views, wrote
to rdd and process data in the form of data frame and save the data as
• handled the real time streaming data from different sources using
administration and optimization of data pipelines and etl processing for
• analyzed the data by performing hive queries and running pig scripts
• used solr to enable indexing for enabling searching on non-primary key
columns from cassandra keyspaces.
• used spark sql to process the huge amount of structured data and
pl/sql queries into hql queries.
aug 2012    hadoop data engineer
analyzed large data sets distributed across cluster of commodity hardware
transformations using hive, yarn, loaded data into hdfs and extracted
scenarios, using json configuration files
• implemented the project by using spring web mvc module
tools like cloud era manager enterprise, and other tools.
sep 2010    java developer
solutions. used javascript for client-side validations.
server.
and production.
• responsible for developing dao layer using spring mvc and
configuration xml's for hibernate and to also manage crud operations
plus ajax and angularjs.
instituto de estudios universiarios, mexico
publications
authors: vicente guevara ayala, luis enrique colmenares guillén[pic]
efficient etl processes for realtime streaming and data analytics.  full-
vincente guevara
amazon aws - ec2, sqs, s3, azure, google cloud, horton labs, rackspace
i was also was responsible for creating the ci/cd pipeline to deploy the infrastructure to run this application.  i performed disaster recovery exercises
tokenized and detokenized records in the data frames, also calculated usage of kms keys to store files at rest.
provided automated solutions in case of a disaster to meet rto and rpo.
refactored maven projects to keep track of version control.
architected ecs, emr, microservices and ci/cd pipelines.
i was involved in the new low latency product development for bank transactions security, in order to reduce the latency on the current one in production and be able to detect fraud. i was responsible for analysis, design, and development of the use case solution. the new engine solution integrates kafka, flink, phoenix, hbase, ambari, yarn, hdfs, xml, nifi, drools and python scripts in order to deliver low latency and high-throughput, by matching the current transaction with a set of rules and sending back a response in high-speed to prevent fraud.
data ingestion by using python kafka producer and apache nifi to send an xml string which has all transaction data required to be transform, analyze and store in hbase.
developed poc to choose the best way to process and store data and be able to provide low latency response.
worked with flink event, process and ingestion time providing the timestamp extractor, watermarks and allowing late data.
used sql queries to get and upsert data into phoenix hbase tables.
windows azure
drools
horizon blue cross blue shield of new jersey, iselin, nj
used secure vlans for data transfer security to secure vpc on aws.
bedrock
used cassandra to work on json documented data.
through sqoop and placed in hdfs for further processing.
extensively used transformations like router, aggregator, normalizer, filter, joiner, expression, source qualifier, unconnected and connected lookup, update strategy and store procedure, xml transformations along with error handling and performance tuning.
implemented workflows using apache oozie framework to automate tasks. used spark-streaming apis to perform necessary transformations and actions on the real-time data using bedrock data management tool.
extensively worked on performance optimization of hive queries by using map-side join, parallel execution and cost based optimization.
replaced default derby metadata storage system for hive with mysql system.
windows management for virtual environments vmware and hyper-v and physical servers. where i managed the whole windows and vmware infrastructure automating and reducing time, human errors and costs, i was able to automate processes and repetitive work, also to keep the windows infrastructure up and running reducing down times from patching servers each month to keep them update and manage security controls for audit propose all following itil processes.
kafka cluster maintenance, trouble shooting, monitoring, commissioning and decommissioning nodes.
spark to work on streaming analyzed data to hbase and make available for visualization and report generation by the bi team.
development
hortonworks (hdp)
misc
 ingestion data through aws kinesis data stream and firehose from various sources to s3.
expert infrastructure engineers skilled in design, documentation and implementation of enterprise data services solutions.
experience working with clouderera distributions, hortonworks distributions and hadoop.
hdfs, mapreduce
sql, mysql, oracle, db2, redshift, amazon aurora, mongodb, dynamodb, cassandradb, amazon rds, arangodb
parquet, avro, orc, json, xml, csv
gulfstream
participated in a pipeline building project focused on the predictive engine maintenance and aircraft health analytics.
graybar electric
used sed spark sql to perform transformations and actions on data residing in hive.
mars, inc.
mars, inc. holding so many food and pet food product companies launched an effort to sample the vast resources of data available on food pathogens and microbiomes transmitted and transformed throughout the food process, packaging and distribution chain.
worked on installing clusters, commissioning & decommissioning of data node, configuring slots, and on name node high availability, and capacity planning.
used spark to transfer data with kafka to and from redshift and dynamo db stored on s3.
giant eagle
wrote sqoop scripts to inbound and outbound data to hdfs, and validated the data before loading to check the duplicated data.
worked with flume to load the log data from multiple sources directly into hdfs.
oracle university
able to drive architectural improvement and standardization of the environments.
hiveql, sql, pig latin, shell script language
continuous integration (ci cd):  jenkins, versioning: git, github, project method : agile scrum, test-driven development, continuous integration, unit testing, functional testing, scenario testing
used broadcast variables in spark, effective & efficient joins, transformations and other capabilities for data processing.
extended spark, hive and pig functionality by writing custom udfs and hooking udf's into larger spark applications to be used as in-line functions.
participation in profiling of source systems data for dwh. implemented etl procedures.
provided operational support for etl processes, olap cubes and other bi solutions.
used python and pyspark to create streaming data solutions for enterprise-wide analytics.
implemented usage of amazon emr for processing big data across hadoop cluster on aws.
aecom - houston, tx	jan 2015-feb 2016
used cloudera manager for installation and management of multi-node hadoop clusters.
bachelor of science in computer science and engineeringbig data engineer
█   █   █   █   █   █   █  professional profile  █   █   █   █   █   █   █
programming languages & ides
methodologies
may 2016 - present
thirdlove also used big data to identity and go after a target market that victoria’s secret – the major competitor was neglecting – nature women past their teens who has need of adult wear made for them.
developed cloud-based big data architecture using hadoop and aws which created the foundation of this enterprise analytics initiative in a hadoop-based data lake.
worked on streaming analyzed data to hbase using sqoop to make it available for visualization and report generation by the bi team.
loaded and transformed large sets of structured and semi structured data from hdfs through sqoop, and placed in hdfs for further processing.
delta air lines, inc – atlanta, ga
phone: 999-999-9999 |  e-mail: consultant@gmail.com
aws tools (redshift, kinesis, s3, ec2, emr, dynamodb, elasticsearch, athena, firehose, lambda).
wrote spark code to run a sorting application on the data stored on aws.
implemented advanced procedures of feature engineering for data science team using the in-memory computing capabilities like apache spark written in scala.
misc o.s. & software applications
architected a light weight kafka broker.
implemented a hadoop cloudera distributions cluster using aws ec2.
plano, tx
developed processes that allowed for conversion from any accounting software to sage 50 including mas90, daceasy, adp, great plains, and quickbooks online.
utilized root cause analysis to analyze bugs in software code and delivered solutions to business analyst.
united states marine corps
trained military personnel and civilians on how to use a variety of military specific software in various locations including japan, korea, and africa.
secret
bachelor degree in
projects
gns healthcare – hadoop hdfs
medline industries – hadoop hdfs
senior hadoop/big data engineer
experience in software development using big data/hadoop echo systems, apache spark, python and etl technologies.
developing spark programs using scala api s to compare the performance of spark with hive and sql.
implemented spark using scala and sparksql for faster testing and processing of data.
used spark-sql to load json data and create schema rdd and loaded it into hive tables and handled structured data using sparksql.
deployed to various hdfs file formats like avro, sequence file and various compression formats like snappy.
hadoop engineer / april 2016 – july 2017
american bureau of shipping abs houston, tx
involved in creating hive tables, loading with data and writing hive queries that will run internally.
used hive to do transformations, event joins and some pre-aggregations before storing the data onto hdfs.
the big data raw material can be a rich asset to discover the underlying disease mechanisms that help improve treatment effectiveness and patient care.  using gns’s big data analytics solutions, the researchers were able to develop detailed risk profiles for individual participants that helped them gain understanding of the underlying causes of the disease and identifying groups that were at risk.
developed map/reduce jobs using java for data transformations.
developed hive queries and udfs to analyze/transform the data in hdfs.
hadoop administrator / october 2013 – january 2015
experience working on various cloudera distributions like (cdh 4/cdh 5), knowledge of working on hortonworks and amazon emr hadoop distributors.
experience working closely with operational data to provide insights and value to inform company strategy.
configured linux on multiple hadoop environments setting up lab, dev, and prod clusters within the same configuration
hdfs monitoring job status and life of the datanodes according to the specs
managed zookeeper configurations and znodes to ensure high availability on the hadoop cluster
configure and set up of ranger policy to handle security among the groups
collaboration with the security management to sync kerberos with knox
setup solr collections to all environments and replications of shards
work one on one with clients to resolve issues regarding spark jobs submissions
work using agile methodology to utilize tasks and delegated between team member
develop a proof of concept to benchmark kubernetes and dockers
margins are tight in the ready-to-eat cereal industry. for a company like kellogg’s, approximately a third of its annual revenue is spent on promotional costs or trade spend: every dollar spent on coupons and special offers, promotions for special pricing, sponsorships, even the location each brand occupies on the grocery-store shelf.
created hadoop clusters using hdfs, amazon redshift for nosql along with arrangodb for multi-modal data warehouse solution.
the company logs events using aws identity and access management (aws iam).
created a data lake for offloading infrequently accessed data from data warehouse and for staging purposes
installed/configured sap vora to integrate with hdp cluster to leverage hana sps11 functionality
configured data ingestion accelerator tool in-house project to automate the ingestion process
generated around 200 million log entries (equivalent to 10gb of log data) per day.
managed jobs using fair scheduler to allocate processing resources.
associate of computer &technology in networking technology
senior hadoop engineer
data cleaning
university of michigan, flint
aws associate
major contributions included design, code, configuration and documentation for components that manage data ingestion, real time streaming, batch processing, data extraction and transformation.
configured sql database to store hive metadata.
hands on experience in linux shell scripting. worked with big data distribution cloudera.
setup, installed, and monitored 3-node enterprise hadoop cluster on ubuntu linux
analyze hadoop clusters using big data analytic tools including hive, and mapreduce.
leveraged sqoop to import data from rdbms into hdfs.
performance tune hadoop cluster to achieve higher performance.
wrote nagios plugins to monitor hadoop namenode health status, number of task trackers running, number of data nodes running.
implemented kerberos for authenticating all the services in hadoop cluster.
configured and deployed hive metastore using mysql and thrift server.
chris.tran430@gmail.com
big data engineering, hadoop
architecture of hadoop clusters in cloud-based distributed systems.
experience with data visualization tools, data analysis, and business recommendations (cost-benefit, forecasting, impact analysis).
cloud platforms:  amazon aws
cloud database & tools:  redshift, dynamodb,
file systems:  hdfs
involved in requirement gathering and analyzing the business requirements for the hadoop project, and involved in full life cycle of the project from design, analysis, logical and physical architecture modeling, development, implementation, testing.
cleaned hadoop data and prepared analytics tables for bi analysts
reviewed functional and non-functional requirements on the hortonworks hadoop project collaborating with stakeholders and various cross-functional teams.
developed sql queries to insert, update and delete data in database.
ability to troubleshoot and tune relevant programming languages like sql, java, python, scala, hive, rdds, dataframe dataset
5 years’ experience  engineering big data ecosystems such as hadoop, cloudera and hortonworks
bachelor  degree
microsoft sql server database administration (2005, 2008r2, 2012)
used data profiling techniques to profile, mine, and gain deeper understanding of the data to meet and refine the business requirements.
worked with automated data and analytic systems, relational databases, structured query language (sql)
(june 2016 – september 2017)
worked on loading and transforming of large sets of structured, semi structured and unstructured data.
performed cluster coordination services through zookeeper.
responsible for writing hive queries for data analysis to meet the business requirements.
used spark framework on both batch and real-time data processing.
(march 2015 – june 2016)
involved in scheduling oozie workflow engine to run multiple hive jobs
worked in an hadoop big data ecosystem on amazon aws using emr, ec2, sqs, s3, dynamodb, redshift, cloud formation.
experience in importing and exporting data using sqoop from oracle, my-sql db to hdfs and data lake.
proficient with cluster management such as ambari, hue and cloudera manager.
hive / hive ql scripts to extract, transform, and load into database, wrote hive udf’s and did incremental imports into hive tables.
perform transformation on raw data from s3 and save to oracle db
created uber jars and perform poc on emr
lead process to write raw data into oracle staging table
monitor active emr clusters and automate emailing to etl team
configured ci/cd pipeline through jenkins and github
mentored jr/new developers in spark/scala to deploy spark jobs for apple
hive partitioning, bucketing, performing joins on hive tables.
implemented data processing using hadoop cloudera  distributions on aws.
expertise
extract data from hdfs, mongodb, cassandra, hbase, hive, ms sql, and other.
ensure data accuracy by validating data for new and existing tools.
experience working on various cloudera distributions like (cdh 4/cdh 5).
programing languages, types, tools
hortonworks hadoop
database & file systems
microsoft office
drove poc"s to define the technolody roadmap
worked with test team to fix character issues within data.
involved in requirement gathering and analyzing the business requirements
engineers structured and unstructured data from source systems to fit business need
developed new topics to segment data from kafka and other web servers into hdfs.
used parquet file compression with snappy codecs for low latency data analysis
implemented partitioning, dynamic partitions, buckets in hive.
generate final reporting data using tableau for testing by connecting to the corresponding hive tables using hive odbc connector.
designing and creation of hive tables, and load data to hive.
provided quick response to ad hoc internal and external client requests for data.
bachelor of science in mathematics
17 years total experience i.t. in data systems with the last 5 years’ experience in big data architecture and engineering;
experienced team lead providing mentoring to engineers, and liaison for team with stakeholders, business units, data scientists/analysts and making sure all teams collaborate smoothly.
have good experience in extracting and generating statistical analysis using business intelligence tool
hands on experience in working with ecosystems like hive, pig, sqoop, mapreduce, flume, oozie. strong
hands-on experience developing teradata pl/sql procedures and functions and sql tuning of large databases.
skilled in database systems and administration
pig/pig latin, hiveql, mapreduce, xml, ftp,
tampa, fl
architected a pipeline to receive, resolve, normalize, route, persistence flows.
design and development of integration workflows.
managed highly available and fault tolerant systems in aws, through various api's, console operations and cli.
used etl to transfer the data from the target database to pentaho to send it to reporting tool microstrategy.
worked on various file formats like avro, orc, text, csv, parquet using snappy compression.
certificate in operations management
implemented big data analytical solutions that 'close the loop' and provide actionable intelligence
bachelor’s in computer science & engineering
migrated mapreduce jobs to spark, using spark sql and data frames api to load structured data into spark clusters.
experienced in managing and reviewing hadoop log files.
participated in development/implementation of cloudera hadoop environment.
successfully loaded files to hive and hdfs from mongo db, and created a data lake with hadoop hdfs and amazon redshift.
involved in creating hive tables, loading with data and writing hive queries, which will run internally in map, reduce way.
menlo park, ca
performance tuning and troubleshooting of mapreduce by reviewing and analyzing log files.
cluster coordination services through zookeeper.
wrote map reduce jobs using pig latin.
worked on cluster coordination services through zookeeper.
verona, wi
technical support for care everywhere, the interoperability product to exchange electronic medical records between organizations.
assisted with setup and ongoing support issues for product’s end-user facing side, back-end configuration, and windows server-side.
6 | pagehumberto.avelar12@gmail.com  / 703-879-2651
phone:  781-786-2394
prepare test cases, documenting and performing unit testing and integration testing.
ides
integrations
data lake, data warehouse, das, nas, san
apache solr, elasticsearch, apache lucene,
apache storm, apache hive, apache cassandra, apache hadoop, apache hadoop, apache hcatalog, spark mllib, graphx, scipy, pandas, mesos, apache tez, apache zookeeper, x-pack
junit, unit testing, functional testing, test-driven development
hadoop big data engineer	02.2014 – 06.2015
responsible for installing and configuring apache hadoop and tools on the cloud.
key technologies:  hadoop, hdfs, lzo, snappy, zookeeper, kafka, spark, hive, pig, sqoop,
implemented amazon redshift in the data lake as the primary storage for nosql data.
developed multiple mapreduce jobs in pig and python for data cleaning and processing.
analyzed the log data using the hiveql.
university of veracruz, veracruz, mexico
5 years in hadoop/big data
pandas experience with operational monitoring of clusters and applications.
proficient performing importing/exporting actions between sql (oracle, mysql) / nosql (realm, mongodb) databases and hdfs using sqoop.
hands on experience migrating complex mapreduce programs into apache spark rdd operations like transformations and actions.
nosql: cassandra, hbase | sql: sql, mysql, postgresql
list here
hands-on big data experience in writing applications on nosql databases like cassandra.
kemper insurance - chicago, il
optimized hive using partitioning and bucketing
building data lakes from scratch which is highly performant, scalable, fault tolerant multi-tenant. migrations from data silos to data lakes
architected modern data pipelines with hybrid architecture
troubleshooting and performance/memory tuning in spark with optimization using tungsten
developed spark code using scala/python and spark-sql/streaming for faster processing of data and data enrichment.
web frameworks
html5, css
deep learning
iot
tableau, kibana, talend,
amazon emr, ec2, sqs, s3, vpc, dynamodb, redshift, kinesis, redshift
set-up hadoop system automation using zookeeper and oozie for workflows, processes and job scheduling.
altice usa is one of the largest broadband communications and video services providers in the united states. altice usa has big plans for data analytics, with focus on long-term plans and investments for mergers, acquisitions and intent to build the most innovative, data rich, and intelligent advertising platform, offering advertising and mvpd clients the ability to implement multiscreen addressability and advanced analytics.  to this end, altice chose to implement advanced analytics using dataproc for google cloud, using google tools such as cloud dataflow and bigquery.
designed batch processing jobs using apache spark to increase speed.
participated in planning meetings and assisted with documentation and communication.
master of science in computer science
awards
poste presentations
“iot challenges in the future internet” based on internet of things”chingiz khalifazada
phone: 1 (925) 815-8108
ide: jupyter notebooks, pycharm, intellij, spyder, anaconda
query languages
development methodologies
moved relational data base data using sqoop into hive dynamic partition tables using staging tables.
implemented enterprise security measures on big data products including hdfs encryption/apache ranger. managing and scheduling batch jobs on a hadoop cluster using oozie.
phone: 425-372-7246
belay goytom
unix shell scripting, java, sql, hive ql, spark, spark streaming, spark mllib, spark api, avro, scala, python, parquet, pig, orc, microsoft powershell, c, c#, vba.
the mobile operator has integrated big data across multiple it systems to combine customer transaction and interactions data in order to better predict customer defections. by leveraging social media data (big data) along with transaction data from crm and billing systems, t-mobile usa has been able to “cut customer defections in half in a single quarter”.
participated in daily scrums which involve discussing outstanding issues in implementing project flow.
used r and microsoft excel on 2 projects for tracking and reporting machine status and inventory levels within multiple departments
apache ant, apache cassandra, apache flume, apache hadoop, apache hadoop yarn, apache hbase, apache hcatalog, apache hive, apache kafka, apache maven, apache oozie, apache pig, apache spark, spark streaming, spark mllib, graphx, scipy, pandas, rdds, dataframes, datasets, mesos, apache tez, apache zookeeper, cloudera impala, hdfs, hortonworks, mapr, mapreduce, apache airflow and camel, apache lucene, elasticsearch, elastic cloud, kibana, x-pack, apache solr, apache drill, presto, apache hue, sqoop, kibana, tableau, aws, cloud foundry, github, bit bucket, pentaho, kettle.
october 2018 	big data engineer
credit policies
involved in testing x1 products by gathering data from comcast x1 xfinity dvrs, which are used for search and play multimedia content for every subscriber. in exchange, comcast gets an enormous amount of data - with the idea being that the data can be used to offer the most appropriate content for the user according to their likes and their experience of the platform. for example, selected menu options on the dvrs tell comcast about utilization, type of content, and geographical variants.
migrated complex mapreduce programs into apache spark rdd operations.
excellent java, j2ee application development skills with strong experience in object oriented analysis, extensively involved throughout software development life cycle (sdlc).
jan 2012	hadoop big data administrator
involved in scheduling oozie workflow engine to run multiple hive, sqoop and pig jobs.
apache storm, kafka, flume, sqoop, hive, along with hadoop distributed
hortonworks, impala, and various mapr, anaconda, jupyter notebooks,
java programming.
• accustomed to working with large complex data sets, real-time/near
zookeeper, sqoop, kafka-storm, spark, flume, and oozie.
• hands-on experience on yarn (mapreduce 2.0) architecture and
data analytics.
• created classes that simulate real-life objects, and write loops to
• expertise in development of multi-tiered web-based enterprise
applications using j2ee technologies like servlets, jsp, jdbc, java
• detailed knowledge and experience of design, development and testing
and maintaining the web applications using the web server tomcat
functional programming,
db2, sybase, rdbms
hadoop, apache hcatalog, spark mllib, graphx, scipy, pandas, mesos,
to build real-time data integration systems.  apache storm is used to
amount of geographical, historical and weather-related data to be used
• collect, aggregate, and move data from servers to hdfs using apache
• used impala where possible to achieve faster results compared to hive
• created hive external tables and designed data models in hive.
jobs submitted by users.
jan 2015    data processing architect/software engineer
worked in alexandria for luxoft developing real-time data processing
data frames, spark sql, sql context, and scala
installation, configuration, issues faced and their resolutions, pig
• analyzed the data using hiveql to identify the different correlations
and unstructured data coming from unix, nosql and a variety of
• evaluated data import-export capabilities, data analysis performance
• created hbase tables to store variable data formats of pii data coming
• imported avro files using apache kafka and did analytics using spark
• active part in applying real-time batch processing in customer service
• compared the execution times for the functionality that needed joins
• worked in agile environment, delivered sprint goals, familiar with
• used real-time batch processing to detect and discover customer buying
optimize the customer experience. this leads to more sales and happier
imported data (csv, plaintext) from amazon s3 using hdfs commands
eclipse, amazon s3, jd edwards enterprise one, jira, git stash
reducing electricity consumption by leveraging the features and
• real-time streaming the data using spark with kafka.
• importing and exporting data into hdfs using sqoop and kafka.
retail data.
the business users to analyze and visualize the data using datameer.
process to the hadoop and the data is back fed to retail legacy
mainframes systems.
• wrote hive and pig scripts as etl tool to do transformations, event
processing, and tracking of claims from vendors to provide estimates
to streamline the process by using the hadoop ecosystem.  using hadoop
• managed and reviewed hadoop log files.
nov 2009    java developer
industry, pulling data from various sources and file formats.
experience in object oriented analysis, extensively involved
• software and system development using jsp, servlet, java server face,
deployment technology.
• integrated the java application to end-users.
• improved user interface for the database by reducing user input with
• provided a ready-made form for a repetitive process to help eliminate
• improved quality of delivered products resulting in a 300% increase of
master of software engineering
francisco granados torres
(208) 297-6165 franciscogranados06@gmail.com
professional it summary
12 years of experience in building ad hoc bi solutions using technologies within the microsoft business intelligence platform in the banking, post-secondary education, and food manufacturing industries.
4 years of experience in installation of sharepoint 2007 and 2010.
6 years of experience in data migration
sql server reporting services (ssrs) to build reports based on t-sql and mdx queries.
creation of etl processes to transform data to one consistent format for data cleansing and analysis.
designed and implemented sql server database objects (stored procedures, functions, views and complex t-sql/queries) to support data integration (ssis), reporting and various business processes.
business intelligence tools
sql server 2005/2008r2/2012, oracle, microsoft access
reduce the number of pending reports from 140 to 35
organized build and deployment processes and created and maintained build scripts in powershell.
created ssis packages to extract data from uccx cisco database information for call log and campaigns statistics.
micron technology – boise, id
demand forecast cube took 3 hours daily on end to end process (etl's to cube).
design and create demand forecast datamart
business intelligence (bi) tools to support transactions and business decision-making.
create ssrs reports and maintain/enhance existing reports
design, development, implementation, and maintenance of tabular models.
gathering requirements for enhancements and new bi developments.
migrated the ssis etl process for a specific business unit from sql to iq based source. this implementation was required for the financial unit to support projections and trend analysis for executive decisions.
create ssis etl packages to extract and transform data from multiple sources such as csv, excel, ssas cubes, sharepoint lists, and sql
performed administration for the olap cubes within obiee.
northware, gruma headquarters – monterrey, mexico
performed a migration from windows server 200 and 2003 to windows server 2008r2.
migrated the academic, teacher, and student models from sql server 2000 dts to ssis 2005 dtsx packages.
designed a real time olap cube using ssas for the enrollment analytic model.
designed, developed, implemented, and maintained the ssrs reports suite of the enrollment analytic model for desktop and mobile users.
responsible for the installation, configuration, test and administration of analytic tools such as qlikview, tableau, panorama and reportportal.
created lambda to process the data from s3 to spark for structured streaming to get structured data by schema.
cloud platforms
aws amazon cloud
security and authentication
kibana, tableau, crystal reports 2016, ibm watson
involved in writing unit test cases for hadoop and spark applications which were tested in mrunit and scalaunit environments respectively.
initialized a data modeling of cassandra to updated and maintained chef cookbook.
aws cloud formation templates used for terraform with existing plugins.
loaded the data from different source such as hdfs or hbase into spark rdd and do in memory data computation to generate the output response.
i am a big data professional with 10 years of experience in information
responsibilities while staying organized.  proficient in problem-solving
using flume.
|and frameworks.                     |data visualization                  |
|pspice • multisim • microsoft office|maven, apache oozie, apache pig,    |
|big data platforms                  |dataframes, datasets, mesos, apache |
being applied in monitoring to collect data from sensors.  this analysis
safety of buildings, regulation and control of electricity grids, control
hadoop data engineer   jun 2012 - feb 2014
and validation to retrieve more reliable information on risk assessment and
investment.
implemented high availability of name node, resource manager on the hadoop
data ingestion is done using flume with source as kafka source & sink as
data into spark clusters.
of clusters.
implemented partitioning, dynamic partitions and buckets in hive for
created partitions, buckets based on state to further process using bucket
server for report and dashboard development.
involved in creating hive tables, loading the data and writing hive
worked with amazon web services (aws) and involved in etl, data integration
worked with different file formats and compression techniques to determine
logs, and foreseeing and preventing potential issues, and escalating issue
successfully loaded files to hdfs from teradata, and loaded from hdfs to
developed client-side testing/validation using javascript.
quality improvement, and risk management.  i have focused on hadoop big
apache kibana, tableau, microsoft power bi
hadoop data architect/engineer    may 2016   present
• optimization of data storage using partitioning and bucketing
• hadoop data ingestion and hadoop cluster handling in real time
• storage capacity management, performance tuning and benchmarking of
hadoop clusters.
turnkey big data analytics solutions.  these solutions aimed at value-based
healthcare to optimize provider networks and population health management.
spark, yarn, kafka, pig, mongodb, sqoop, storm, cloudera hadoop and
• oozie workflows used to run hive and pig jobs in hadoop ecosystem for
• used hive, spark sql connection to generate tableau bi reports from
• apache kafka to transform spark streaming with the batch processing to
file system (hdfs using spark streaming & kafka, and wrote complex
distributed file system (hdfs).
hadoop data engineer   jan 2014 - may 2015
• used oozie to automate/schedule business per the requirements.
(hdfs using sqoop.
distributed file system (hdfs)  and used flume to stream the log data
• used oozie scheduler system to automate the pipeline workflow and
• worked on hive for exposing data for further analysis and for
• design and of large database systems: oracle 8i and oracle 9i, db2,
• involved in design phase meetings for business analysis and
business needs while minimizing the impact to hr dw and other
• led the technical lifecycle of data presentation from data sourcing to
loads, addressing user questions concerning data integrity, monitoring
time, level of parallelism, and memory tuning, and changing the
configuration properties, and using broadcast variables.
• skilled in phases of data processing (collecting, aggregating, moving
architectures, systems, and processes that make a vast difference in the
← significant contribution to the development of big data roadmaps.
← able to work with existing eds platforms and strategic initiatives
← worked with various file-formats (parquet, avro & json) and
cleaningdatacleaner, wainpure, patnab, openrefine, drakepipelinehadoop hdfs
reportinghive, spark, spark streaminggoogle analytics
searchlucene, elastisearch, apache solrkibana, drill, presto
tableauadministrationetl
architectural planning
gliffy
virtual serversdevelopment, 
present apple – sunnyvale, ca
evaluated zeppelin-0.7.3 apis.
created a ticket in central station with their api to log failed jobs from spark.
involved in converting hive/sql queries into spark transformations using spark rdds, python, and scala.
design and implementation of secure hadoop cluster using kerberos.
hadoop cluster performance monitoring and tuning, disk space management
worked on data modelling using various ml (machine learning algorithms) via r and python (graph lab).
created partitioned tables in hive.
extensively used pig for data cleansing.
greatly reduced the number of blocks to ease cluster memory stress
worked with amazon web services (aws) and involved in etl, data integration, and migration.
configured kerberos for the clusters
scooped over 36tb of data from teradata and transformed to parquet
used beautifulsoup for extracting data from html and xml files.
experience in deploying reports to system test and product test environments.
create, customize & share interactive web dashboards in minutes with simple drag & drop method and access dashboards from any browser or tablet.
effectively made use of table functions, indexes, table partitioning, collections, analytical functions, materialized views, index, synonyms, and views.
hands on experience on sql server 2005, 2008, 2008r2, 2012 native and sharepoint integrated mode reporting services.
extensive experience in controlling the user privileges, monitoring the security and audit issues as per enterprise standards
transferring streaming data from data sources to hdfs & hbase w/apache flume.
importing of data from various data sources, performed transformations using hive, loaded data into hdfs and extracted the data from relational databases like oracle, mysql, teradata into hdfs and hive using sqoop.
business analysis, data analysis, use of dashboards and visualization tools, ssrs, ssis, power bi, tableau, qlik view, pentaho, microsoft visio
big data consultant & developer, march 2018- present
scrum based project, having diverse ceremonies like daily scrum meetings, backlog grooming sessions, sprint planning, sprint retrospective.
development of hive scripts for data extraction and transformation for daily, weekly, monthly and quarterly reports.
responsible for the development of the kafka consumer (spark-karka) source code.
using sqoop to move the structured data from mysql to hdfs, hive, pig, and hbase.
creating the hive tables and partitioned tables using hive index and bucket to make ease data analytics.
big data architect-engineer/ october 2015 – april 2017
further used pig to do transformations, event joins, elephant bird api and pre -aggregations performed before loading json files format onto hdfs.
execution and debugging commands to run optimized code.
good understanding of partitions, bucketing concepts in hive and designed both managed and external
tables in hive to optimize performance.
johnson controls, inc.
this johnson controls project was the building of a pipeline and data frames and datasets for analysis which helped the company pinpoint issues, and prioritize actions and investments to maximize roi.
involved in collecting, aggregating and moving data from servers to hdfs using flume.
migrated complex programs to in-memory spark processing using transformations and actions.
worked on creating the rdd's, df's for the required input data and performed the data transformations using
transformations, read/write operations, and save the results to output directory into hdfs.
this energy company used data warehousing to accumulate data from use and we transferred all of this data over to a more efficient storage system using hive, pig, hdfs to manage data and queries more efficiently, which greatly improved the reporting process.
extensively worked on performance tuning of hive scripts.
developed the sqoop scripts in order to make the interaction between pig and mysql database
analyzed the web log data using the hiveql.
bi analyst/data engineer/ february 2012 – may 2013
used tableau for dashboards, analysis, and reporting.
hands on experience in working with cloudera distributions.
performing the detailed analysis on the requirement to break them down into use cases, user stories and subsequently functional requirements.
ministry of finance
amazon web services (aws) solutions architectdata engineer
ability to troubleshoot and tune relevant programming languages like sql, java, python, scala, pig, hive, rdds, dataframes & mapreduce. able to design elegant solutions through the use of problem statements.
experience in collecting the log data from different sources (web servers and social media- tweets) using flume and storing in hdfs to perform the mapreduce jobs/hive queries.  knowledge in installing, configuring, and using hadoop ecosystem components like hadoop map reduce, hdfs, hbase, oozie, hive, zookeeper, sqoop, kafka-storm, spark, pig, impala, and flume.  experience in installation, configuration, supporting and managing - cloudera's hadoop platform along with cdh4 & cdh5 clusters, hdp 2.2 with kafka-storm and ec2 platform, ibm's big insight hadoop ecosystem.
operatign systems
(408) -784-5721
utilized spark data frame and data set through spark sql api for optimized processing.
performed data ingestion, entity resolution and ran ad-hoc queries using hdfs and hive.
developed new flume agents to extract log data from data sources into hadoop file system (hdfs).
created hive managed and external tables with partition and bucket in hive and loaded data in to hives.
admin
hdp, cluster
improved performance of data pipeline by transitioning from the existing data frame api and by modifying the schema as a python list, then applying it to the data.
used spark structured streaming with spark sql engine to process real time structured data.
jan 2017 – mar 2018
mattel
improving big data ecosystems using hadoop, spark, microsoft azure, amazon
• experience collecting real-time log data from different sources like
webserver logs and social media data from facebook and twitter using
unix shell scripting, sql, hive ql, python, scala, xml, blueprint xml,
ajax, rest api, spark api
•     implemented workflows using apache oozie framework to automate tasks.
batch interval time, correct level of parallelism, selection of correct
•     collect, aggregate, and move data from servers to hdfs using apache
•     involved in creating hive tables, loading with data and writing hive
anomaly detection.
•     worked on importing the unstructured data into the hdfs using spark
•     handled 20 tb of data volume with 120-node cluster in production
•     involved in creating hive tables, loading the data and writing hive
•     developed metrics, attributes, filters, reports, dashboards and also
•     imported data into hdfs and hive using sqoop and kafka. created kafka
•     involved in scheduling oozie workflow engine to run multiple hiveql
•     administered hadoop cluster(cdm) and reviewed log files of all
•     set-up qa environment and updated configurations for implementing
• involved in researching various available technologies, industry
trends and cutting-edge applications.
• collected the business requirements from the subject matter experts
• load and transform large sets of structured, semi structured and
• worked on streaming the analyzed data to hive tables using storm for
analyze the data.  the company has a tremendous existing well count from
environment: hadoop cluster, hdfs, hive, pig, sqoop, linux, hadoop,
• experienced as red hat enterprise linux administrator.
• performed red hat linux kickstart installations on redhat 4.x/5.x,
may 2008    bi developer
developed custom j2ee and ejb for custom analytical platforms in public
• conducted classroom and staff training sessions.
data centers.
experience in deploying and managing the multi-node development, testing and production hadoop
extensive experience with using hortonworks ambari and cloudera impala, spark, storm, and kafka.
extensively worked on configuring namenode high availability, and hadoop cluster disaster management.
developed add-ins using angularjs, rest / json, jquery to create customized web forms.
environment:
hadoop yarn, spark core, spark streaming, spark sql, scala, kafka, hive, sqoop, amazon
affirma – seattle, wa
developed udfs for hive and pig, and worked on reading multiple data formats on hdfs using scala.
developed multiple pocs using scala and deployed on the yarn cluster, compared the performance of
spark, with hive and sql/teradata.
import of data using sqoop from oracle to hdfs.
import and export of data using sqoop from or to hdfs and relational db teradata.
developed poc on apache-spark and kafka.
state farm – bloomington, il
computing capabilities like apache spark written in scala.
involved in running ad-hoc query through pig latin language or hive.
data analytics developer
led a big data project on a gigantic amount of taxonomy data and customer portfolio using hadoop, cloudera, hive, pig, hdinsight, and facilitating real-time data which was both analyzed and also in real time restructured the dell website on the demographic portfolio of the customers.
used customized faceting to overwrite the default search criteria.
wrote variation of batch files, python for solr/lucene deployment and configurations.
created taxonomy data visualization using the cloudera visualizations, dashboards, and reports to monitor customer profile, demography, and other useful data. other visualization tools were also created using c#.
used powershell scripts to automate user admin tasks.
managed backup and recovery of active directory and database systems
placed the reusable html contents like the header and the footer in the reusable library.
planned and executed the application design based on these requirements for a dedicated ecm portal, an extra-net partner portal, and an internal collaborative portal
utilized multi-threading for improved performance and better management of resources.
involved in the design for remote upgrade, remote monitoring applications and auto-updated of service packs
e: consultant@gmail.com
primary technical skills in hdfs, spark, kafka, hive, sqoop, hbase, flume, oozie, zookeeper, yarn.
etl from databases such as sql server and oracle to hadoop hdfs in data lake.
agile, kanban, scrum, jira, continuous integration (ci cd), test-driven development, unit testing,
big data visualization
used spark sql to perform transformations and actions on data residing in hdfs.
kellog brown & root (kbr)
implemented enterprise security measures on big data products including hdfs encryption/apache ranger managing and scheduling batch jobs on a hadoop cluster using oozie.
b.s. in computer science
manhattan, kansas
phi theta kappa fraternal order of honor students
• experience in xml technologies like informatica xml parser & xml
|cassandra, apache lucene, apache    |hortonworks, cloudera, impala       |
|xml, ajax, json, avro, parquet, orc |aws, azure, anaconda cloud,         |
clusters, bigquery, and storage.
• perform tuning, firewall setup, monitor and troubleshooting cluster-
the team developed a series of econometric regression models that predict
• hands on experience on cloudera distributed apache hadoop (cdah) for
feb 2013    hadoop data engineer
validation, filtering, sorting, or other transformations.
drive value for both the usaa membership and the enterprise marketing using
• created hive fact tables on top of raw data from different retailer's
• developed highly complex python and scala code, which is maintainable,
may 2007    tac analyst
problems.
teams.
• worked with onsite and offsite support staff and vendors as needed to
applications and systems to navy enterprise enclaves, or to
feb 2002    systems support specialist
• developed static and dynamic web pages using html and css.
• worked on javascript for data validation on client side.
and visual basic and web base application. developed queries and reports
september 1998) 68f/10 aircraft electrician repair wiring on aircraft on
67n/10 uh-1 huey mechanic. perform routine maintenance helicopter, remove,
controls, and hydraulic.
air national guard 2e231 computer, network, cryptographic and switching
operational checks, fault isolation, and pc measurement, processors,
input/outputs power supplies, cable construction, transport systems to
operated the integrated navigation system, where information from satellite
and sonar technology together with gyro and compass data is channeled,
quality control of positioning data and associated deliverables,
• 3+ years professional experience with modeling and analysis,
zookeeper, cloudera impala, hdfs, hortonworks, apache airflow and camel,
analyzer (mbsa) aws (configuring/deploying software)
|             |hadoop data analyst/architect                               |
commodity markets, food marketing, agricultural trade, diet and health,
queries for hadoop data processing.
models.
• implemented best practices to improve tableau dashboard performance &
infrastructure servers for git and chef.
• implemented hadoop data ingestion and cluster handling in real time
system (hdfs).
|[pic]        |may 2015 - may 2016                                         |
predictive analytics methods on market trends, risk, compliance, and
investment analytics, and valuation to state insurance regulatory agencies.
• spark context, spark -sql, dataframe and pair rdds in hadoop
• hands on with serverless architecture using services like lambda and
different consumer applications.
•  extracted the data from rdbms (oracle, mysql) to hadoop distributed
file system (hdfs). using sqoop.
•  transferred data using informatica tool from aws s3, and used aws
and running.
system (hdfs) data, and used avro and parquet file formats with orc
increased regulations have put enormous pressure on banks to demonstrate
banks facing regulatory fines. central to this is the ability to calculate
separate trading desks. this involves using hadoop to bring together this
conditions.
environment: maven, sql, xml,
• connected to read, write data.
university of baltimore, baltimore, md
between all hadoop clusters.
collection of real-time log data from webserver logs, social media data
python, scala, pig, hive, rdds, dataframes.
(hdfs), hadoop   +,-45rt½ùúéîñöÿ" i     éòéòé»¤“‚sdufsfsfs7sfhýƒh,-
build of multi-tenant hadoop cluster.
the creation of platforms, pipelines, and process.
• experience in developing applications using rdbms, hive, linux/unix
• experience in gathering & analyzing the business requirements and
modeling and report development.
• experience in creating scripts and macros using microsoft visual
exceptional ability to learn and master new technologies and to
deliver outputs in short deadlines
pig core functionalities.
relational database systems (rdbms), teradata and vice versa.
• knowledge in writing unix shell scripts.
data profiling, and etl processes features for data warehouses.
|intellij, pycharm                   |warehouse sql database, nosql       |
|working relationships               |                                    |
|big data platforms, software, & tools                                     |
|mllib, graphx, scipy, pandas, rdds, dataframes, datasets, mesos, apache   |
functional requirements document.
• design cloud architecture on aws, spin up a cluster for developers
• performance tuning of the big data components to meet the sla which is
critical for the customer.
join etc.
virtualization and to generate reports for the bi team.
may 2015    airbus usa – mobile, al
jan 2013    bi developer
various file formats from various sources into hadoop hdfs with hive,
404-9601549
isra.vertiz@gmail.com
languages: python, asp.net, c#, wcf, mvc, vb.net, som, csom, jsom, css, javascript, jquery, bootstrap, powershell, html 5, dhtml, xslt, xml, web services (soap, rest), azure.
sql database: azure sql, sql server 2012, sql server 2008, sql server 2008 r2, sql server 2005, oracle 10g, mysql.
cloud technologies: aws, emr, ec2, ec3, s2, s3, azure, google, elastic cloud
over 10 years’ experience in the it industry providing enterprise solutions.
use of columnar file formats like rcfile, orc and parquet formats.
hands-ono experience building large-scale, high-availability, distributed, enterprise-grade systems.
expertise in microsoft business intelligence technologies, azure data factory, azure data warehouse, power bi data visualization and reporting, and azure data pipeline architecture and tools such as azure data factory and azure stream analytics.
performed performance tuning and productivity improvement activities.
in charge of migrating on premises data operations to a customized, cloud-based data analysis platform.  responsible for planning, architecture and implementation of the entire project including the data migration and the new cloud-based environment using microsoft azure hdinsight based on hortonworks hadoop (hdp).  led a team of engineers and worked with stakeholders, project management and cross-functional teams, including analysts and smes.
administered azure data platform including azure data store and azure blob storage.
deployed and configured virtual machines on azure compute, apps on azure platform as a server (paas) and websites using .net, node, php, python on azure webjobs.
served as program manager supervising 40 remote developers, providing mentoring, support, review and management oversight.
installed ranger in all environments for second level of security in kafka broker.
worked on oozie job scheduler.
installed docker for utilizing elk, influxdb, and kerberos.
successfully upgraded hdp in all environments and software patches.
sharepoint developer / administrator
created scripts using stsadm commands for the automation of processes such as backups and list/library migrations from one environment to another.
set up managed metadata in a brand new sharepoint 2010 environment.
rebranded sharepoint 2010 environment to match the corporate standards by created a custom solution in visual studio including new master pages and page layouts.
leon li
5 years of experience engineering big data environments on premise and cloud environments using amazon web services (aws) for hosting cloud-based data warehouses, and databases using redshift, cassandra, and rdbms sources.
spark, kafka, spark streaming
cloudera manager, ambari, zookeeper
big data engineer	06/2018 - present
setup elk collections to all environments and replications of shards
work with off-shore team to troubleshoot oozie jobs, elk, hive, ranger in all environment
worked on aws cloud formation templates for using terraform with existing plugins.
used aws cloud formation to ensure successful deployment of database.
developed docker images to support development and testing teams and their pipelines and distributed images like jenkins, selenium, jmeter and elasticsearch, kibana and logstash (elk) and handled the containerized deployment using kubernetes.
implemented usage of amazon emr for processing big data across hadoop cluster of virtual servers on amazon elastic compute cloud (ec2) and amazon simple storage service (s3)
setup the scala scripts to create the snapshots on aws s3 buckets and delete the old snapshots.
created dataframe to store the processed results in a tabular format.
worked on tickets related to various hortonworks hadoop/big data services which include hdfs,  yarn, hive, sqoop, spark, kafka, hbase, kerberos, ranger, knox.
location houghton, michigan
expert analytics manager specializing in engineering and predictive/prescriptive analytics using hadoop, spark, mllib, and related ecosystem tools along with reporting a visualization tools (tableau, powerbi).
involved in implementation of analytics solutions through agile/ scrum processes for development and quality assurance.
documented the requirements in project approval document for approvals from stake holders and further reference in subsequent phases of the project.
environment: hadoop cloudera distribution, spark, spark streaming, kafka, hive, hdfs services, microsoft azure, microsoft team foundation server (tfs), big data, distributed computing.
created various dashboards which includes claim analysis, sales management, broker management and fraud analysis.
developed tableau visualizations and dashboards using tableau desktop.
made use of data view, relationship view and report view in microsoft power bi.
applied technical skills using sas, tableau and sql in data collection, data analysis and reporting to procure data from database structures to report and provide solutions to client requests in a timely manner.
participated in the design, development, deployment and support of reporting solutions in the areas of sales and marketing, forecasting and planning, and supplier performance to identify, investigate and
environment:  olap, molap, sas, sql, tableau, business objects, reports, drill, sql
interview business leaders to develop a feasible objective.
analyzing and predicting the behaviors of individuals to help avoid conflict.
cpr/first aid certifications
neerxxxxxxxxxxxxxxxxxxx@gmail.com
assisted in the implementation and debugging of a dimensional/fact model in snowflake
used matillion scripting to track active status, created & updated dates for tables
calculate a rolling median value in kstreams using windowing functions and watermarks.
december 2016- february 2019
sensely, san francisco, ca
performed etl operations between data warehouse and hdfs.
data administrator
handled importing of data from various data sources, performed transformations and analysis using hive and mapreduce.
managed, configured, tuned and continuous deployment of 80 hadoop nodes in a red hat enterprise edition 5; configured via the aws console for 2 medium scale ami instances for the name nodes
5+ years total information technology
email: mangornongfrancis@gmail.com
data frameworks and tools
my major contributions to the project included:
these workflows would run on their custom provisioned spark/hadoop clusters (abstracted by an enterprise cluster provision that was maintained by a different team) to avoid queueing and resource contention faced by running via the on-premise airflow instance.
documentation was managed via confluence articles
all issues/tasks and feature work were managed via jira
collaborated with data delivery teams for feature requirements, user acceptance testing, and general user feedback on the system and to ensure sla deadlines.
developed the transactional database system as the application’s backend.
key technologies:  google cloud platform, microsoft azure, hadoop, spark, cloud (gcp, azure), python, sql (postgresql), flask, kubernetes, docker, unix/linux
worked on hortonworks hadoop (ibm big insights) cloud-based system to configure hadoop clusters and kafka clusters, and data pipelines.
tested and configured web server with spark cluster; tested zeppelin with sparksql and python clients (pluggable interpreters).
key technologies:  hadoop, hdfs, zookeeper, aws, ec2, dynamodb, sql, dataflow, data models, class models, unix, zookeeper, yarn, spark, python, scala
used apache yarn for resource allocation on hadoop clusters.
use of end to end hive queries to parse the raw data and use of hive to populate internal/external tables.
use of hive functions such as partitioning, bucketing, index and udf.
may 2019 to present
discovery channel – sterling, va
assisted other team members through knowledge transfers and insight on their respective projects.
managed external applications needed for authentication for certain services such as doubleclick for publishers.
technologies:  aws, hive, spark, postgresql, jenkins, agile, scrum
worked with aws emr and s3.
ciena - alpharetta, ga
florida international university, miami, fl
[pic] hadoop big data architect and engineer
• specializing in cloud architecture using aws and google cloud
components like hive, hbase, pig, sqoop, zookeeper, and oozie
decision-making for enabling effective solutions leading to high
customer satisfaction and low operational costs.
hadoop, cloudera, hortonworks, hdfs, hive, spark, storm, kafka, pig, hbase,
storm
maven, apache oozie
google cloud, horton labs, rackspace, adobe, anaconda cloud, elastic
|elastic                           |storm                             |
|database                          |web & app servers                 |
✓ designed and built data linking and enrichment processes using
✓ collaborate with users and technical teams to implement requirements
nike, beaverton, or    november 2014-april 2016
proper hdfs locations.
0.12, hdfs, unix, shell scripting, hue, hbase 1.9, tableau 8.2,
✓ created innovative solutions in media streaming and mobile user
✓ worked with application teams to install operating system, hadoop
✓ developed multiple jobs for data cleaning and preprocessing
✓ responsible to manage data coming from different sources.
for visualization and to generate reports for the bi team.
✓ responsible for cluster maintenance, adding and removing cluster
hadoop log files.
while testing the application.
✓ created and executed various scenarios, generated graphs, overlaid
✓ involved in developing detailed test cases including test steps and
and actual outcome.
✓ used vb script to develop a hybrid automation framework in uft.
✓ environment: selenium, hp alm, uft, jira, soap ui, ms visio, .net,
phone:  999-999-9999
extensive experience on importing and exporting data using flume and kafka.
amazon aws (emr, ec2, ec3, sqs, s2, s3, redshift), azure cloud, google cloud
python, html, c++ programming languages, running scrips with powershell and bash
teradata
etl/data pipelines
created the hadoop system to be easy to access usable data derived from hadoop data lakes and hadoop clusters for predictive analytics.
designed and developed interactive tableau dashboards and custom reports which involved modeling and algorithms from data derived from hadoop data lakes and pipelines.
•	used zookeeper for providing coordinating services to the cluster.
•	moving data from oracle to hdfs and vice-versa using sqoop.
used matrix management to define role and resource allocation.
adjusted application security parameters as needed.
citrix, iis, winsows server 2000, 2003, exchange 5.5, web sense, norton anti-virus, cisco, active directory, windows nt
supported 80 plus users, setting-up and implementing new servers and workstations, ensuring all security measures were properly implemented and kept up-to-date.
built new custom servers & workstations. assisted users and outside sales representatives with all help desk requests.
aremukayode01@gmail.com 917-677-5790
experience in big data security using kerberos, ldap, active directory, apache ranger, apache kms, ssl, and apache knox gateway.
expert in leveraging apache yarn for workload management and resource management.
experience in data migration from rdbms for streaming or static data into the hadoop cluster using hive, pig, flume and sqoop.
hadoop ecosystem
architecture
rhel(6 &7)/centos/ubuntu
cluster monitoring
rdbms
sql, bash scripting, python
architected structured data on redshift and unstructured on dynamodb using amazon aws services.
aws administration, hardware architecture and scaling
verizon wireless
data warehousing: designed a data warehouse using hive, created and managed hive tables in hadoop
strong experience on hadoop system administration (hortonworks/ambari) and linux system administration (rhel 7, centos.)
cleaned and made necessary changes to data as part of data transformation.
chicago state university
(908) 388-1632
about
configured the elk stack for jenkins logs, and syslogs
jersey city, nj
trained it professionals in python and spark that at the end of the program will be able to understand capabilities, features, custom solutions and limitations in order to deliver high-quality solutions to a model of the data processing by using the pyspark programs for proof of concept.
wrote streaming applications with spark streaming/kafka.
created infrastructure design for elk clusters.
coordinated kafka operation and monitoring with dev ops personnel; formulated balancing impact of kafka producer and kafka consumer message(topic) consumption.
working on aws kinesis for processing huge amounts of real time data.
big data developer	aug 2015- oct 2016
spark api over hadoop yarn to perform analytics on data in hive.
configured hadoop components (hdfs, zookeeper) to coordinate the servers in clusters.
education
analytics academy: google analytics (google)
experience collecting log data from various sources and integrating it into hdfs using flume; staging data in hdfs for further analysis.
developed scripts and automated end-end data management and sync between all the clusters.
extending hive and pig core functionality by writing custom udfs.
data pipeline tools
apache airflow, apache camel, apache flink/stratosphere, nifi
features:
summary: as part of a risk oversite committee, my task was to write a python script that helps in generating reports from the company’s internal websites.
daily incident reports, monthly business review reports, incident graph generation etc.
used spark sql and dataframes api to load structured and semi structured data into spark clusters.
data engineer		january 2016 – august 2017
ford motor
managed, configured, tuned and continuous deployment of 80 hadoop nodes in a red hat enterprise edition 5; configured via the aws console for 2 medium scale ami instances for the name nodes.
worked at storing non-relational data on hbase.
us bancorp
ibm – hadoop 101
▪ experience collecting log data from various sources using various file
▪ manages integration of files of various file types into hdfs using
cluster.
orchestrating the etl process.
▪ skilled in hadoop architecture, implementation and configuration of
▪ design and implementation of hadoop ecosystems using yarn, sentry,
▪ automation of data management from end to end and sync between
clusters.
across the cluster.
|languages/scripting                 |platforms & distributions           |
|tool, patnab, openrefine, drake     |                                    |
senior big data engineer          october 2018- present
in aws glue.
• served as resource for newer developers on more-complex topics like
• provisioned, maintained, and terminated emr clusters and kinesis tools
environment: aws:  kinesis,  redshift,  lambda, aws cli.
• built kudu tables with 1010 customer data and tested for speed/fault
• created python functions to parse cef-formatted information security
• optimization of data storage in hive through partitioning and
hoc queries.
the purpose of improving patient care and managing risk.  by pooling and
• import/export data into hdfs and hive using sqoop and kafka.
• wrote complex hive queries, spark sql queries and udfs.
• cassandra data modeling for storing and transformation in apache spark
• analyzed hadoop cluster using big data analytic tools including kafka,
• developed various data connections from data sourced to ssis, and
efficiencies.
• installed and configured pig for etl jobs and made sure we had pig
scripts with a regular expression for data cleaning.
• moving data from oracle to hdfs and vice-versa using sqoop.
• collected and aggregated large amounts of log data using apache flume
• worked with different file formats and compression techniques to
scripting, eclipse, oozie, navigator.
• eliminated the sharing and/or selling of the company’s revenue
generating information/data.
• decreased onboarding time and labor costs.
that needed to be “rolled back”.
• decreased average deployment downtime from three hours to five
management issues, and progress of teams' projects.
annually.
environment.  as the capstone, i walked the participants through how to
contact
optimized hive using partitioning and bucketing, and incremental imports from sqoop to hive.
cassandra, elk, kibana, scala, pyspark, cicd, jenkins
skilled in hadoop stack with yarn using sqoop, hive, hdfs,
created and maintained lambdas used for processing data in real-time.
created hive tables containing datasets to be used by data scientists for ai model training.
ensured data security through the use of kms encryption and the use of passwords in aws secrets manager.
wilmington, de
june 2014 – dec 2015
job management using fair scheduler, and development of job processing scripts using oozie workflow to run multiple spark jobs in sequence for processing data.
integrated kafka with spark streaming for high speed data processing.
loaded data from unix file system to hadoop file system (hdfs) and wrote hive udfs.
rutgers university
lukerichterr@gmail.com
experience in large scale distributed systems with extensive experience as hadoop developer and big data analyst.
hadoop experience hands-on with hdfs, yarn, hive, sqoop, hbase, flume, oozie, zookeeper.
technical skills profile
programming/scripting, etc. █████████
amazon cloud platform (aws) █████████
elasticsearch, elastic mapreduce, elk stack (elasticsearch, logstash, kibana)
big data engineer 	atlanta, ga
worked as a big data developer, where i leveraged various aws services to build and developed complex big data pipelines. these pipelines were used to analyze drink sales in relation to social media ads (particularly twitter and youtube.) several kafka producers were built to ingest ad click data. i leveraged emr clusters to ingest data from kinesis. we analyzed data to find which cities and regions have people that are more likely to buy our drinks. we built heat maps and other dashboard visuals for the end user. the end user then uses the dashboards to target which cities and regions to focus sales on.
moved data to emr cluster where the data is set to go live on the application using kinesis.
applied the latest development approaches including applications in spark using scala. integrated spark code into the sdlc with the ci/cd pipeline using jenkins ci with gitlab.
created kafka cluster which uses a schema to send structured data via micro-batching.
log monitoring and generating visual representations of logs using elk stack. implement ci/cd tools upgrade, backup and restore.
designed and developed etl jobs to extract data from aws s3 and load it in amazon redshift.
experienced collecting real-time log data from different sources like webservers and social media using flume, and storing in hdfs for further analysis.
performed  partitioning and bucketing to optimize hive app.
used hiveql scripts to create and load data into diverse hive tables
used zookeeper and oozie for coordinating the cluster and scheduling workflows.
bachelor of computer engineering
gainesville, florida
|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |jeeeun song`  senior big data engineer   phone: 999-999-9999   email: consultant@gmail.com
jeeeun song   senior big data engineer   phone: 999-999-9999   email: consultant@gmail.com
master’s degree in computer science
aws cloud data services
aws redshift
sqoop
hbase
javascript, mysql, android, r, java, linux, shall, python
cluster management
development of pyspark jobs based on multi-threading proof of concept.
experience in collecting real-time log data from diverse sources like webservers and social media using web crawlers in python, and storing in hdfs for further analysis - ingested through flume.
wrote shell scripts for exporting log files to hadoop cluster through automated processes.
collecting the real-time data from kafka using spark streaming and perform transformations.
knowledgeable of installation and configuration of hive, pig, sqoop, flume and oozie on hadoop clusters.
jupyter notebooks, netbeans, intellij, visual studio
test-driven development (tdd), continuous integration (ci), unit testing, scenario testing
virtualization
vmware, vsphere, virtual machine/ big data vm, virtualbox, oracle big data lite virtual machine v 4.9
data cleansing
project management & methods
implementation of cicd pipelines using gitlab runners.
involved in the creation of vpc and capacity planning.
involved in the design of s3 bucketing strategy to store confidential data.
involved in scheduling oozie workflow engine to run multiple hiveql, sqoop and pig jobs.
kibana setup, dashboarding and visualization configuration.
nrg energy	houston, tx
identified dependencies of different components and documented requirements gathered from stake holders.
architecting and devops for aws & google cloud services including in house data center for middleware system and web services. also, managing security review and web compliance management
transferred data using informatica tool from aws s3.
developed spark code using scala and spark-sql/streaming for faster processing of data.
analyzed the data originating from various xerox devices and stored it in hadoop warehouse.
used pig as etl tool to do transformations, joins and some pre-aggregations before storing the data into hdfs.
hadoop engineer
administration of hadoop cluster(cdm); review of log files of all daemons.
visualization and reporting tools
caterpillar data innovation labs project.  worked on data analytics pipeline to analyze factors affecting product uptime performance.  this project focused on how to use analytics to improve customer experience by creating data analytics solutions capable of predictive analytics of machine failures, factors and improvement of manufacturing and maintenance – determining when a failure of going to occur before it happens and preventing it. experience in transferring streaming data from different data sources into hdfs and hbase using apache flume.
collecting the real-time data from kafka using spark streaming and perform transformations
real-time and near-real-time streaming processing with spark streaming and real-time data indexing.
data analyst and developer
bachelor’s degree in computer systems engineering
linkedin learning
phone: (703) 910-3326  email: dirksorkin0@gmail.com
extending hive core functionality by using custom user defined function's (udf), user defined table-generating functions (udtf) and user defined aggregating functions (udaf) for hive.
hands-on experience processing data using spark streaming api.
have handled over 70 billion messages a day funneled through kafka topics.
spark sql to perform transformations and actions on data residing in hive.
compute engines
apache flink/stratosphere
virtualization, haas environments, lambda
windows  xp, 7, 8, 10, 2016, 2012, 2008 2003, 2000, unix, linux
ingestion
onboarded existing spark features in production so it could be used with quantum spark jobs.
created many leadership boards for upper management to keep track of twenty plus apis and endpoints first with new relic insight. then using grafana using the new relic data source.
used spark to do parsing of the data to get the required fields of data.
used both hive context as well as sql context of spark to do the initial testing of the spark job.
created multi-node hadoop and spark clusters in aws instances to generate terabytes of data and stored it in aws hdfs.
developed a task execution framework on ec2 instances using sqs and dynamodb.
august 2012-
data/system administrator
optimized the data pipeline by using in-memory datastores for faster object dereferencing which lead to a 60% reduction in job duration.
created documentation for training for knowlagent customized salesforce crm application.
implemented design using oracle apps “service for comms” manually as well with pl/sql.
wrote detailed design documents from requirements from customer.
navigator 4 and other methods to identify the root cause of defect.
ran shell scripts in unix environment to manually get customer provisions and billed correctly for
the dsl services ordered.
wrote document on how to recover customer lost ip
• 5 years of experience in it and with database, data storage, and data
• expertise with the tools in hadoop ecosystem including hdfs,  pig, hive,
pig using python.
gzip
cloud, ibm bluemix cloud, cloudfoundry, mapr cloud, elastic cloud, anaconda
big data is being used in the music industry to analyze royalty payouts
implemented data lake concept.
distribution, and data processing
environment: hadoop, hdfs, hive, mapreduce, impala, sqoop, pig, hbase, git,
sept 2015   gulfstream – savannah, ga
lakes and tools.
• worked on poc for iot devices data, with spark.
• worked with the client to reduce churn rate, read and translated the
• performed sentiment analysis using text mining algorithms to find out
extensively used impala to read, write and query the hadoop data in
• created tables, views in teradata, according to the requirements.
• performed bulk data load from multiple data source (oracle 8i, legacy
commands matching the business requirements to teradata rdbms.
• modified bteq scripts to load data from teradata staging area to
• used fastexport utility to extract large volumes of data at high speed
sources and determined the hierarchies in dimensions.
daily data from the source system.
• used informatica debugging techniques to debug the mappings and used
session commands.
experience in data and batch migration to hadoop, and hands on experience in installing, configuring cloudera's and horton distribution.
monitored clusters and grids using ganglia with spark ui for the web interface.
responsible for the cluster performance evaluation, which i wrote and presented
worked with pyspark and pandas libraries in python with sample data using data frames to validate the data before spinning up an emr cluster; checking for _success file and/or number of partitions.
used sphinx to automatically create python documentation.
provided significant input into the modularization of the software and producing test data for the tool.
environment: aws, spark, terraform, docker, databricks, aws lambda, s3
involved in the process of designing cassandra architecture including data modeling.
performance tuning of hive service for better query performance on ad-hoc queries.
performed performance tuning for spark steaming e.g. setting right batch interval time, correct level of parallelism, selection of correct serialization & memory tuning.
implemented data ingestion and cluster handling in real-time processing using kafka.
implemented high availability of name node, resource manager on the hadoop cluster.
environment: hdfs, pig, hive, sqoop, oozie, hbase, zoo keeper, cloudera manager, ambari, oracle, mysql, cassandra, sentry, falcon, spark, yarn
developed various data connections from data sourced to ssis, and tableau server for report and dashboard development.
worked with clients to better understand their reporting and dashboarding needs and present solutions using structured waterfall and agile project methodology approach.
jan 2014	may 2015
integrated kafka with spark streaming for real-time data processing
load and transform large sets of structured, semi-structured and unstructured data.
tuning and operating spark and its related technologies like spark sql and streaming.
used oozie to automate/schedule business workflows which invoke sqoop, and pig jobs as per the requirements.
hadoop data developer
gartner is a leading global research and advisory company.  gartner uses hadoop big data analytics to help business leaders across all major functions and industries make the right decision using objective and accurate insights gathered from real-time data.
worked on installing cluster, commissioning and decommissioning of data node, namenode recovery, capacity planning, and slots configuration.
worked with different file formats and compression techniques to determine standards.
involved in production support, which involved monitoring server and error logs, and foreseeing and preventing potential issues, and escalating issue when necessary.
nov 2007	aug 2010
intellicast – baltimore, md
provided productivity tool development and engineering support to the goes data operations support team (dost), a tiger team tasked with operationalizing the downstream data processing and distribution of geostationary weather products.
feb 2003	april 2005
software and hardware integration for the hunter program. worked with the team of developers across the country to integrate c/c++ code, gots and cots hardware, ethernet, kgs, atm, and hippi interfaces.
lockheed martin – orlando, fl
email:  rahibamin1985@gmail.com
worked with various stakeholders for gathering requirements to create as-is and as-was dashboards.
created dashboards for tns value manager in tableau using various features of tableau like custom-sql, multiple tables, blending, extracts, parameters, filters, calculations, context filters, data source filters, hierarchies, filter actions, maps etc.
involved in performance tuning the data heavy dashboards and reports for optimization using various options like extracts, context filters, writing efficient calculations, data source filters, indexing and partitioning in data source etc.
recommended and used various best practices to improve dashboard performance for tableau server users.
have managed teams ranging from 5 to 20 members with on-site and remote members, across multiple time zones, in a culturally diverse environment.
jupyter notebooks (formerly ipython notebooks), eclipse, intellij, pycharm
scripting:
csv, json, avro, parquet, orc
etl tools:
data viziualization tools:
apache lucene, elasticsearch
created a kafka broker which uses schema to fetch structured data in structured streaming.
used apache spark framework with scala mainly.
wrote hive queries and wrote custom udf’s.
interaction with noc team to work with hadoop to provide large-scale solutions.
extracted metadata from hive tables with hive ql.
installed and configured tableau desktop to connect to the hortonworks hive framework (database) which contains the bandwidth data
made use of python libraries for analytic processing, such as scipy, pandas, and numpy.
deliver effective presentations of findings and recommendations to multiple levels of stakeholders, creating visual displays of quantitative information.
skill with spark framework on both batch and real-time data processing.
experience with using hadoop clusters, hadoop hdfs, hadoop tools and spark, kafka, hive in social data and media analytics using hadoop ecosystem.
hands on experience using cassandra, hive, no-sql databases (like hbase, mongodb), sql databases (like oracle, sql, postgressql, my sql server, as well as data lakes and cloud repositories to pull data for analytics.
spark, python, java, scala, hive, kafka, sql, html, css, javascript, php
alpharetta, ga
used spark-streaming apis to perform necessary transformations and actions on the real-time data using kafka
alcon labs
iam user, group, roles & policy management. aws access key management
setup internal and external load balancer for application and manage cloud.dns setup.
configured zookeeper to coordinate the servers in clusters to maintain the data consistency and to monitor services.
william m. sigler
7 years of experience with the hadoop ecosystem and big data tools and frameworks.
strong hands on experience in hadoop framework and its ecosystem including but not limited to hdfs architecture, hive, pig, sqoop, hbase, mongodb, cassandra, oozie, spark rdds, spark dataframes, spark datasets, spark mllib, etc.
project management methodologies
big data development engineer iii/knowledge consultant
developed and distributed vba macros for business team spreadsheets, allowing for greater automation of data entry tasks, helping reduce manual labor and errors entering the data.
•	monitored production software with logging, visualization, and incident management software such as splunk, kibana, and pagerduty.
•	implemented acceptance test-driven development (atdd) test suites using cucumber for scala.
•	designed process allowing business users to dynamically update card rewards. data consumed by the interchange application, removing the need for developer intervention and reducing costly change orders.
•	provided live demonstrations of software systems to non-technical, executive-level personnel, showing how the systems were meeting business goals and objectives.
•	led team daily standup and planning meetings, utilizing agile processes such as scrum and kanban to direct and maximize engineer productivity.
the project was a recommender system for recommending credit products to prospective members.  i was in charge of upgrading this production system and also worked to optimize maintenance and performed some administrative tasks.  the system environment was a remote private cluster. we directly connected to the mapr filesystem for making changes and doing computations although we did utilize aws for file storage and data warehousing.
introduced system to create and review pull requests in jira for feature enhancements to production software, reducing defects and regressions by more than 300%.
created udfs and stored procedures to add customizable, modular, and reusable functionality to existing hive, pig, and sql programs.  the udfs and stored procedures were written in hive, pig, and sql.
led a group of six business analysts, 4 analysts on-site, 2 remote analysts
utilized subversion and github repositories to prevent data loss and backup critical work artifacts.
wrote user-defined functions (udfs) in order to add custom functionality to the podium data tool.
administered performance testing and recommended hardware upgrades to boost project performance and boost analyst productivity
technologies: hdfs, pig, hive, sqoop, oozie, hbase, zoo keeper, cloudera manager, ambari, oracle, mysql, cassandra, sentry, falcon, spark, yarn
import/export data into hdfs and hive using sqoop and kafka.
developed pipeline jobs to process the data and create necessary files.
documented requirements gathered from stake holders.
worked with several clients with day to day requests and responsibilities.
wrote the shell scripts to monitor the health check of apache tomcat and jbos; daemon services and respond accordingly to any warning or failure conditions.
lead software engineer
amazon mws – carrollton, tx
performed amazon inventory management, operations, and logistics.
email:  williamthorndikeiii@gmail.com
5 years’ experience in hadoop/big data
5 years’ experience i.t. and hadoop/big data
etl, data extraction, transformation and load using hive, pig and hbase.
excellent understanding of hadoop architecture and its components such as hdfs, job tracker, task tracker, name node, and data node.
expertise in python and scala, user-defined functions (udf) for hive and pig using python.
extensive experience with databases such as mysql, oracle 11g.
apache
versioning
test-driven development
jupyter notebooks, pycharm
data management
aws emr, aws redshift, aws s3
aws lambda, aws kinesis, aws elk, aws cloud formation, aws iam
implemented advanced procedures like text analytics and processing using the in-memory computing capabilities like apache spark written in scala.
implemented spark using scala and utilizing data frames and spark sql api for faster processing of data.
used apache spark and scala on large datasets using spark to process real time data.
worked on building input adapters for data dumps from ftp servers using apache spark.
configured aws iam and security group as per requirement and distributed them as groups into various availability zones of the vpc.
designed hbase row key and data modelling to insert to hbase tables using concepts of lookup tables and staging tables.
balancing hadoop cluster using balancer utilities to spread data across the cluster equally.
• experience in migrating the data using sqoop from hdfs to relational
nas, san
sigma
kibana, x-pack, apache solr, apache drill, presto, apache hue, sqoop,
• worked closely with the source system analysts and architects in
versioning of the records and involved in setting up the standards for
using spark sql, python and scala.
• used apache nifi for ingestion of data from the ibm mq's (messages
processing.
understanding the issue.
• involved in unit level and integration level testing and prepared
• developed a procedural guide for implementation and coding to ensure
• implemented data ingestion and cluster handling in real time
• extract real time feed using kafka and spark streaming and convert it
• developed custom aggregate functions using spark sql and performed
node, configuring slots, and on name node high availability, and
capacity planning.
doing it on production cluster.
• used spark sql and data frame api extensively to build spark
assessment of stocks for the financial strategy of this investment
data in hdfs.
such as cosma (csx onboard system management agent), bomr (back office
hbase and use the hawq querying as well.
spark for data aggregation, queries and writing data back into rdbms
through sqoop.
• worked with a team to gather and analyze the client requirements
the data from oracle into hdfs using sqoop
• load the oltp models and perform etl to load dimension data for a star
into hdfs and pre-processing with pig
• created complex schema and tables for analysis using hive
provided & receive feedback on the features, suggest/implement optimal
sql, web services, micro services, ioc.
• involved in developing portlets and deploying in weblogic portal
• monitored the server load average and prepare a status report.
environment: jsp, struts, spring, tomcat, javascript, oracle10g, weblogic,
jquery and css.
bau.
• involved in build process to package & deploy the jars in the
production environment.
jdeveloper, sql developer, maven, xml, css, javascript, json.
antonio saldivar lezama |   big data engineer |  antoniolezama552@gmail.com  |  703-936-4065
7 yrs big data engineer
bachelor’s degree in information technology
itilv3 foundations
• vmware data center virtualization 6.0
able to design and document the technology infrastructure for all pre-production environment and partner with technology operations on the design of production implementations.
implementation and configuration of administration using
i was involved in multiple projects, such as spark batch processing, and design and development of multiple ci/cd pipelines and work with apis.
refactored microservices made by lambdas, kinesis, cloudformation stack.
lead jira epics and created stories to distribute with the team, keeping track and delivering on time.
automated monitoring, working with splunk dashboards and alerts.
environment
responsible for understanding of business rules, business logic, and use cases to be implemented.
store and query data from hbase using apache phoenix.
xml schemas and pojos to map the input request and send the response.
stored encrypted data to the data base and certificates to connect via https and accomplish iso 8583.
drools to create the template, drt files and start the kiesession to match all incoming transactions to the engine as a first step.
hortonworks ambari
apache flink
apache kafka
hadoop architect & big data engineer
used cassandra and mongodb to work on json files.
involved in migrating jobs to spark, using spark sql and dataframes api to load structured data into spark clusters
spark data frames
architected the big data architecture to create the foundation of this enterprise analytics initiative in a hadoop-based data lake using cloudera platform.
developed scala scripts and udfs using dataframes and rdd in spark for data aggregation, queries and writing data back into oltp system through sqoop.
pig
october 2011 – may 2013
installed and configured various components of the hadoop ecosystem.
developed custom ftp adaptors to pull the clickstream data from ftp servers to hdfs directly using hdfs file system api.
mapreduce
september 2007 – september 2011
knowledge on no-sql databases like cassandra and hbase.
formation, aws iam and security group in public and private subnets in vpc.
cloudera hadoop( cdh)
panasonic avionics corporation - lake forest, ca
 implemented jenkins server for cicd integrated with git version control.
 installed, configured, and tested an aws lambda function workflow in python
 utilized spark data frame and data set from spark sql api extensively for data processing.
saic - huntsville,  al
responsible for designing logical and physical data modelling for various data sources on confidential amazon redshift.
managing and scheduling batch jobs on a hadoop cluster using oozie.
performed cluster maintenance and upgrades to ensure stable performance.
hands-on experience architecting and implementing hadoop clusters on amazon (aws), using emr, s2, s3, redshift, cassandra, anangodb, cosmosdb, simpledb, amazonrds, dynamodb, postgresql., sql, ms sql.
research and present potential solutions for current aws platform in relation to data integration and visualization and reporting.
expertise is dealing with multi-petabytes of data from mobile ads, social media, iot in various formats, structure, unstructures and semi-structured.
hadoop file system
cloudera (cdh), hortonworks (hdp)
apache misc
apache ant, flume, hcatalog, maven, oozie, tez
cloud watch monitoris3 & glacier storage management, access control and policing
january l 2015 – december 2015
integrated zeppelin daemon with spark master node.
august  2012 – august 2013
maintained and ran mapreduce jobs on yarn clusters to produce daily and monthly reports per requirements.
oracle certified associatenatallia laurova
phone: (999) 999-9999   email: consultant@gmail.com
amazon aws - ec2, sqs, s3, kinesis, azure, google cloud, horton labs, rackspace, cloudera hadoop, cloudera impala, hortonworks hadoop, mapr, spark, spark streaming, hive, kafka, nifi
kroger, cincinnati, oh 	march 2017-present
worked with apache spark which provides fast and general engine for large data processing integrated with functional programming language scala.
designed and developed spark code using scala, pyspark & spark sql for high-speed data processing to meet critical business requirement
created snapshots on aws s3 buckets with python scripts.
created basic infrastructure of the pipeline using aws cloud formation.
developed spark streaming jobs in scala to consume data from kafka topics, made transformations on data and inserted to hbase.
importing and exporting data using sqoop from database to hdfs and data lake.
worked on nosql databases including hbase and mongodb. configured mysql database to store hive metadata.
belarus state university, school of computer science and radio electronics (bsuir) - minsk, belarus
email:  consultant@gmail.com
apache cassandra, apache hbase, mapr-db, mongodb, oracle, sql server, db2, sybase, rdbms, mapreduce, hdfs, parquet, avro, json, snappy, gzip, das, nas, san
senior hadoop big data engineer
this type of tracking allows companies to spend money more effectively; they can target consumers of a certain age and income, who are seeking a specific type of product. they don’t try to engage every single person, or those who aren’t interested in buying.
created variation of the lambda architecture consisting of near real-time using spark sql; spark cluster 1.4 consisting of 25 nodes running with 200gb ram/24 tb
benchmarking hadoop and spark cluster on a terasort application in aws.
captured and transformed real-time data from amazon aurora into a suitable format for scalable analytics.
hadoop big data developer
vpc setup for multiple project
howard community college: music composition & technology a.a.'s
carlos emilio sturdivant
created hive managed and unmanaged tables with partition and bucket in hive and loaded data into hive.
aws rds, aws emr, aws redshift, aws s3, aws lambda, aws kinesis, aws elk, aws cloud, aws iam formation
springfield, ma
installed elasticsearch, logstash, kibana, kafka, and zookeeper.
developed etl pipelines to process log data from internal systems using kafka.
implemented data ingestion and cluster handling for real time processing using kafka.
developed spark applications using spark core, spark sql and spark streaming api
jan 2017 – march 2018
managed policies for s3 buckets and glacier for storage and backup on aws.
created aws cloud formation templates for data pipelines in aws.
installed, configured, and tested an aws lambda function workflow in python.
used cloudera manager for maintaining a healthy cluster.
“infosys was my introduction to hadoop, kafka, spark, and other technologies.  the processes and knowledge accumulated was key to my development in big data.
responsible for maintenance and performance of clusters.
sage software
magna cum laude
page 7phone    (415) 423-0612
phone    (415) 423-0612
5 years big data engineering
full-stack software engineer experienced in hadoop and other big data platforms.
experience in designing and handling of various data ingestion patterns (batch and near real time) using sqoop, distcp, apache storm, flume and apache kafka.
experience in designing and handling of various data transformation/filtration patterns using pig, hive, and python.
skills summary
apache ant, apache cassandra, apache flume, apache hadoop, apache hadoop yarn, apache hbase, apache hcatalog, apache hive, apache kafka, apache maven, apache oozie, apache pig, apache spark, spark streaming, spark mllib, graphx, scipy, pandas, rdds, dataframes, datasets, mesos, apache tez, apache zookeeper, cloudera impala, hdfs, hortonworks, apache airflow and camel, apache lucene, elasticsearch, elastic cloud, kibana, x-pack, apache solr, apache drill, presto, apache hue, sqoop, kibana, tableau, aws, cloud foundry, aws, azure, anaconda cloud, elasticsearch, solr, lucene, databricks
designed and created hive external tables using shared meta-store instead of derby with partitioning, dynamic partitioning and buckets.
tested on mongodb nosql data modeling, tuning, disaster recovery and backup.
used oozie workflow to co-ordinate pig and hive scripts.
teamed up with architects to design spark model for the existing model and migrated models to spark models using scala.
installed and configured hadoop hdfs, developed multiple jobs in java for data cleaning and preprocessing.
migration of etl processes from oracle to hive to test the easy data manipulation.
worked with hiveql on big data of logs to perform a trend analysis of user behavior onvarious online	modules.
involved in creating hive tables, pig tables, and loading data and writing hive queries and pig scripts.
fluent in architecture and engineering of the hadoop ecosystem.
creates and maintains environment configuration documentation for all pre-production environments
hands-on experience with aws, emr and s3.
big data
apache spark, spark streaming, spark mllib, graphx, apache hive, hive ql
hdfs, snappy, gzip, das, nas, san
sql injection, data ftk imager, wan/lan, tcp/ip, routing, vmware, virtualbox, osi model
management of spark-submit jobs to all environments
implemented hortonworks medium and low recommendations on all environment
use remedy for ticketing system
implemented amazon web services (aws) sap hana environment to achieve the speed, performance, and agility it required without making a significant investment in physical hardware.
amazon simple storage service (amazon s3) was used for data backups, including hana, and amazon elastic block store (amazon ebs) provisioned iops (p-iops) volumes for storage.
san francisco, ca	june 2015 – january 2017
ingested data to hdfs from ibm as400, mssql server, teradata, oracle, db2, unidata databases using sqoop
designed managed/external hive tables as per the requirements and stored them in orc format for efficiency
configured capacity scheduler to create multiple queues and assigned the users/groups based on the resource requirements
worked on spark sql to check the pirated data.
created executive dashboards for etl process and reporting
created data loss prevention plan using amazon s3 storage for backups with amazon glacier for archival.
designed and developed parallel jobs by using different types of stages such as transformers, aggregator, merge, join, lookup, sort, remove duplicate, funnel, filter, pivot, shared container for developing jobs.
used zookeeper for various types of centralized configurations, git for version control, and maven as a build tool for deploying the code.
mcgraw-hill education // boston, ma
cargill // minnetonka, mn
flint, michigan
nigeria
senior big data engineer	february 2019 – present
open learning data migration – all mcgraw-hill data was stored in old legacy systems and that data needed to be consolidated and migrated into the new open learning ecosystem. this involved building streaming pipelines to stream this data into datamarts.
migrated data stored in old legacy systems that used batch etl jobs to load data to streaming pipelines that stream real-time events and store them in datamart.
used databricks, jenkins, circleci, intellij idea, third-party tools in software development.
tasks, sprints, stories and backlog management tracked using jira agile development software.
used spark-sql to load json data and created schema rdd and loaded it into hive tables and handled structured data using sparksql.
developed mapreduce program to extract and transform the data sets and resultant dataset were loaded to hbase and vice-versa using kafka 2.0.x.
develop mapreduce jobs in java for log analysis analytics and data cleaning.
perform big data processing using hadoop, mapreduce, sqoop, oozie and impala.
conduct in-depth research on hive to analyze partitioned and bucketed data.
loaded unstructured data (log files, xml data) into hdfs using flume.
komodo health, san francisco, ca
building large-scale and complex data processing pipelines.
solid experience with apache spark data structures, critical features and performance tuning.
amazon aws services (ec2, s3, rds etc)
worked on analyzing hadoop cluster and different big data analytic tools including mapreduce, hive, hdfs, spark, kafka and apache nifi.
hadoop system administrator	november 2013 – february 2015
helped the team to increase cluster size from 16 nodes to 52 nodes. the configuration for additional data nodes managed by using puppet.
enabled kerberos for hadoop cluster authentication and integrate with active directory for managing users and application groups. used ganglia and nagios for monitoring the cluster around the clock.
worked with the linux administration team to prepare and configure the systems to support hadoop deployment
christopher tran
(402) 218-2731
associate of applied science in chemistry and bio-chemistry
associate of applied science in mathematics
comptia a+ 220-801, 220-802, network+ n10-006, security+ syo-401
able to optimize storage of various platforms (i.e.., data warehouse, rdbms, nosql) through hdfs data lake environments.
distributions:  cloudera hadoop, hortonworks hadoop
june 2017 – present~
built out hadoop data lake using hadoop file system (hdfs), with the use of hive and spark, and managing hadoop log files.
tested reports and uploaded tableau dashboards to server and provided production support for tableau users.
created dashboards using calculations, parameters in kibana.
accessed hadoop file system (hdfs) using spark and managed data in hadoop data lakes from different sources accessed for data processing using spark.
worked on stored procedures, triggers and on substantial number of business analytical functions.
worked with inbuilt transformations in etl/ssis like persistent lookups to enhance performance for lookups and more utilization of i/o resources.
created logging for etl load at package level and task level to log number of records processed by each package and each task in a package using ssis.
used zookeeper and oozie for coordinating the cluster and scheduling workflows.210-960-3326
big data engineer skilled in hortonworks hadoop and cloudera hadoop distributions, hadoop distributed file system (hdfs).  background in management information systems provides unique adaptation to understanding business processes and business intelligence, needs and efficiencies.
hiveql, mapreduce, xml, ftp,
cloudera, hortonworks, mapr, emr
consulted in the areas of data and analytics, specifically using hadoop, spark, hive and related tools.
hands-on major components in hadoop echo systems like spark, hdfs, hive, hbase, zookeeper, sqoop, oozie, flume, kafka.
ensured best practices for data integration and automation: quality control checks, reconciliation, error handling, checkpoint/restart design, data profiling, etc.
integrated multiple technologies and datasets to solve the business problem.
implemented hive scripts and ran hive queries on top of hdfs.
analyzed large amounts of data sets to determine optimal way to aggregate and report on it.
provide support data analysts in running hive queries.
responsible for cluster maintenance, adding and removing cluster nodes, cluster monitoring and troubleshooting, manage and review data backups, manage and review hadoop log files.
fortunately, involved in the creation of new clusters from build.
configured github plugin to offer integration between github & jenkins.
involved in converting hive/sql queries into spark transformations using spark rdd and data frame.
used spark sql to perform data processing on data residing in hive.
involved in processes using spark streaming to receive real time data using kafka/ on prem and aws cloud.
used spark structured streaming for high performant, scalable, fault-tolerant data processing of real-time data streams by extending core spark api on prem and aws.
june 2019 – present
process data from s3 to redshift: load eurosport data provided by apple and android to redshift.
performed spark-submit along with flag configurations to maximize parallelism
involved in processes using spark streaming to receive real time data using kafka.
large-scale hadoop deployments (40+ nodes; 3+ clusters) on aws.
hadoop administrator/engineer
cerasis
hadoop big data engineer, creating custom data pipelines and systems using open source tools and technologies on premises and in the cloud.  skilled with hadoop ecosystem such as kafka, spark,storm, hive,  mesos, yarn, flume, ambari, tez, and more.
work with teams to identify key initiatives to apply sophisticated analytical decisions.
experience in designing and implementing of secure hadoop cluster using kerberos.
effective communicator of technical information to all levels of the organization.
apache cassandra, apache flume, apache hadoop, apache hadoop yarn, apache hbase, apache hcatalog, apache hive, apache kafka, apache oozie, apache spark, spark streaming, scipy, pandas, rdds, dataframes, datasets, mesos, apache tez, apache zookeeper, cloudera impala, hdfs, hortonworks, mapr, mapreduce, elasticsearch, elastic cloud, kibana, apache hue, sqoop, tableau, aws, git hub
etl processes, real-time processing, batch processing, streaming processes
cloud services
microsoft powerbi
tableau
qlikview
navy federal credit union |  vienna, va	may 2019 - present
presented and defined testing process for business and tech teams.
identify and organize source and transactional data for delivering solutions using hadoop data lakes and hadoop clusters in hadoop ecosystem, and spark and hive.
involved in full life cycle of the project from design, analysis, feature extraction, training, model optimization, development, implementation, testing, visualization.
intel corporation | santa clara, ca	may 2014– june 2015
proposed an automated system using shell script to sqoop the job.
mainly worked on hive queries to categorize data of different claims.
performed aggregations and analyses on large sets of log data.
character
flexible and adaptable
experience in writing sql queries, stored procedures, triggers, cursors and packages.
skilled in architecture of big data systems, etl pipelines, and analytics systems for diverse end users
management experience as team lead, project planning, facilitation and coordination using both waterfall and agile processes  such as agile scrum.  experience managing costs, materials, labor, time, hiring and managing team memberz.  managing budgets over $32m.
used amazon web services (aws) like amazon s3 and amazon ec2.
january 2015 – march 2016
poc involved in loading data from linux file system to aws s3 and hdfs.
experience in monitoring tools like nagios and amazon cloudwatch to monitor major metrics like network packets, cpu utilization, load balancer latency etc.
integrated jira with ci/cd pipeline as defect tracking system and configured workflows to automate deployment and issue tracking.
may 2013 – dec 2014
analyzed the business requirements and involved in writing test plans and test cases.
involved in creating hive tables, loading with data and writing hive queries which will invoke and run spark jobs in the backend.
brite systems, inc.
jan 2010 – jan 2011
the university of yaoundé i, yaoundé, cameroon
india institute of science
used spark to fine-tune query responsiveness for better user experience.
cassandra, datastax, hbase, phoenix, redshift, dynamodb, mongodb, ms access, sql, mysql, oracle, pl/sql, postgres sql, rdbms
responsible for managing data coming from different sources.
developed workflow using oozie for running mapreduce jobs and hive queries.
wrote multiple mapreduce programs in java for data extraction, transformation, and aggregation from multiple file formats.
assisted in exporting analyzed data to relational databases using sqoop.
involved in developing message driven and session beans for claimant information integration with mq based jms queues.
hive queries and pig scripts to make udfs.
6+   years big data
email: humberto.avelar12@gmail.com  /
json, avro, parquet, orc, xml, hdfs
big data engineer/big data developer  	09.2018 – present
worked in a private cloud environment on amazon aws.
synchronized and ensured availability of data sources in aws east and west regions.
used github for control version and jira for issues and project tracking.
custom kafka broker design to reduce message retention from default 7-day retention to 30 minute retention - architected a light weight kafka broker
hands on experience on fetching the live stream data from db2 to hbase table using spark streaming and apache kafka.
developed internal and external tables, and used hive ddls to create, alter and drop tables.
coca cola – femsa-  mexico
database design and development
data analysis in water pipes
5 years
responsible for moving and transforming big data for insightful information.
responsible for building quality for big data transfer pipelines for data transformation using flume, spark, spark streaming, and hadoop.
2002 - 2004
optimizing the hive queries using partitioning and bucketing techniques.
creating hive tables, loading with data and writing hive queries to process the data.
hadoop big data & spark
built kafka clusters, spark clusters, elk cluster and their integration with hadoop clusters.
extract real time feed using kafka and spark streaming and process data in the form of data frames to do transformations and aggregations.
scala c, c++, c#.net, ado.net, python, r, javascript, visual basic, posh script, javascript, angular.js, react.js, bootstrap, entity framework, unix
database languages
web forms, mvvm, mvc 5.0/4.0.
search & query
virtual computer
mongodb,arangodb
cloud platforms & services
data skills
tune and optimize,
imported data from disparate sources into spark rdd for processing
long island, ny
technologies: spark, hadoop, google cloud, data proc, cloud dataflow, bigquery,, data frame, spark streaming, kafka, pig, hive, spark sql, sql api
dissertation in sspl (solid state physics laboratory), drdo, in a project entitled “gallium nitride monolithic microwave integrated circuit based amplifier designing”.
(gpa: 4.0/4.0)
professional training
well-rounded big data engineer and developer with hands-on experience in all phases of big data environments such as design, implementation, development and customization and performance tuning, data cleaning and database.
applies extending hive and pig core functionality by using custom user defined function's (udf), user defined table-generating functions (udtf) and user defined aggregating functions (udaf) for hive and pig.
vmware, vsphere, virtual machine
hadoop, cloudera, hortonworks
streaming data
chevron, san ramon, ca.                                                                 august 2017-present
hands-on with spark core, spark sql and data frames/data sets/rdd api.
implemented ci/cd tools upgrade, backup and restore
implementation of several applications, highly distributive, scalable and large in nature using cloudera hadoop.
worked on hortonworks hadoop distributions
email: goytomb82@gmail.com
data architecture, storage, etl, bi analysis
project description
extensively worked on datastage sever and parallel job controls and sequencers. designed and developed parallel jobs by using different types of stages such as transformers, aggregator, merge, join, lookup, sort, remove duplicate, funnel, filter, pivot, shared container for developing jobs.
belay goytom   |   phone  999-999-9999   |  email@gmail.comaldo angulo  703-936-4037   |   aldo.anguloaa@gmail.com
present 	capital one financial services – mclean, va
files processing (scala program in an emr cluster)
created drools files with business rules to approve credit lines
modified schema classes to match format of a json object provided by an api
blueprinted an infrastructure built in terraform scripts, to maintain independence from aws as cloud service provider.
environment: hadoop, hdfs, hive, spark, yarn, mapreduce, kafka, pig, mongodb, sqoop, storm, cloudera, impala
may 2015	gulfstream – savannah, ga
environment: java, jdbc, jndi, struts, maven, subversion, junit, sql language, spring, hibernate, junit, oracle, xml, putty and eclipse.
optimized the performance of the database (db2) of credit and collection of elektra stores by tuning
recoveries of programs and objects in the database. i automated the collection of statistics
and elastic. expert in etl and data pipeline methods and tools and
able to design elegant solutions through the use of problem
statements.
node, and mapreduce concepts and experience in working with mapreduce
programs using apache hadoop for working with big data to analyze
large datasets efficiently.
• experience in developing rest api's for use in single page or native
applications and
(spring tool), android studio, etc with deployment on server/cloud
• strong analytical skills with ability to quickly understand client’s
in predictive analytics.
queries, which will invoke and run mapreduce jobs in the backend.
• consumed data from kafka queue using storm.
• set up apache storm on aws for etl pipeline with pig latin, with hbase
• implemented yarn resource pools to share cluster resources for yarn
and analytics applications for clients.  work involved considerable
• developing a financial model engine for the sales department on big
• prepared technical documentation of the poc with all the details of
• design and develop hadoop mapreduce programs and algorithms for
analysis of cloud-scale classified data stored in cassandra
and the eco system components like sqoop, pig, hive, hbase and oozie
into hdfs
various perspectives to detect aberrations in data, provide output to
centers for customer experience optimization
• used apache spark to execute scala source code for json data
• worked on analyzing hadoop cluster using different big data analytic
• configured spark streaming to receive real-time data from the kafka
• orchestrated hundreds of apache sqoop scripts, pig scripts, and hive
of sprint and release progress.
monitoring.
productivity.
• involved in providing inputs for estimate preparation for the new
proposal.
environment: linux, hadoop, pig, sqoop, hive, hbase, sqoop, mongodb,
throughout software development life cycle (sdlc).
automated inputs and redesigning forms for easier access.
(281) 853-8897   franciscogranados06@gmail.com
experience with custom development of reporting dashboards (tableau, microsoft power bi, roambi mobile, proclarity)
worked closely with project management team to analyze and consolidate request requirements to improve decision planning processes and meet project deadlines.
sql server analysis services (ssas) 2005/2008r2/2012, sql server integration services (ssis) 2005/2008r2/2012, sql server reporting services (ssrs) 2005/2008r2/2012, proclarity, performancepoint server, performancepoint services, roambi mobile, powerpivot, powerview, reportportal, oracle business intelligence enterprise edition (obiee), salesforce, artus, qlikview, cognos, tableau, panorama, sap sybaseiq, sharepoint 2007/2010.
microsoft azure
to improve the reporting capacity and the use of power bi as a reporting tool, i participated in the design and implementation of a data warehouse solution hosted on sql azure.
participated as a single resource in the bi area in charge to attend the entire organization, sales, finance, operations, hr.
implemented the data warehouse data retention procedures
extracted data from several external sources (ups, fedex, cisco) to create ad hoc reporting solutions
responsible for creating and changing the visualizations in power bi reports and dashboards on client requests.
reduce processing time from 8 to 3 hours, finishing 2 hours before sla.
create ssis packages for datamart implementing partitioning strategy in sql tables for data retention.
create tableau dashboards for self-service bi across supply chain and procurement areas and replacing proclarity as the analytical tool
mentored my team on best practices and analysis of olap cubes and dimensions.
design, development, implementation, and maintenance of repots and dashboards in tableau.
monitored performance and optimized sql queries for maximum efficiency.
experience in creating reports using ssrs report builder 3.0
created data driven subscriptions for different users using report manager.
create and maintain ssas tabular models
hands on creating end to end etl strategies that will include data profiling as well
designed, developed, implemented, and maintained the ssrs reports suite of the school accounting model for desktop and mobile users.
apache cassandra, aws redshift, amazonrds, apache hbase, sql, nosql, elasticsearch
prepared spark builds from mapreduce source code for better performance.
exploring with spark improving performance and optimization of the existing algorithms in hadoop mapreduce using spark context, spark-sql, data frames, pair rdd's and spark yarn.
infrastructure design for the elk clusters.
used ansible python script to generate inventory and push the deployment to aws instances.
implemented usage of amazon emr for processing big data across hadoop cluster of virtual servers on amazon elastic compute cloud (ec2) and amazon simple storage service (s3)aws redshift
aws iam was used for creating new users and groups.
handled the data exchange between hdfs and different web applications and databases using flume and sqoop.
installed and configured tableau desktop on one of the three nodes to connect to the hortonworks hive framework (database) through the hortonworks odbc connector for further analytics of the cluster.
assist in install and configuration of hive, pig, sqoop, flume, oozie and hbase on the hadoop cluster with latest patches.
implemented user access with kerberos and cluster security using ranger.
master’s degree in electronic commerce, dalhousie university, halifax, ns, canada
and the ability to combine logic with creativity to derive innovative
solutions that match the customer’s unique needs precisely.  i bring to any
• able to gather, and aggregate various sources, integrating into hdfs.
• realtime log data collection from multiple sources including social media
(facebook, twitter, google, linkedin), webserver logs, and databased
|agile, kanban, scrum, devops,       |hdfs, avro, parquet, snappy, gzip,  |
|sql, nosql, apache cassandra,       |apache ant, apache cassandra, apache|
|suites                              |apache spark, spark streaming, spark|
|amazon aws, microsoft azure,        |tez, apache zookeeper, airflow and  |
specific to current financial objectives in marketing and risk management.
of key stakeholders and engage them to advance the interest of the company.
hadoop data engineer   may 2010 - may 2012
serialization & memory tuning.
used spark sql and dataframes api to load structured and semi structured
performance tuning of hive service for better query performance on ad-hoc
performed both major and minor upgrades to the existing cloudera hadoop
designed cassandra architecture.
fashion.
policy.
needs and present solutions using structured waterfall and agile project
implemented spark using scala and spark sql for faster analyzing and
involved in converting hive/sql queries into spark transformations using
it available for visualization and report generation by the bi team.
connected various data centers and transferred data between them using
imported data using sqoop to load data from mysql and oracle to hdfs on
staging data in hdfs for further analysis.
flume to stream the log data from servers.
hive.
implemented mail alert mechanism for alerting the users when their
selection criteria are met.
designed and developed front end using html, css, ajax, xml, javascript.
from multiple tables on mysql database.
phone: (413) 306-3980  |  e-mail: carlos.sturd@gmail.com
in economics from virginia state university, and an associate degree in
hadoop ecosystem and spark structures along with hive, to collect and
process data from various sources for analysis for use in continuous
apache hadoop core, apache ant, apache flume, apache yarn, apache hcatalog,
eq health solutions     baton rouge, la
• spark steaming for performance tuning, e.g. setting right batch
hadoop data architect/engineer    may 2015 - may 2016
data processing and analytics.
sqoop and kafka. created kafka topics and distributed to different
needs and present solutions using structured waterfall and agile
project methodology approach.
using sqoop and kafka.
• wrote scripts in pig and hive to move the data files to/from hadoop
humana      louisville, ky
• used shell scripts to dump the data from mysql to hadoop distributed
• involved load and transfer of large sets of structured, semi
scripts with regular expression for data cleaning.
• assisted in designing, building, and maintaining database to analyze
requirements and data profiling analysis.
transforming into user-facing metrics.
emr, ec2, s3, ebs and iam entities, roles, and users.
• administration of hadoop cluster(cdm); review of log files of all
from various sources) using apache flume and kafka.
• experience working in hadoop-as-a-service (haas) environments with
processing and real-time data processing.
5 years have been in hadoop hdfs and hadoop ecosystem and tools.
← design and build big data architecture for unique projects, ensuring
development and delivery of the highest quality, on-time and on
performant metadata-driven data pipeline with kafka and hive/presto on
← provides clear and effective testing procedures and systems to
ensure an efficient and effective process.
← experience deploying large multiple nodes of a hadoop and spark
apache kafkaapache camel, apache airflowanalysis
pantaho
compression
conversion
jan 2018 hadoop big data developer
this big data hadoop-based project for apple focused on customer development centered around apple proprietary applications, connectors and apis.  worked on an environment based in a proprietary apple connect system which authenticates users. the apple connect system is similar to ldap; a token-based, three-way handshake.  wrote script to upload to zeppelin and azkaban.  in this project i lent support by creating and scheduling azkaban workflows using zeppelin notebook.
used a proprietary apple code repository and version control.
predictix work involved consulting for a variety of clients in various industries including healthcare and marketing to help them improve roi on their big data analytics platform investments.  i focused on optimization and performance and streamlining processes.
performed sqooping for various file transfers through the cassandra tables for processing of data to several nosql dbs.
performance tuning for kafka cluster (failure and success metrics).
migrated programs into apache spark rdd operations.
designed generic extensive framework using python libraries load data from multiple sources into data lake
environment: hdfs, pig, hive, sqoop, oozie, hbase, zookeeper, cloudera manager, ambari, oracle, mysql, cassandra, sentry, falcon, spark, yarn
may 2015 hadoop data engineer
developed pig scripts to transform the data into structured format; automated through oozie coordinators.
worked on pulling the data from relational databases, hive into the hadoop cluster using the sqoop import for visualization and analysis.
developed shell scripting and python programs to automate the data flow of daily tasks.
worked on comparing the databases using comparison and publish and then updating the data into the tfs server using ssdt
participated in system testing, integration testing, performance and product testing, defect management, database standardization and tuning
210-807-8313anthony pina
multi clustered environment and setting up cloudera hadoop echo system.
hands-on experience working on nosql
hive, pig, c#, .net vb.net, asp.net, ado.net, web api, html5/css3, javascript, jquery, php, bootstrap, rest, json, soap, xml, ajax
workflows, event receivers, web parts, site definitions, site templates, timer jobs, sharepoint hosted apps, provider hosted apps, search, business connectivity services (bcs), user profiles, master pages, page layouts, managed metadata, sharepoint designer, infopath, nintex, sharegate, oauth, templates, taxonomy, sharegate, metalogix, nintex, forms, infopath, sharepoint designer, visual studio, ms office, sharepoint search, sharepoint user profiles.
development of unit tests for hive scripts using hiverunner.
use of gitlab/git bash as version control and code repository.
knowledge transfer to tech. support team.
apple
hadoop lead tech:  big data / spark developer, jan 2018- march 2018
unit testing and integration testing was performed using scalatest.
hadoop big data engineer & architect, april 2017 – jan 2018
i worked with i3 solutions to construct custom data pipelines and manage the etl and transformation process, data lakes, etc. for client projects.  i made sure to gather requirements and clearly understand the needs so that each use case was clearly addressed. for these clients, i constructed platforms using cloud solutions, analytics platforms, hadoop hdfs, database, and microsoft sharepoint.
working extensively on hive, sqoop, pig, and python.
develop hiveql scripts to perform the incremental loads.
involved in data migration from one cluster to another cluster.
involved in resolving performance issues in pig and hive.
worked with hortonworks distribution
optimized amazon redshift clusters, apache hadoop clusters, data distribution, and data processing
used rest api to access hbase data to perform analytics.
perform analytics on time series data exists in cassandra using cassandra api.
the log data.
developing custom pig loaders.
worked on oozie workflow engine for job scheduling.
sempra energy
using sqoop to extract the data back to the relational database for business reporting.
environment: hadoop, hdfs, hive, pig, sqoop, hbase, oozie, my sql, svn, putty, zookeeper, unix, shell scripting, hiveql, nosql database (hbase), rdbms, eclipse, oracle 11g.
be a part of design reviews & daily project scrums.
mexico
administration and updating of the active directory. responsible for sharepoint profile synchronization with active directory using the import feature from central administration.
creation of site pages, lists, and documents for internal users and administration of access permissions to such users.
creating the system requirement documents, to detail out the functional and changes expected from various technical teams.
coordinating work with project managers, technical leads and offshore resources on requirement breakdown and problem-solving.
created mvc applications for providing updates
certified hadoop engineer
expertise in development of multi-tiered web-based enterprise applications using j2ee technologies like servlets, jsp, jdbc, java beans, spring framework, mvc and hibernate.  having experience of applications development on tools eclipse, sts (spring tool), android studio, etc with deployment on server/cloud like ibm's bluemix using inbuilt services.
detailed knowledge and experience of design, development and testing software solutions using java and j2ee technologies with developing and maintaining the web applications using the web server tomcat
experience with front-end technologies like html5, css3, javascript and jquery for ui to get a complete end to end system
strong analytical skills with ability to quickly understand client’s business needs.
sql, nosql, apache cassandra, mongodb, hbase, rdbms, hive
data engineer		june 2017 - present
evanflint09@gmail.com
big data engineer & enterprise architect
5 years of big data experience in different industries and environments.
devops
workflows
created an architecture using aws lambda in conjunction was an adobe-designed task scheduler similar to airflow.
achieved the goal of process the entire dataset files (1.34 gb) in under 5 minutes.
mar 2018 – june2019
pfizer
• 5 years of experience with the hadoop ecosystem and big data tools and
• familiarity with the entire hadoop ecosystem including, sql, scala, pig,
• developed scripts and automated data management from end to end and sync
spark mllib, etc.
• extensively used apache flume to collect logs and error messages across
in the competitive market.  the consulting team built a system to analyze
•     worked on importing and exporting data using sqoop between hdfs to
•     implemented data ingestion and cluster handling in real time
mongodb to build a continuous etl pipeline. this is used for real time
•     worked on disaster management with hadoop cluster.
•     involved in the process of designing cassandra architecture including
•     integrated hadoop with active directory and enabled kerberos for
merchandising, as well as sales and marketing.  systems were able to track
customer activity in real time to provide advanced analytics on data
files to/from hdfs.
•     apache kafka to transform live streaming with the batch processing to
environment:  hadoop, spark, hdf, oozie, sqoop, mongodb, hive, pig, storm,
across its acreage, thus enabling pioneer to economically optimize the
•  responsible for building scalable distributed data solutions using
tools including apache sqoop, flume, among others.
• creating hive external tables to store the pig script output
involved in system administration of linux based systems..
• provide technical support via telephone/email to over 3,000 users.
redhat linux.
rhel, hp-ux, aix operating systems
aug 2010    j.i.s.d. candlewood elementary – san antonio, tx
end to end support of all internal online web applications and servers.
big data cloud services:  aws, azure, adobe, elastic cloud, anaconda cloud
hadoop big data ecosystem:  apache sqoop, spark, park streaming, storm, kafka, presto, zookeeper, hdfs, tez, hive, hcatalog, maven, pig, oozie, spark, impala, ant, ivy, pandas, pyspark, camel, airflow, jelly, solr, lucene
collaborated with the infrastructure, network, database, application, and bi teams to ensure data quality and availability.
hadoop big data consulting engineer
worked on the core and spark sql modules of spark extensively.
apache hadoop, edw, sql server 2005, toad, rapid sql, oracle 10g (rac), hdfs, vmware, hive, pig, hive, hbase, sqoop, flume, linux, unix, db2.
trigent – southborough, ma
created data quality etl packages to correct and cleanse the taxonomy data and enhance the quality of consolidated data. the consolidated taxonomy data then were segmented using hadoop and cloudera.
11/2010 – 10/2011
created project design document with microsoft sharepoint project templates.
developed and created the gui using c#.net and asp.net.
worked with xml documents and used xmldocument, xpathiterator, and xpath expressions.
hadoop, cloudera, hortonworks, aws
hive's analytical functions, extending hive core functionality by writing custom sql queries.
senior big data engineer april 2017 – present
only have 3 companies!!!
tysons corner, va
associates of applied science with honors
• primary technical skills in hdfs, yarn, pig, hive, sqoop, hbase,
• hands on experience in working with ecosystems like hive, pig, sqoop,
and pig core functionality with udfs.
• hadoop installation; configuration of nodes/clusters, administration,
• good knowledge in pl/sql, hands-on experience in writing medium level
|apache drill, apache kafka, apache  |rdds, dataframes, datasets,         |
|apache hue, apache sqoop, apache    |warehouse, data analysis            |
|apache hcatalog, apache ant, apache |hadoop, hdfs, hadoop yarn,          |
|zookeeper                           |mllib, graphx                       |
|kibana, tableau                     |apache hive, hive ql                |
|apis                                |elasticsearch, solr, lucene,        |
• integrating on-premises cluster to better work with transient, cloud-
• loaded data from diff servers to aws s3 bucket and setting appropriate
aug 2011    hadoop data engineer
feb 2013    usaa – san antonio, tx
and how to create processes tailored to their concerns and specific use
and analytics using inbuilt libraries.
oct 2009    compqsoft – kansas city, mo
• managed, monitored and troubleshot all mceits network and application
• create sops for my shift to follow for completely nightly checks and
enterprise migration, consolidation, rdt&e).
systems are transitioned to navy enterprise enclaves.
• recommended a technical solution for enterprise review to
as they relate to research and assessment of emerging technologies,
• worked with xacta to maintain ia packages.
jdbc, jndi, struts, maven, subversion, junit, sql language, spring,
hibernate, junit, oracle, xml, putty, and eclipse.
• responsible for gathering the requirements, designing and
developing the applications.
feb 2002    communication technologies, inc. – norfolk, va
military task force projects. use of microsoft ms-access, sql/pl, oracle
sep 1998    army national guard. – warrenton, mo
include hubs, routers, switches, bridges, multiplexes, modems, displays and
structural specialists are dedicated to erecting strong frameworks for any
customer.  routinely performed on-site installation and troubleshooting of
western geco houston texas    marine junior positioning specialist
within 5 meters. fixed and trouble gps systems. collect data from unix
database and use 2d/3d seismic modeling. your responsibilities will include
defense message system (dms). this includes microsoft windows nt, unix
cloud computing and data quality.  background in business administration
• 5 years of experience with cloudera hadoop distribution and
learning, deep learning, and/or large-scale “big data” mining
visualization tools.
vba.
use of databases and file systems  in hadoop big data environments such as
xml, blueprint xml, ajax, rest api, json,  pytorch, tensorflow, tableau,
apache ant, , apache flume, apache hadoop, apache hadoop yarn, apache
scipy, pandas, rdds, dataframes, datasets, mesos, apache tez, apache
foundry, github, bit bucket, microsoft power bi, microsoft visio, tableau,
|[pic]        |may 2016 - present                                          |
• using curator api on elasticsearch to data back up and restoring.
• used sqoop for etl of dataset between rdbms databases and hadoop
(elasticsearch, fluentd, kibana) for aws ec2 hosts.
hadoop distributed file system (hdfs)
• manage aws ec2 instances utilizing auto scaling, elastic load
• performed both major and minor upgrades to the existing hortonworks
• used spark streaming with kafka & hadoop distributed file system
analytics.
environments.
• designed and developed etl workflows using python and scala for
balancer, lambda, route53, cloudformation, auto scaling groups.
backups of ephemeral data-stores to s3 buckets, ebs.
files to/from hadoop distributed file system (hdfs).
digital marketing involves collection, aggregation, and analysis of massive
data scientists.
•  load and transform large sets of structured, semi structured and
spark streaming, acting directly on hadoop distributed file system
and pig jobs in the hadoop system.
data processing in hadoop system.
for data visualization.
db
|               |hadoop data engineer                                     |
|               |the weinberg group, washington, d.c.                     |
workflows in hadoop.
• moving data from oracle to hadoop distributed file system (hdfs). and
bring multiple clients into standard database format.
application server, web sphere application server, web sphere portal
performance, distributed nfs storage architecture
|               |government publishing office – washington, dc             |
• participated in data migrations from on premise to cloud systems.
• worked on entry level assignments.
• developed static web pages using html and css.
environment: javascript, html, php, css
cyber security & cyber policy
using flume, and storing in hadoop distributed file system (hdfs) for data
etl process.
experience collecting log data from various sources and integrating it into
distributed file system (hdfs) for further analysis.
• requirements documents) and technical documentation for metadata
• experienced in writing custom udfs and udafs for extending hive and
• knowledge in importing and exporting data using sqoop from hdfs to
• aws emr, ec2, data pipeline, sns, redshift, aws cli.
|maven, apache oozie, apache pig, apache spark, spark streaming, spark     |
processing model.  architected online systems and developed custom
• analyzed the requirements and provided estimation for the project.
• data modeling, technical design, and implementation for hive etl.
• export/import data from mainframe db2 into hdfs using sqoop.
analysis to meet business requirements.
• oozie scheduler to automate the pipeline workflow and orchestrate the
may 2016    nike, whq – beaverton, or
facilitated the movement of data to cloud clusters, and created custom bi
using spark rdds, python, and scala.
better scaling, more analytics and cost savings.  created multi-node hadoop
• wrote the shell scripts to monitor the health check of apache tomcat
to bring multiple clients into standard database format.
database systems: oracle 8i and oracle 9i, db2, pl, sql.
bachelor of science in computer information technology
university of vermont
emamm
monterrey institute of technology and higher education, mexico
management of structured data using azure sql database.
excellent written and oral communication skills with the ability to communicate complex business requirements at all levels including stake holders, power users, end users and developers.
developed architecture on microsoft azure spinning up hdinsight instances.
secured the system by creating surrogate key generations and hash key generation
involved in scheduling workflow engine and automating tasks to run multiple jobs.
huawei technologies in santa clara, ca
tested advertiser listener property through zookeeper data for securing kafka brokers.
deployed spark cluster and other services in aws using console.
enabled influxdb and configured influx database source into grafana interface
worked on ddl-oracle schema issues at time of ambari upgrade.
worked on disk space issues in production environment by monitoring how fast that space is filled, review what is being logged created a long-term fix for this issue (minimize info, debug, fatal logs, and audit logs).
intergrupo in miami, fl
responsible for creating development, staging and production environments for a new sharepoint server 2010 farm.
clouzzy s.a. de r.l. in mexico city, mx
email: xiangliangli101@gmail.com
spark api, scala, java, python, c, assembly language, unix shell scripting
jupyter notebooks, pycharm, intellij, eclipse, netbeans, vscode
hdfs, data warehouse, data lake, s3
use jira for ticketing system
use trello for tracking task and kanban
big data developer 	02/2016 – 05/2017
installed oozie workflow engine to run multiple spark jobs.
setup, configuration and management of security for hortonworks hadoop clusters using kerberos and integration of ranger at an enterprise level.
michigan technological university
big data fundamentals ibmethan karlson  |  hadoop |  702-430-3575 | ethanlkarlson@gmail.com
hadoop big data analytics project manager
comcast
syntel
maricopa cty
continuous integration servers
source control management
managed data modeling, forecasting and predictive analytics, and the creation of reports and presentations for management.
identified and ingested source data from different systems into hadoop hdfs and creating hbase tables to store variable data formats.
conducted exploratory data analysis and managed dashboard for weekly report, using
developed new flume agents to extract data from kafka and other web servers into hdfs.
integrated with the business users and documented their query, business, and reporting needs and analyzed the data and business logic underlying the requirement.
prepared and promoted reports of financial and operational data from company's financial reporting
a business analyst is responsible for understanding and utilizing the various agile methods used in the organization. the sdlc is integral to keeping a business innovating and staying ahead of the competition. the business analyst will research business trends to obtain the validity of any projects the organization needs to move forward. opening a dialogue with department heads to develop user stories to understand the functional and nonfunctional requirements. received a syntel certificate for level 1 insurance, a basic knowledge of insurance functionality along with risk evaluation.
the primary duty is the safe, orderly, and expeditious flow of air traffic in the designated air space. conducted air traffic operations in al asad, iraq during operation iraqi freedom/operation enduring freedom. during this operation earned the prestigious order of the spur from the 3rd cavalry and a certificate of appreciation from the talce. this required using encrypted radio transmissions within a hostile environment. using clear and concise communication with pilots and airfield crew in order to complete operations. during post 9/11 operations was assigned to airport checkpoint security and earned three awards. two certificate of appreciation from phoenix pd and department of the army homeland defense mission. passed the passenger screening checkpoint training from us dot faa.
design & implement end-to-end data pipeline using matillion etl tool
used matillion scripts to populate stage tables for visualization in looker
kept comprehensive up-to-date documentation of new code & functionality
integrated custom api endpoint & matillion components using python
supervised implementation of kafka-hive connector.
presented kstreams presentation.
successfully customized confluent hdfs connector code to expand security options.
implemented a cost-effective archival platform for storing big data using hadoop and its related technologies.
worked on streaming the analyzed data to the hbase using sqoop for making it available for visualization.
texas instruments, dallas, tx
francis mangornong
phone:  513-586-4775
html, css, php, javascript, markdown
data lake, data warehouse, s3
aws, azure hdinsight, google cloud, ibm
responsible for the design and implementation of the application’s postgres back-end.
responsible for design and implementation of rest endpoints and integrations with external services (i.e google cloud storage, google pubsub, airflow)
collaborated and integration with the ui/ux teams as to interface our back-end api for managing our airflow instance with the ui to enable users by allowing them to either create/customize or bring their own workflow to deploy into airflow.
set-up our own custom smtp server to allow for emails to be sent from our airflow instance to anyone within the enterprise.
collaborated with ci/cd teams to create and maintain teamcity pipelines to continuously publish and release iterations of our service, and on versioning, artifacts and docker images in the enterprise artifactory (and eventually google container registry).
performed evaluation of the existing system, and outlined proposed tasks, with the existing rudimentary flask application close to initial poc web application based on google kubernetes engine in gcp.
implemented spark using scala, utilized dataframes and spark sql api for faster processing of data.
this project involves the use of sensors to collect data on livestock health, feed and maintenance, and prediction of market demand and logistics from data stores.
worked on hive to create numerous internal and external tables.
rafael alejandro morell
skilled in the creation of kafka producer to connect to different external sources and bring the data to a kafka broker.
performed qa testing on data pipeline repositories using jenkins as a ci/cd service.
met with required parties to prepare and plan execution for new data ingestion.
collaborated with members responsible for etl to satisfy customer requirements.
wrote documentation for certain issues to dissect possible causes and alternative solutions.
west corporation - omaha, ne
frameworks and tools
• installation, configuration, and optimization of hadoop ecosystem
• expertise in big data cluster solutions, administration, securities
|ftp, python, html, xml, vbscript, |                                  |
|javascript                        |                                  |
hadoop architect and big data engineer
__________________________________________________________
✓ checking the code to github.
✓ developed postgres queries for real-time reports.
github2.4.1, maven3.2.1
big data dimension, atlanta, ga   may 2013-october 2014
✓ devised and lead the implementation of the next generation
architecture for more efficient data ingestion and processing.
experience.
each sprint.
state of louisiana, baton rouge, la     jan 2011-dec 2011
several builds and versions; also used database, text, checkpoints
✓ interacted with the developers to get an estimate and to resolve
technical issues.
✓ prepared weekly reports and build status reports.
java script, vb script, php, windows, java, sql, css, html, xml, ms
sql, mysql, rdbms, cassandra, hbase, dynamodb, redshift, cloudformation
data warehouse
file systems/formats
hadoop big data architect	october 2016 - present
responsibilities:
architected custom hadoop big data pipelines for specific use case analytics using hadoop to construct etl pipelines and analytics platforms on a cloud-based architecture.
hadoop data engineer	june 2015 - october 2016
aeris communications, chicago, il
big data and analytics solution architecture using hadoop hdfs and etl tools to transform iot data.
set-up oozie scheduler to automate the tasks of loading the data into hdfs and pre-processing with pig to process very large data sets of both batch and streaming data.
established preprocessing of the logs that were stored on hdfs using pig.
imported the processed data is into hive warehouse which enabled business analysts to write hive queries.
evident.io, inc., pleasanton, ca
heartbyte, inc. atlanta, ga
•	creating hive external tables to store the pig script output. working on them for data analysis in order to meet the business requirements.
•	worked with spark context, spark-sql, dataframe and pair rdds.
•	developed various data connections from data sourced to ssis, and tableau server for report and dashboard development.
designed high-detail network infrastructure and prepared detailed schematics using microsoft visio with specific descriptions on connections, ports, and addresses and with detailed instructions on how to implement.
experienced and certified professional in cloudera distributed apache hadoop (cdah) and oracle big data appliance (bda) in development and production environment.
migrated multiple databases environments to exadata machine using multiple migration approach.
hadoop distributions
security
apache hbase, dynamodb,  apache cassandra, datastax cassandra, apache phoenix, mongodb
big data tools
hadoop /aws data architect
paraccel, elasticsearch, cassandra, aurora
importing the data from the mysql and oracle into the hdfs using sqoop
experienced on loading and transforming of large sets of structured and unstructured data
deployment of applications using aws ec2
rolled out new staging tier by repurposing existing hardware, integrating with puppet and following the development team software and configuration specifications.
optimized and integrated hive, sqoop and flume into existing etl processes, accelerating the extraction, transformation, and loading of massive structured and unstructured data.
used hive to simulate data warehouse for performing client based transit system analytics.
performed security audit of mysql internal tables and user access.  revoked access for unauthorized users.
analyzing, profiling data for quality and reconciling data issues using sql.
regular database maintenance.
jr. data analyst
machine learning
hortonworks, inc.
big data engineer
alex.car3590@gmail.com
experience writing streaming applications with spark streaming/kafka.
handling schema changes in data stream using kafka.
big data engineer 	march 2018- present
built real-time streaming data pipelines with kafka, spark streaming and hive.
responsible for designing and deploying new elk clusters.
handled schema changes in data stream using kafka.
support for the clusters, topics on the kafka manager.
specified nodes and performed the data analysis queries on amazon redshift clusters on aws.
processed multiple terabytes of data stored in aws using elastic map reduce (emr) to aws redshift.
extensively worked on hiveql, join operations, writing custom udfs, and skilled in optimizing hive queries.
wrote shell scripts to automate workflows to pull data from various databases into hadoop.
ftr transportation intelligence
enabled security to the cluster using kerberos and integrated clusters with ldap at enterprise level.
performed cluster tuning and ensured high availability.
worked with cluster users to ensure efficient resource usage in the cluster and alleviate multi-tenancy concerns.
performed analytics on data using spark.
moving data into hadoop (ibm)
deep learning a-z: hands-on artificial (udemy)
phone: (703) 936-4058
used apache flume to collect logs and error messages across the cluster.
programming/ scripting and frameworks
database:  nosql: mongodb, firebase: sql: sql, mysql, postresql
cloudera, hortonworks, mapr, elastic
query/search
sql, hiveql, impala, apache solr, kibana, elasticsearch
visualization tools
tableau, pentaho, qlikview,
i was involved with two different teams during the capital one project.  my work involved coding in python and work with git, github, aws, nlp.
work: pull an entire information out of the team’s internal git repos
summary: using git-python library, write functions that give the desired info on any of the team’s repos. the objective was to create a software that helps in creating repos that have less errors and fall under good repos category.
send a preview of the email to the user logged in via sso
used spark dataframe api over cloudera platform to perform analytics on hive data.
created alter, insert, and delete queries involving lists, sets, and maps in datastax cassandra.
worked on impala to compare processing time of impala with apache hive for batch applications to implement the former in project.
extensively used impala to read, write, and query hadoop data in hdfs.
analyzed rto/rdo availabilities for virtual servers for time-lapse of recovery scenarios.
as part of batch modernization initiative in e2c, analyzed existing batch ingestion developed in oracle data integrator and developed pyspark application as etl tool. this reduced the batch ingestion time from 3.5 hrs to 15 minutes.
successfully loaded files to hdfs from teradata, and loaded from hdfs to hive.
education & training
ibm – moving data into hadoop
manish kathait   |   phone:  703-936-4058   |  email:  manishkathait484@gmail.com
concepts in hive and spark sql needed for optimization.
apache flume.
▪ experience using custom enterprise applications for data processing,
configuration of namespaces, znode data registers.
▪ able to achieve high throughput and low latency using zookeeper.
▪ use of various data visualization and reporting tools.
|apache spark, python, sql, oracle   |cloudera, hortonworks, ms azure,    |
|server 2003, 2008, 2008r2, red hat  |search tools                        |
|jira, solarwinds, rabbitmq, sensu,  |data mining                         |
|apatar, atom, fivetran, heka,       |apache kudu/arcadia, knime, tableau,|
|datacleaner, winpure data cleaning  |1010data                            |
procter & gamble – cincinnati, oh
• automated workflows with cron/oozie
arcadia (visuals)
• used postgresql cli for automated etl of data
• used aws cli for automated data pulls from psql/ga to s3
logs into headers and key/values
hive, apache impala, apache kudu, apache oozie, apache spark, hdfs,
backend.
• worked with engineers to implement yarn for analytics on data in hive.
• implemented kafka for realtime processing with data ingestion and
directory along with kerberos.
• modeled data in hive using hive external tables.
architecture.
environment: hdfs, pig, hive, sqoop, oozie, hbase, zookeeper, cloudera
• involved in creating hive tables, loading the data and writing hive
logical fashion.
pig, hive, spark.
• built continuous spark streaming etl pipeline with spark, kafka,
dashboarding needs and present solutions using structured waterfall
• using aws redshift for storing the data on the cloud.
• designed a cost-effective archival platform for storing big data using
• integrated kafka with spark streaming for real-time data processing
hadoop technologies.
• tuning and operating spark and its related technologies like spark sql
environment:  hadoop hdfs, spark, hdf, oozie, sqoop, mongodb, hive, pig,
hadoop data engineer   august 2012- february 2014
drillinginfo, austin, tx
• used linux shell scripts to automate the build process, and to perform
• documented requirements gathered from stakeholders.
to hive.
identify cyber-criminals stealing incentives and member information.
• irregular member behavior would trigger osint to begin intelligence
party vendors costing the
for automated access
• vulnerability and privacy advisor for all client contracts and third-
• developed and deployed a centralized knowledge base and documentation
• created executive weekly reports for the vice president of
• created templates, automated reporting, and documentation for
infrastructure knowledge bases and playbooks.
o decreased downtime by twenty-six percent.
provided employees with the common resolutions to problems.
• the open source replacement saved the company $350,000 dollars
• resolved efficiency issues that were previously unknown.
disruptions.
solarwinds, nagios, and nimsoft.
resolution.
university of north texas
– patent pending
708-669-9905
ou
skilled in on-prem and cloud environments such as aws, hadoop, hortonworks, cloudera
interacted with 3rd party rest api’s to ingest data
contributed to creating a data lake used for business reporting through the thoughtspot visualization tool.
created and maintained various s3 buckets through cloudfomation requiring differing requirements in configurations.
participated in an agile work place using jira, scrum, and kanban
architected and built robust data pipelines to ensure the minimization of data loss.
provided performance and cost analysis on various aws cloud architectures
configured kafka broker for the kafka cluster of the project and streamed the data to spark for structured streaming to get structured data by schema.
interacted with data residing in hdfs using spark to process the data.
sterling, va
deploy spark jobs into aws emr.
big data developer
used spark api over hadoop yarn to perform data processing to hive.
ran hadoop streaming jobs to process terabytes of xml format data.
new brunswick, njluke t richter
liaison for team with stakeholders, business units, data scientists/analysts and making sure all teams collaborate smoothly.
facilitation of meetings following scrum processes such as sprint planning, backlog, sprint retrospective, requirements gathering and providing planning and documentation for project; ensuring project is on track with stakeholders requirements.
worked with parquet, avro, and orc file formats.
used apache hadoop hdfs/s3 for creating data lakes for large disparate data sets.
hive ql, spark sql, sql, cql
developed a spark streaming service to receive real time data using kibana.
lockheed martin 	june 2016- april 2017
used hive query language (hql) for getting customer insights, to be used for critical decision making by business users.
handled security of the cluster by administering kerberos and ranger services.
responsible for data loading techniques like sqoop, flume.
worked with over 100 terabytes of data from data warehouse and over 1 petabyte of data from hadoop cluster.
developed a big data pipeline that streams microsoft kinect data through kafka and into spark with spark streaming. machine learning and etl is performed, and the data is stored within hive. legacy data is imported into hive via sqoop, and then processed with hadoop. an elk stack was set up to give visualizations to the end user.
worked extensively with kafka, spark streaming, hive and elk stack
managed hadoop clusters via command line, and hortonworks ambari agent.
jeeeun song
written and maintained automated salt scripts for elasticsearch, logstash, kibana,(elk).
worked with different teams to install operating systems, hadoop updates, patches, version upgrades of hortonworks as required.
configuring spark streaming to receive real time data from internal system and store the stream data to hdfs.
kafka or kafaka and spark streaming for data ingestion and cluster handling in real time processing.
moved transformed data to spark cluster where the data is set to go live on the application using kafka.
hortonworks(hdp)
aws s3
zookeeper
big data aws engineer
extensively worked on architecting serverless design using aws api, lambda, s3 and dynamo db with optimized design with auto-scaling performance.
worked on manage policies for aws s3 buckets and glacier for storage and backup on aws.
environments on aws cloud.
build and configure a virtual data center in the amazon web services (aws) cloud to support enterprise data warehouse hosting including virtual private cloud (vpc), public and private subnets, security groups, route tables, elastic load balancer.
hands-on experience installing, configuring, and deploying hadoop distributions in cloud environments
experience in oozie and workflow scheduler to manage hadoop jobs by direct acyclic graph (dag) of actions with control flows.
migrated data from rdbms for streaming or static data into the hadoop cluster using hive, pig, flume and sqoop.
worked on multi-clustered environment and setting up a hortonworks hadoop echo-system.
data processing like collecting, aggregating, moving from various sources using apache
relevant competencies
able to perform cluster and system performance tuning.
implemented, set-up and worked on various hadoop distributions (cloudera, hortonworks, amazon aws).
uses expert skills across a number of platforms and tools and working with multiple teams in high visibility roles.
search tools
ile formats
parquet, avro
writing of design documents for continuous integration and continuous deployment of ml models in aws environments.
wrote emr step jobs that automatically started spark jobs o emr cluster.
use aws sagemaker to serialize models and create inferencing endpoint.
used aws lambda to automate to create of sagemaker training jobs.
using flume to handle streaming data and loaded the data into hadoop cluster.
involved in complete big data flow of the application starting from data ingestion from upstream to hdfs.
exported analyzed data to relational databases using sqoop and generated reports for the bi team.
wrote sqoop scripts to inbound and outbound data to hdfs and validated the data before loading to check the duplicated data.
design and build scalable hadoop distributed data solutions using native, cloudera and hortonworks, spark, and hive.
experience working in hadoop-as-a-service (haas), subversion (svn), and sql and nosql databases
experienced in amazon web services (aws), and cloud services such as emr, ec2, s3, ebs and iam entities, roles, and users.
handling of large datasets using partitions, spark in-memory capabilities, broadcasts, joins, transformations in the ingestion process.
ability to contribute to design, architecture and technical strategy.
seattle, wa
developed application in several database environments including hadoop and sql server.
vpc, route 53, security groups, manage route, firewall policy, load balance dns setup.
hands-on fetching the live stream data from hdfs to hbase table using spark streaming and apache kafka.
responsible for database administration and provided business intelligence solutions. developed data analysis applications based on my needs analysis.  i also acted as team lead and mentor for data analysis requirements and development.
data transformation for proper scaling, decomposition, and aggregation of data.
hadoop 101
learning hadoop
skilled in aws, redshift, cassandra, dynamodb and various cloud tools.
use of cloud platforms aws, microsoft azure, and google cloud platform.
able to design and develop new systems and tools to enable clients to optimize and track using spark.
aws redshift, aws lambda, aws kinesis, aws, elk, aws cloud formation, aws iam
hadoop / apache big data technologies
adobe acrobat, lotus notes, ms office, rumba, oracle crm
redshift knowledge transfer: created powerpoint and presented to team covering aws data warehouse, oltp vs olap, managed, cluster based, scalable, architecture, what makes redshift fast, formats of data, create redshift cluster.
re-wrote sql scripts written in teradata sql scripts to postgres sql scripts that required a output derived from two tables.
deployed visualization files to github.
used jira for ticket tracking, change management and agile/scrum methodologies
transformed data from legacy tables to hdfs, and hbase tables using sqoop.
migrated the client facing database from postgres to mongodb leading to a 90% decrease in query response times.
created and maintained technical documentation for launching hadoop clusters and for executing pig
system administrator for three windows 2003 servers
installation configuration and troubleshooting ip dvrs and surveillance cameras
key person for setups in production release nights.
made changes to web pages for bellsouth’s front end online order application.
produced a document with one or more suggestions on how to fix defect.
found orders in oracle workflow to see where it got stuck.
bachelor of science in computer information systems
• etl, data extraction, transformation and load using hive, pig and hbase.
• spark architecture including spark core, spark sql, spark streaming,
• experience in using kafka as a messaging system to implement real-time
intelligence tool, tableau for better analysis of data.
hdfs, job tracker, task tracker, name node, and data node.
• very good knowledge and hands-on experience in cassandra, flume and yarn.
hive ql, sql, spark, python, scala, pig latin, c, c++, sql
data tools
tez, apache zookeeper, cloudera impala, hdfs, hortonworks, apache airflow
present     recording industry association of america (riaa)
• good exposure to iri end-end analytics service engine, new big data
• most of the infrastructure is on aws (aws emr distribution for hadoop,
• created etl functions between oracle and amazon redshift
mapr hadoop solution for digital media
exposure and advertising data related to consumer product goods.
structured data files, json datasets, hive tables, external databases
computation to generate the output response.
provided by sqlcontext and preferred to write queries using the hiveql
parser to read data from hive tables (fact, syndicate).
engine.
using sqoop.
environment: hadoop hdfs, hive, python, scala, kafka, spark streaming,
this global business process consulting company provides analytics for a
data processing and analysis for the corporate travel industry.  i
using amazon cloud services such as redshift, and hadoop ecosystem tools.
queries and pig scripts.
hive tables for the processed data in tabular format.
• wrote code to process and parse data from various sources and stored
db2, sql server, teradata to hdfs using sqoop.
streaming and perform transformations and aggregation on the fly to
• transformations, perform read/write operations, save the results to
• written hive jobs to parse the logs and structure them in tabular
dec 2013    jpmorgan private bank – austin, tx
hive for optimized data retrieval.
mysql) to hdfs using sqoop.used the hive jdbc to verify the data
stored in the hadoop cluster.
streamline diagnostics and maintenance process, and can streamline
from teradata rdbms.
• performance tuning for teradata sql statements using teradata explain
• developed a number of informatica mappings, mapplets, and
transformations to load data from relational and flat file sources
into the data mart.
strategy, router, filter, sequence
business rules and technical specifications.
transformations.
• trace errors occurred while loading.
which includes teradata sql assistant, teradata manager 6.0, bteq, mload,
erwin designer.
[pic][pic]
well-rounded, forward-thinking, and a highly motivated, self-starter with excellent communication and presentation skills. known for creative insights and creative solutions, and able to adapt to changing environments and needs.  a natural leader, and an active, contributing member of any organization, helping people to work together to achieve common goals.
7+ years of experience in hadoop-big data engineering, data analytics, data processing and database technologies, database and systems administration.
cloudera hadoop and hortonworks hadoop
feb 2019	present
management of usage credentials depending on which s3 bucket was being used.
the laurel and hardy team consisted of 5 software/data engineers, 2 project managers, and 1 data scientist.
daily scrum standups included the managers and the data scientist.
updated software for data validation lambda functions.
experience in optimizing the data storage in hive using partitioning and bucketing mechanisms on both the managed and external tables.
used spark api over hadoop yarn to perform analytics on data in hive.
open lending – austin, tx
installed and configured pig for etl jobs and made sure we had pig scripts with regular expression for data cleaning.
documented technical specs, dataflow, data models and class models.
environment: hadoop cluster, hdfs, hive, pig, sqoop, linux, hbase, shell scripting, eclipse, oozie, navigator.
software engineer ii
university of central florida – orlando
rahib aminbig data engineer  |  phone: 1-210-273-0469  email: rahibamin1985@gmail.com
phone: 1-210-273-0469
hadoop big data engineer and developer with skills in legacy hadoop ecosystems, cloudera hadoop, hortonworks hadoop and amazon web services. skilled in use of spark, spark streaming, spark sql, kafka, and kibana.
writing sql queries for data validation of the reports and dashboards as necessary.
able to design new custom solutions to solve business issues and advance goals.
worked on spark sql to check the data; wrote spark applications for data validation, cleansing, transformation, and custom aggregation.
set-up jenkins ci server for use by developers for continuous integration.
capco, new york, ny	january 2015-june 2016
installed oozie workflow engine to run multiple hive jobs.
moved relational database data using sqoop into hive dynamic partition tables using staging tables.
performed upgrades, patches and bug fixes in hadoop in a cluster environment.
wrote the hive scripts to process the hdfs data.
understands customer use cases and can create a vision on how to design and implement a solution.
cleanse, aggregate, and organize hadoop hdfs data lake.
hadoop ecosystem tools for etl and analysis, pipelines, and cleaning data in prep for analysis.
experience data processing like collecting, aggregating, moving from various sources using apache flume and kafka.
flume, apache storm, apache spark, nifi, apache kafka
batch & stream processing
developed a task execution framework using sql and hiveql.
master of arts in french and francophone studies
involved in building a multi-tenant cluster.
apache cassandra, apache hbase, mapr-db, mongodb, oracle, sql server, db2, sybase, rdbms, hdfs, parquet, avro, json, snappy, gzip, das, nas, san
aws, azure, anaconda cloud, elasticsearch, solr, lucene, cloudera, databricks, hortonworks
automated end-to-end ci/cd with jenkins pipeline across multiple environments
august 2018- march 2019
•	presented training seminars on how to improve development processes and foster a best-practices work culture.
•	mentored junior engineers, providing supervision and guidance on how to improve their skills and grow their potential.
american express – phoenix, az
created poc for migrating existing mapr hive on mr systems to apache spark 2.3.0, resulting in 1000% faster query responses.
tracked team’s biweekly sprint assignments in jira using scrum and kanban
created manual and automated test cases to test business workflow functionality.
amazon web services (aws) and involved in etl, data integration and migration.
cassandra data modeling for storing and transformation in spark using datastax connector.
• responsible for building scalable distributed data solutions using hadoop.
technologies:  rdbms, sql, oracle, xml.
may 2010- jun 2011
experience processing data using spark streaming api with scala.
very good knowledge and hands-on experience in cassandra, flume and yarn.
hands on experience on major components in hadoop echo systems like spark, hdfs, hive, pig, hbase, zookeeper, sqoop, oozie, flume, kafka.
expert in writing complex sql queries with databases like db2, mysql, sql server and ms sql server.
apache airflows
project methodology
big data distributions and platforms
amazonrds
implemented spark using scala and spark sql for faster testing and processing of data.
documented the requirements including the available code which should be implemented using spark, hive, hdfs and elasticsearch.
extracted real time feed using spark streaming and convert it to rdd and process data into data frame and load the data into cassandra. elasticsearch and logstash performance and configure tuning.
responsible for designing and deploying new elk clusters (elasticsearch, logstash, kibana, beats, kafka, zookeeper etc.
bash source databases and creating etl pipeline into kibana and elasticsearch. involved in the process of data acquisition, data pre-processing and data exploration of project in scala.
implemented hadoop using hortonworks data platform (hdp).
big data engineer – amazon aws cloud
clif bar, emeryville, ca     		april 2016 - july 2017
automated the installation of elk agent (file beat) with ansible playbook. developed kafka queue system to collect log data without data loss and publish to various sources.
intuitive research & technology, huntsville, al 	december 2014 - april 2016
worked in big data hadoop ecosystem technologies like hdfs, map reduce, yarn, apache hive, apache spark, hbase, scala and python for distributed processing of data.
spark involved in creating frameworks which utilized a large number of spark and hadoop applications running in series to create one cohesive e2e big data pipeline.
sage rutty, rochester ny 	august 2013 - december 2014
configured yarn capacity scheduler to support various business sla's.
• expertise in storm for reliable real-time data processing capabilities to
• bi (business intelligence) reports and designing etl workflows on
json, avro, parquet, orc, jupyter notebooks, eclipse, intellij, pycharm,
reporting / visualization [pic][pic]
data analysis, data modeling, artificial neural networks, jax-rpc, jax-ws,
bi, business analysis, risk assessmeent
kibana, tableau, aws, cloud foundry, github, bit bucket, sap hana,
etl tool. splunk, spark dataframes, apache storm, informatica power center,
into technical requirements.
• actively involved in setting up coding standards, prepared low and
requirement and worked with source system sme's in understanding the
source data behavior.
as per the design using orc file format and snappy compression.
• wrote different pig scripts to clean up the ingested data and created
ambari, spark, oozie, impala, sql, java (jdk 1.6), eclipse. spring mvc,
cybersecurity, ip addresses, ontological classification, url, code and
indexes.
data by running hive queries and pig scripts.
• partitioned and bucketed hive tables; maintained and aggregated daily
accretions of data.
• configured spark streaming to receive real time data and store the
stream data to hdfs.
in different environments.
• worked on installing clusters, commissioning & decommissioning of data
applications.
• migrating the needed data from oracle, mysql into hdfs using sqoop and
aggregations before storing the data onto hdfs.
mobile and network devices from the ptc (positive train control)
network using apache nifi and stored the data into hdfs for analysis.
implemented spark rdd transformations, actions to migrate map reduce
olap systems and scheduled jobs to call the packages and stored
• involved in scheduling oozie workflow engine to run multiple hive and
pig jobs.
zookeeper, apache ignite, sql, pl/sql, unix shell scripts, java, python,
administration, customization and etl data transformations for bi analysis
programs on servers
to study customer behavior
• worked on creating mapreduce programs to parse the data for claim
developed data management system using mysql
solutions, and tailor application to customer needs.
pig, hbase, cassandra, scala, sqoop, oozie, unix shell scripting, linux,
for setup the environment and production environments in server and
database level.
configuration data to provide persistence services (and persistent
• implemented oracle advanced queuing using jms and message-driven
(insert, update, and delete)
• implemented dependency injection of spring framework. developed and
operations.
• wrote unix shell scripts and used unix environment to deploy the ear
• took ownership of implementing & unit testing the apis using java,
master of business administration in engineering
routing networks based on interests for structured peer-to-peer system
vicente guevara
470-344-0250
antoniolezama552@
• oracle certified professional 0 mysql 5 developer
• microsoft certified professional windows server 2012
experience converting hive/sql queries into spark transformations using spark rdds, python and scala.
able to work with team and cross-functionally to research and design solutions to speed up or enhance delivery within the current platform.
expert in big data ecosystem using hadoop, spark, kafka with column-oriented big data systems such as vertica and cassandra.
batch processing expertise
scala, python, hiveql, sql, pig latin
october 2018 – present
ci/cd pipeline
i was responsible to design and implement  a ci/cd pipeline to deploy ecs infrastructure that was able to support 2 apis endpoints with fault tolerance and high availability.  i also decoupled the platform refactoring the maven projects and designed a solution in which different teams can have a solid contribution model with have multiple environments to develop the solution.
performed disaster recovery exercises to meet rto and rpo.
designed and implemented multiple ci/cd pipelines using terraform and cloudformation.
ensuredssecurity and quality of the applications, microservices and apis.
responsible for atdds and unit testing for java and scala application.
designed ecs cluster to support two api endpoints and meet the required performance and on-peak point autoscale in the most optimized and cost-effective way.
hadoop, aws, agile
integrated frameworks to match security rules like drools with flink cep patterns in a single datastream.
maven used for the managing the project lifecycle, svn repository, log4j for logs.
union of multiple streams with coprocessfunction matching the elements by the key.
flink working with tumbling windows, sliding windows and global windows setting an alert trigger based on the current state for each keyed element.
java
lombok
created data modeling and implemented redshift instance on amazon.
developed shell scripting to automate the data flow of daily tasks.
created both internal and external tables in hive, and developed pig scripts to preprocess the data for analysis.
worked on cassandra query language to load the bulk of data and execute queries.
designed appropriate partitioning/bucketing schema to allow faster data retrieval during analysis using hive.
redshift
json
performance tuning
importing and exporting data into hdfs using sqoop.
may 2013 – june 2014
real page, inc., carrollton, tx
used scala to store streaming data to hdfs and to implement spark for faster processing of data.
ftp
t-systems méxico, puebla, mexico
transitioned and transformed projects from the customer to the t-systems standards
vmware
big data professional with 5 years of experience in engineering, development and admin
installed and configured a three-node cluster with hortonworks data platform (hdp 2.5) on the hp infrastructure and management.
developed custom aggregate functions and udf - spark sql and performed interactive querying.
kafka/hadoop upgrades on large environments.
created aws lambda function for extracting the data from kinesis firehose and post the data to aws s3 bucket on scheduled basis (every 4 hours) using aws cloud watch event.
download data through sqoop and hive in hdfs platform.
real time streaming of data using spark with kafka.
education and certifications
fluent in architecture and engineering of the cloudera hadoop ecosystem.
mapr, elastic cloud, anaconda cloud
developed a task execution framework on ec2 instances using amazon redshift and arangodb.
involved in loading data from linux file system to aws s3 and hdfs.
configured jupyter for spark web clients.
used iot with big data analytics architecture on aws to transform supply chain management.
collected the real-time data from kafka using spark streaming and perform transformations.
using sqoop to extract the data back to relational database for business reporting.
pittsburg,  pa
hadoop components
zookeeper, oozie, cloudera manager, ambari, yarn
architecture of etl data pipelines
spark api, spark streaming, spark sql, spark structured streaming
infusionsoft, chandler, az 	feb 2016-march 2017
administered hadoop cluster (cdm) and reviewed log files of all daemons.
used hadoop streaming api with python for rdf data files' extraction and transformation.
created phoenix tables, mapped to hbase tables and implemented sql queries to retrieve data.
proficient in major vendor hadoop distribution like cloudera, hortonworks.
aws, azure, anaconda cloud, elasticsearch, solr, lucene, cloudera, databricks, hortonworks, elastic mapreduce
thirdlove’s distinction is it’s use of data to create a better fit- voluminous amounts of data—600 million data points such as breast shape, cup fit and band tightness from over 11 million women that it has culled from an online questionnaire—to create better-fitting bras, especially by pioneering the use of half-sizes. this has resulted in 24 bra styles in 74 sizes.
investigation of machine learning at scale using amazon sagemaker on aws.
architecting and devops for aws & google cloud services including in house data center for middleware system and web services. also, managing security review and web compliance management.
cloud watch monitor for s3 & glacier storage management, access control and policy
google cloud platform (gcp)
used spark to optimize etl jobs.
work closely with management and team to understand the existing systems and provided recommendations for cicd automation.
hadoop hdfs, nosql, apache cassandra, apache hbase, mongodb, mysql, mssql, crystal reports, netsuite, great plains, salesforce, sap, oracle
hadoop distributions & cloud
elasticsearch, elastic mapreduce, elk stack (elasticsearch, logstash, kibana), nifi
netsuite, great plains, salesforce, sap, oracle, daceasy, ms office suite, quickbooks, windows linux, sage50
“currently working with a team where we setup an elk stack along with other tools to monitor data transfers.  we also used kafka and spark to pull in logs and processed the information for better usage.”
install and configure kafka cluster and monitoring the cluster.
optimized a spark cluster for better querying, reading, and writing of data saved in aws s3 buckets.
set-up aws lambdas to process event-driven functions to various aws resources.
managed and monitored aws ec2 instances through aws management console.
boston, ma
configured, installed and managed hortonworks distributions is a multi-cluster environment.
worked on resolving ranger and kerberos issues.
implement and maintain security ldap, kerberos as designed for cluster.
“working as the primary analyst developed my skills as a team leader.  many customers and co-workers dependent on me to provide solutions to their issues.”
provided customer service and technical support by solving questions on accounts payables, accounts receivables, and payroll modules in a high volume call center.
helped customers create custom invoices, statements, and reports.
designed and developed applications for business partners and customers to automate daily processes.
ibm cognitive class - hadoop
ibm cognitive class - spark fundamentals
senior hadoop engineer / july 2017 – present
good experience with nosql database hbase and creating hbase tables to load large sets of semi structured data coming from various sources.
developed pig latin scripts to extract the data from the web server output files to load into hdfs.
develop different components of system like hadoop process that involves map reduce, and hive.
commissioning and de-commissioning the data nodes and involve in name node maintenance.
regular backup and clear logs from hdfs space. this is to utilize data nodes optimally. write shell scripts for time bound commands execution.
involve code review tasks in simple to complex map/reduce jobs using hive and pig
develop custom pipelines for real-time or near realtime data analysis using spark, spark streaming and kafka.
xml, ajax, json, avro, parquet, orc
linux/unix, windows
work with kubernetes containers launching spark applications with scala
pushing containers and to aws emr using scala
this aws implementation allowed the company to use accelerated trade promotion planning, a sap solution powered by sap hana, and used this as a source of intelligent data.
for high availability, kellogg leverages multiple aws availability zones (azs) without the additional cost of maintaining a separate data center.
designed architecture/hadoop infrastructure layout for development/test and production environments
upgraded ambari to 2.2.2 and hdp stack to 2.4.3 (hive, spark, yarn, hbase, ranger)
managed five member offshore team.
utilized 6 to 10 amazon ec2 instances for data analysis, scaling them according to demand, adding more instances as needed to process extra data or extract more information.
used amazon s3 for data storage, and elastic load balancing for performance and stability.
edina, mn	march 2013 – january 2014
involved in scheduling oozie workflow engine to run multiple hiveql, sqoop and spark jobs.
1- 978-708-1080
mcgraw-hill education, boston, ma
used databricks running spark scala for event processing and transformations on event data.
built data ingestion and processing pipelines with spark on databricks.
created solutions to transform data from various sources and load it into platforms such as hadoop, snowflake to create a data lake.
excellent understanding of hadoop architecture and underlying hadoop framework including storage management.
created hive schemas using performance techniques like partitioning and bucketing.
imported data from different sources like hdfs/hbase 0.94.27 to spark rdd.
big data engineer	may 2016 – august 2017
responsible for development and management of all cluster related testing activities.
implemented hadoop data pipeline to identify customer behavioral patterns, improving ux on e-commerce website.
integrated hadoop into traditional etl, accelerating the extraction, transformation, and loading of massive structured and unstructured data.
used hive to analyze partitioned and bucketed data and compute various metrics for reporting.
big data engineer	february 2015 – may 2016
cluster and configuration management systems, like docker and kubernetes
created volume groups, logical volumes and partitions on the linux servers and mounted file systems on the created partitions.
deployed network file system for name node metadata backup.
architecture of etl data pipelines within hadoop ecosystem making use of hadoop, cloudera hadoop, hortonworks, hadoop and aws emr to process data from a variety of data stores and file types; structures and unstructured data.
proficient in cloudera hadoop distribution, hortonworks distribution, etl processing using spark, spark streaming, storm.
analysis of data lakes including hadoop ecosystem repositories (hive, nosql, rdbms, redshift).
aws tools (redshift, kinesis, s3, ec2, emr, dynamodb, elasticsearch, lambda)
programming & scripting languages:  spark, spark streaming, hive, mapreduce, python, scala, sql, mysql, shell scripting, mongodb, java, python
project methods:  agile, scrum, devops, continuous integration,
security:  kerberos authentication, applications on kerberos secured cluster
designed and developed interactive kibana dashboards and custom reports which involved modeling and algorithms from data derived from hadoop data lakes and pipelines.
defined job flows, using spark
created multiple visualization reports/dashboards using dual axes charts, histograms, filled map, bubble chart, bar chart, line chart, tree map, box and whisker plot, stacked bar etc.
capital one | mclean, va
worked on large data warehouse analysis services servers and developed the different reports for the analysis from that servers.
tiaa| new york, ny
implemented applications on kerberos secured cluster.
--------------------------
strong hands on experience in hadoop framework and its ecosystem including but not limited to hdfs architecture, mapreduce programming, hive, sqoop, hbase, mongodb, cassandra, oozie, spark rdds, spark dataframes, spark datasets, etc.
excellent knowledge on hadoop ecosystems such as hdfs, configuration of hadoop clusters, yarn, mapreduce, spark, hbase, hive, ranger.
(september 2017 - present)
valero energy corp.- san antonio, tx
assisted in the acquisition, transformation, and preparation of data for analysis and mining.
extracted and translated data from various file formats such as json, text, avro, and orc.
performed different types of transformations and actions on the rdd to meet the business requirements.
developing scripts and batch job to schedule various hadoop program.
kraft- chicago, il
capital group is an american financial services company. it ranks among the world’s oldest and largest investment management organizations, with $1.87 trillion in assets under management.
configuring views in ambari on separate ambari server.
checking the compatibility of spark jar with the existing environment & related to spark tuning.
implemented cicd tools such as jenkins and used to improve and automate processes.
elasticsearch and logstash (elk) performance and configure tuning.
kafka for data ingestion and extraction with move into hdfs.
setup airflow and jenkins server on ec2 instances
in charge of development of spark jobs using spark 2.3 on emr
created emr/spark cluster to perform etl from diverse clients
fine-tuned resources for long-running spark applications to utilize better parallelism and executor memory for more caching.
performed transformations and analysis using hive
used aws kinesis process data and load into aws rds mysql database and s3.
implemented kafka security features using ssl and without kerberos.
knowledge of horton works and amazon emr hadoop distributions.
suggested new technologies and approaches for new tasks.
worked closely with other data scientists to complete special projects and achieve project deadlines.
migrating the needed data from oracle, mysql in to hdfs using sqoop and importing various formats of flat files in to hdfs.
wrote complex hive and sql queries for data analysis to meet business requirements.
used oozie scheduler to automate the pipeline workflow and orchestrate the sqoop, hive and pig jobs that extract the data on a timely manner.
phone: (813) 882-5638
collaborative, team spirit
bright systems
facilitation of meetings following scrum processes such as sprint planning, backlog, sprint retrospective, requirements gathering and providing planning and documentation for project; ensuring project is on track with stakeholder wishes.
qlikview for better analysis of data.
expert in performance optimization
ran spark jobs on top of raw data and transforming
designed and implemented incremental imports into hive tables.
experienced in managing and reviewing the hadoop log files.
worked on uml diagrams for the project use case.
performed the performance and tuning at source, target and data stage job levels using indexes, hints and
good knowledge in pl/sql, hands on experience in writing medium level sql queries
in-depth understanding of data structures and optimization.
ingalls memorial hospital, harvey, illinois
performed data entry functions while creating daily reports on treatment programs through in-house engineered records system.
responsible for the day-to-day database and management operations of a psycho- therapy terminal databases for client access service delivery and customer account management with an average of $400,000-$600, 000 in annual revenue;
hadoop/big data engineer
built real-time big data solutions using hbase handling billions of records
troubleshooting spark applications to make them more error tolerant.
expert knowledge in hadoop/hdfs, mapreduce, hbase, pig, sqoop, cloud: amazon elastic map reduce (emr), amazon ec2, rackspace, google cloud.
experience in hadoop framework and its ecosystem including but not limited to hdfs architecture, mapreduce programming, hive, pig, sqoop, hbase, oozie etc.
reddyshiva0007@gmail.com
cloud data systems
engineering
srm university, india
provide proof-of-concepts to reduce engineering churn.
give extensive presentations about the hadoop ecosystem, best practices, data architecture in hadoop.
implemented spark using scala, and utilized data frames and spark sql api for faster processing of data.
implemented partitioning, dynamic partitions, and buckets in hive. involved in converting hiveql/sql queries into spark transformations using spark rdds, python and scala.
installed and configured hive and also written hive udfs.
worked in installing cluster, commissioning & decommissioning of data node, name node recovery, capacity planning, and cluster configuration.
proficiency with modern natural language processing and general machine learning techniques and approaches.
load and transform large sets of structured, semi structured and unstructured data
developed map reduce jobs using api.
experienced in running hadoop streaming jobs to process terabytes of xml format data.
created hbase tables to store variable data formats of pii data coming from different portfolios.
10+ years total information technology
contact information
spark, scala, kafka, mapreduce, sql, python
data analysis
apache solr, cloudera impala, cloudera, hortonworks, mapr
this project required an understanding of business rules, business logic, and use cases to be implemented.
developed and coded exclusion rules workflow to connect it to ability-to-pay external process using spark, quantum, sql and python.
worked on data analysis pipelines for the company’s affymetrix microarray analysis product line which includes products and tools used by researchers studying plant and animal genomics and transcriptomics, including basic research and industrial application of technologies for breeding, population diversity and conservation, trait analysis, and more.  this research is used in cancer research, pharmacological trials and more.  the products and tools provided collect data and the big data system provides a place to gather and make use of that data.
worked on an azure cloud environment implementing azure hdinsights.
setup the automatic tuning on production database to create index based on inheritance from azure sql. the improvement increases sharply to show the result in few milliseconds.
hadoop cloud engineer	06.2015 – 11.2016
communicated deliverables status to stakeholders and facilitated periodic review meetings.
parker hannifin corp., cleveland, oh
iot, big data and analytics platform development using aws to transforming supply chain management.
database development & administration using sql server as well as access support to in-house systems for sales analysis and forecasts.  development of ad-hoc systems for sales forecasts extraction, download and processing of sap bw / bo2 system information.
5 years in information technology
mastered use of the different columnar file formats like rcfile, orc and parquet formats.
walmart	bentonville, ar
through analysis of customer preferences and shopping patterns, walmart can accelerate decision making on how to stock store shelves and display merchandise.  big data provides insight on new items, discontinued products and which private brands to carry.
standard chartered bank usa	manhattan, ny
developed and ran map-reduce jobs on yarn and hadoop clusters to produce daily and monthly reports per requirements.
georgia tech signal processing lab	atlanta, ga
collaborated and assisted with researching and implementing c++ code to track and locate speakers’ voices in speech recordings.
eric bannavti   |   phone  999-999-9999   |  email@gmail.comchris peng
formats
consultant@gmail.com
spark sql to perform big data transformations and actions on data residing in hdfs.
/2018 -present
applied the latest development approaches including applications in spark using scala. integrated spark code into the sdlc with the ci/cd pipeline using jenkins ci with git versioning.
handled over millions of messages per a day funneled through kafka topics.
worked with jenkins ci for cicd and git version control.
hands-on data extraction from different databases on aws and scheduling job with airflow to execute the task daily.
worked with aws emr and s3
mobile
implemented lambda architecture in data warehouses like redshift for business intelligence analytics
web scripting
service layer
javascript, jquery, ajax, knockout, bootstrap, angularjs, react js.
slomin’s
provided status to stakeholders and facilitated periodic review meetings.
captured data and importing it to hdfs using flume and kafka for semi-structured data and sqoop for existing relational databases.
loaded rdbms of large datasets to big data by using sqoop
physics intern
professional development
university of bridgeport, bridgeport, ct
amity institute of nanotechnology, amity university, noida, india
honor society of the upsilon pi epsilon for the computing and information
use flume, kafka, nifi, and hiveql scripts to extract, transform, and load the data into database.
hadoop, hive, spark, maven, ant, kafka, hbase, yarn, flume, zookeeper, impala. hdfs, pig, mesos, oozie, tez, zookeeper, apache airflows
kinesis, spark, spark streaming, spark structured streaming
worked on multi-clustered environment and setting up cloudera and hortonworks hadoop echo-system.
handled structured data via spark sql, stored into hive tables for consumption.
migrated streaming or static rdbms data into hadoop cluster from dynamically-generated files using flume and sqoop.
created unix shell scripts to automate the build process, and to perform regular jobs like file transfers between different hosts.
created hive generic udf's to process business logic.
worked with various file formats (delimited text files, click stream log files, apache log files, parquet files, avro files, json files, xml files).
proficient in writing stored procedures, complex sql queries, optimizing the sql to improve performance, packages, functions and database triggers using sql and possess strong data analysis skills using python, hive, apache spark, ms excel and access db.
experience in using ides such as eclipse, intellij for debugging and developing python and spark applications.
distributions & cloud
project points
data engineer		november 2015 – april 2017
involved in design and architecture of new hadoop project.
x-fab semi-conductors usa		lubbock, tx
worked with large datasets from database utilizing sql
platform by per scholas, new york, ny
active participant of two big data projects:
files provisioning:  the ecs cluster on linux red hat, transfers files from different amazon s3 storage. coding is in java using aws api.
optimized spark job significantly by proposing new file format, avro, in a spark process of joining multiple data frames in the program.
environment: amazon web services (aws)
may 2016 	hadoop data architect/engineer
worked with amazon web services (aws) and involved in etl, data integration and migration.
responsible for administration of big data system using hadoop on premises including data loss prevention policies, optimization, backup and restore of databases and management of hadoop clusters on servers.
may 2010	programmer
sql, java, python, scala, pig, hive, rdds, dataframes & mapreduce.
distributed file systems -hdfs, parallel processing - mapreduce
hive and pig core functionality by writing custom udfs.
• well-versed in and implemented partitioning, dynamic-partitioning and
business needs
scripting, object-oriented design, object-oriented programming,
aug 2016    distributed data processing software architect /engineer
present     the weather channel– atlanta, ga
spark & spark streaming.
• administered hadoop cluster(cdh) and reviewed log files of all
authentication.
data from third-party api to amazon aws
• developed spark sql script for handling different data sets and
in scala did troubleshooting using scala problems to produce
dec 2014    trendkite – austin, tx
campaigns.  created analytics pipelines and etl processes with spark,
tools like nagios, ganglia, cloudera manager.
hortonworks for multiple clients.
basis, maintaining cluster on healthy on different hadoop
into the hdfs.
environment: hadoop, mapreduce, cloudera, hive, pig, kafka, sqoop,
mainframes, java 7.0, log4j, junit, mrunit, svn, jira.
aug 2013    kaiser permanente – oakland, ca
• created hbase tables to load large sets of structured data.
• profound knowledge shared with teammates in zookeeper, mongodb,
• excellent java, j2ee application development skills with strong
ejb, jdbc, jndi, struts, maven, git, junit, sql language.
track of the employees and optimize the usage of their skills.
• designed all user interfaces using jsp and deployed the application in
• involved in production support.
university of florida, gainesville, fl
oracle certified associate
oliver teng
senior business intelligence analyst and data engineer
accustomed to working with database, data warehouse, data mart and data lake as repositories for volumes of data in various formats.
conducted quantitative analyses on available metrics and briefed senior leadership, enabling them to improve production and enhance corporate strategy.
sql, c, c++, asp/asp.net, .net framework 2.0/3.0/3.5
support for existing and new packages in  ssis
created a metrics report/dashboard with daily refresh, collecting data from several databases, which saved 10 hours of workload for 1 user, in a previous version updated on demand.
created several reports on demand for hipa compliance.
developed automated deployment testing scripts in powershell for windows
created calculated columns and measures in power bi and excel depending on the requirement using dax queries.
update some bi solutions redesigning etl's and reports due change of business rules
reduce processing time from 1.5 to half hour, finishing 3 hours before sla.
integrated with 3rd party packages, custom built solutions, and system integrations with salesforce.
designed ssrs reports with sub reports, dynamic sorting, defining data source and subtotals for the report.
created different types of reports such as cross-tab, drill-down, drill through, olap and sub reports, and formatting them using both ssrs 2012.
grupo ti mexico, banregio financial group – monterrey, mexico
created olap cubes and dimensions for the insurance model using ssas.
redesign olap cubes for the supply chain area to improve processing time implementing partition strategy and data mart utilization.
creating reports as per user requirements using sql server reporting services 2008 r2.
designed and implemented a worldwide financial portal using sharepoint 2007, with information of branches in usa, europe, asia, mexico, central and south america and the consolidated corporate.
designed and implemented analytical dashboard for the school hypercube model using microsoft proclarity and performancepoint for desktop and mobile users.
designed analytical dashboards for school hypercube model using microsoft proclarity and performancepoint server for desktop and mobile users.
in charge of installing and configuring sharepoint 2007 and 2010.
5 years  - big data engineering / information technology -  5 years
summary of competencies
set the spark job to process the data to redshift and emr hdfs(hadoop).
hadoop, cloudera (cdh), hortonworks data platform (hdp)
java, pyspark, python, spark, scala
agile, kanban, scrum, continuous integration, test-driven development (tdd), unit testing, functional testing, git, github, jenkins ci (ci/cd for continuous integration)
hdfs, parquet, avro, snappy, gzip, orc
spark sql, hiveql
data frames
dick's sporting goods-oakdale, pa 	july 2017-present
used spark api over hortonworks, hadoop yarn to perform analytics on data in hive.
analyzed and tuned data model cassandra tables during db2 to cassandra migration process.
migrated data from elasticsearch-1.4.3 cluster to elasticsearch-5.6.4 using logstash, kafka for all environments.
pinnacle financial partners, nashville, tn 	jan 2016-july 2017
used ambari stack to manage big data clusters, and performed upgrades for ambari stack, elastic search etc.
certified scrum master
technology and big data analytics systems using open source technologies,
communication skills.  i excel at creative cohesive teams, workability,
• accustomed to working with large complex data sets, real-time/near real-
hortonworks, and mapr.
|and database technologies.          |• c++ • xml • html • css, visual    |
|development, unit testing,          |operatign systems                   |
can be used to make valuable predictions and forcasts involving health and
this project focused on process and performance improvement of the data
singhofen & associates, inc. – miami, fl
performed performance tuning for spark steaming e.g. setting right batch
to build a continuous etl pipeline. this is used for real time analytics
collect, aggregate, and move data from servers to hdfs using apache spark &
migrated etl jobs to pig scripts for transformations, joins, aggregations
implemented yarn resource pools to share resources of cluster for yarn jobs
to hdfs using scale.
collected the business requirements from the subject matter experts like
involved in design and development of technical specifications using hadoop
tuning and operating spark and its related technologies like spark sql and
the products in all the stores without taking performance hit.
analysis including cost, shipment details and track the inventory.
design and development of action & form objects as part of struts frame
i am a hadoop data systems engineer with 8 years of professional experience
sql, oracle, msql, toad, nosql, rdbms, apache cassandra, apache hbase in
• managed system security using sentry, and administered hadoop cluster
• used spark sql and dataframes api to load structured and semi
worked with hadoop ecosystem, hadoop distributed file system (hdfs), hive,
to manipulate the hadoop derived data.
scala, hdfs and mongodb.
using spark rdds, python and scala.
• handled 20 tb of data volume with 120-node cluster in production
• consumed the data from kafka queue using storm.
system (hdfs) using kafka.
• used sqoop to efficiently transfer data between databases and hadoop
bi developer     sep 2010 - jul 2012
forward. worked to understand information needs and build solutions to
deliver the key metrics to business decisions and expand the content of
• worked closely with peers, other business areas in and outside of hr,
generating transforming files from different analytical formats to
life cycle data events.
dashboards and reports that make data actionable.
• worked with it teams to identify specifically how changes should be
impact to other data/reports.
• performed user acceptance testing as new content is being added to hr
• supported solutions by monitoring and fine-tuning queries and data
associate of science in oracle database management, techgigsolutions
• experience with hadoop big data infrastructure for batch data
← 10 years of experience in data management, data processing, and data
← experience collecting log data from various sources and integrating
including but not limited to hdfs architecture, hive, pig, sqoop,
languagestypessystemsinfrastructuretoolshadoop ecosystemhive
query
transformhdfsnas, sanorc
nosqlwebhtml, css< jquery, javascript, bootstrapos/networkamazon ec2, aws3, ses, route 53, google app engineer, herokuubuntu
supported gbi platform team.
automate installation of hadoop on aws using cloudera director
scheduled oozie workflow engine to run multiple hive and pig jobs, which independently run with time and data availability.
implemented http requests to call the specific api using the python libraries.
jan 2014 hadoop data engineer
sep 2010 bi analyst
i worked in data processing in database administration.  this involved some hardware side, database servers, software use or reporting and querying to compile data for management reports on inventory and forecasting.
proficient with etl tools for apache hadoop for analysis of big data.
hands-on experience on fetching the live stream data from rdms to hbase table using spark streaming and apache kafka.
hands on experience on working with amazon emr framework transferring data to ec2 server.
good knowledge in software development life cycle (sdlc) and software testing life cycle (stlc).
-technical skills-
hadoop, cloudera hadoop, hortonworks hadoop, cloudera impala, talend, informatica, aws, microsoft azure, adobe cloud, elastic cloud, anaconda cloud,
bi/reporting & visualization
waterfall, agile scrum, governance, change management, policies, specifications, security, hippa, onboarding, offboarding, managed metadata, infrastructure, best practices, app support, user level agreements, service level agreements, migration, integration, customization, communication, mentoring, team lead, implementation planning, microsoft office, outlook
san antonio, tx
development of oozie workflows for data extraction and transformations from s3 into hive.
development of oozie workflows for transferring output data into ftp servers for reporting.
the kafka consumer reads from a hard-coded kafka topic, creates a dataset and delivers parsed outputs to the console.
note that in one case i used spark context, which is deprecated and other case with spark session, which is the new standard.
involved in complete big data flow of the application starting from data ingestion from upstream to hdfs,
using pig predefined functions to convert the fixed width file to delimited file.
environment: hadoop, hdfs, hive, pig, sqoop, cloudera, hortonworks, nosql, hbase, shell
the development team software and configuration specifications.
understand the requirements and prepared architecture document for the big data project.
spark python.
developed oozie workflow for scheduling and orchestrating the etl process.
wrote shell scripts for rolling day-to-day processes and it is automated
created custom dashboards in tableau, and prepared workflows to allow power users to interact with the system to request queries and report information.
work closely with business, transforming business requirements to technical requirements.
hands on experience on pig, to join raw data using pig scripting.
hands on experience on extracting data from different databases and scheduled oozie workflow to execute this job daily.
partitioned and bucketing the hive tables, keep on adding data daily and perform aggregations.
communicate deliverables status to user/stakeholders, client and drive periodic review meetings.
creation of front-end pages using jquery, javascript, and html of the intranet.
creating the solutions requirement document, listing the solution against each functional requirement.
developed and consumed web services for analysis and integration with field reports.
experience in developing rest api's for use in single page or native applications and
implementing rails migrations, active record, action pack and action mailer.
autocad • matlab • revit • ltspice • pspice • multisim • microsoft office suites
amazon aws, microsoft azure, elasticsearch, apache solr, lucene, cloudera hadoop, cloudera impala, databricks, hortonworks hadoop
tableau, microsoft power bi
note:  siemens big data is in princeton new jersey.  the use cosmos
data engineer		month, year
made recommendations and significant improvements through cicd automation.
experience with spark structured streaming to process structured streaming data.
hands-on experience using apache spark framework with scala.
resolved the problem of duplicate columns, and thus the inefficiency of the job as a whole.
worked with analysts to model cassandra tables from business rules and enhance/optimize the existing tables.
built real-time streaming data pipelines with kafka, spark streaming and hbase.
cloudera hadoop distribution version cdh5 for executing the respective scripts.
el segundo, ca
university of nevada at las vegas
hive, rdds, dataframes.
into hdfs using flume; staging data in hdfs for further analysis.
• experience deploying large multiple nodes of a hadoop and spark cluster.
• experience developing custom large-scale enterprise applications using
• excellent knowledge on hadoop architecture and ecosystems such as hdfs,
configuration of nodes, yarn, sentry, spark, falcon, hbase, hive, pig,
mongodb, cassandra, oozie, spark rdds, spark dataframes, spark datasets,
parquet, avro, json, snappy, gzip
•     performance tuning of hive service for better query performance on ad-
•     created hive external tables and designed data models in hive.
consulting projects for clients using pos systems analytics and automation
workflows to execute the task daily.
processing data in hdfs & mongodb.
kafka, pig, hive, spark.
based on policy.
• involved in meetings with cross-functional team of key stakeholders to
derive a set of functional specifications, requirements =, use case
• integrated kafka with spark streaming for real time data processing
• tuning and operating spark and its related technologies like sql.
kafka, sql, acro, rdd. sqs s3, cloud, mysql, informatica, dynamo db
well inventory, pioneer natural resources began using a statistical system
• responsible for architecture and implementation of data pipelines in
• worked on installing cluster, commissioning and decommissioning of
• configuring dns and dhcp on clients’ networks.
ftp.
• installation, configuration and troubleshooting of solaris, linux
• assist users and students in support of windows xp, 2000 environments.
• installed & configuring virtualization technologies like vmware esxi.
bachelor of science in information systems security
associate of applied science in it/computer network systems
career summary
cox communications – atlanta, ga
used spark-streaming apis to perform necessary transformations and actions on the fly for building the common learner data model which gets the data from kafka in near real-time and persists into cassandra.
experienced in performance tuning of spark applications for setting right batch interval time, correct level of parallelism and memory tuning.
involved in creating hive tables, and loading and analyzing data using hive queries.
good experience with talend open studio for designing etl jobs for processing of data.
used reporting tools like tableau to connect with hive for generating daily reports of data.
developed and executed shell scripts to automate the jobs.
wrote complex hive queries and udfs.
leading the team, we migrated and deployed multiple projects to azure cloud. i was involved in the full cycle of vendor selection, requirement gathering, design, development and the deployment of these projects. the migration included different aspects of the projects from the frontend, backend, and integration.
created custom httpmodules to take care of errors such as 404 errors.
created the acceptance test criteria.
.net developer
wrote com objects for interactions with server-side objects and used client-side scripts for client functions.
visual studio, .net framework, ado.net, asp.nethadoop engineer
have good experience in extracting and generating data visualizations.
hands on experience in working with ecosystems like hive, sqoop, mapreduce, oozie.
leadership, adaptability, self-motivation, mentoring, communication, team lead, team work
professional expereince
developed etl pipeline to process log data from kafka/hdfs sequence file and output to hive tables in orc format.
hadoop developer 	feb 2014 – july 2015
job
• tracker, task tracker, name node, data node.
• hands-on experience developing teradata pl/sql procedures and
• job workflow scheduling and monitoring tools like oozie and ganglia.
de/commissioning.
sql queries.
|maven, apache oozie, apache pig,    |pipelines, data lakes, data         |
|solr, apache airflow, apache camel, |spark                               |
|apache mesos, apache tez, apache    |apache spark, spark streaming, spark|
|project management                  |elastic.  cloud foundry, elastic    |
remotely, and data fed into the big data system along with various data
• the principal architect of a new on-site cloudera hadoop cluster.
contract cost and reduce technical debt.
• work in key areas within google cloud compute (gcp) such as dataproc
feb 2015    hadoop data architect/engineer
monthly revenue over a two-year horizon based on a combination of macro-
• migrated multiple database environments to exadata machine using
• • maintained end to end ownership for analyzed data, developed
• optimized the configuration of amazon redshift clusters, data
telecom data, shipment data, point of sale (pos), exposure and advertising
structured data files, json datasets, hive tables, and external
actions are taken on any issues found.
• problem manager for mceits, which included the creation, tracking, and
• monitored production environment using hp tools (bac, insight, omw,
• utilized vcenter to troubleshoot issues with virtual machines
outage/troubleshooting/resolution details were documented.
• ensured the end-state posture of enterprise networks is not
• provided quality-engineering consulting for the development of
technical processes, procedures, and solutions to problems
preventing the successful transition of legacy systems to
• applied technical skills and knowledge in support of activities
• worked on entry-level programming assignments.
oct 1998    computer programmer
program, analysis, design, tests and debugs databases for experimental
basic, visual studio 2008, javascript, html, classic asp, asp.net.
warrensburg, mo
small aircraft, flight instrumentation, avionic, and battery repair.
able to perform computer, network, cryptographic and switching systems
maintenance. knowledge and experience data communications, general computer
sustainment. have hand on experience in the areas of computer repairs,
kind of building an air national guard unit may need – from an improvised
emergency disaster relief shelter to a formal air guard unit facility. as a
configured cisco 3700, 3600, 2600 series routers and c6500, 4500, 3700
the swan network, satellite communication, and radio communication.
security+
provides unique adaptation to understanding business processes, needs and
requirements & competencies
strong background in natural language processing, text mining;
elasticsearch, redis
•  working knowledge of distributed processing systems, e.g. hadoop
• solid statistics knowledge e.g. hypothesis testing, anova, chi-square
sql and nosql databases, apache cassandra, apache hbase, mongodb, oracle,
hbase, apache hcatalog, apache hive, apache kafka, apache maven, apache
apache lucene, elasticsearch, elastic cloud, kibana, x-pack, apache solr,
information on farming practices, structure, and performance, ers uses
hadoop to produce data on such diverse topics as farm and rural households,
• worked with hadoop data lakes and hadoop big data ecosystem using
avro, spark streaming, spark mllib, and hadoop distributed file system
• using curator api on elasticsearch to data back up and restoring.
• performed automation engineer tasks and implemented the elk stack
modeling, data analysis, and reporting with the ability to assist in
optimization using various options like extracts, context filters,
|             |hadoop data analyst/engineer consultant                     |
|             |risk & regulatory consulting – farmington, ct               |
agile project methodology approach for hadoop big data environments.
their integration for repeated use across accounts and environments
• scheduled and executed workflows in oozie to run hive and pig jobs on
from various digital communication sources using hadoop ecosystem for
•  collected the business requirements from subject matter experts and
redshift for cloud data storage.
•  used the image files to create instances containing hadoop installed
•  tuning and operating spark and its related technologies like spark sql
etl tools in hadoop system.
•  used shell scripts to dump the data from mysql to hadoop distributed
|[pic]          |aug 2012 - dec 2013                                      |
• developed hadoop pipeline jobs to process the hadoop distributed file
vice-versa (etl) using sqoop.
risk analysis to auditors. inability to show risk analytics can result in
client and counterparty risk more precisely with credit value adjustments.
disparate data and provide the requisite power and storage to achieve
• developed, tested, and implemented financial-services application to
• enable fast and easy access to all the data sources through a high-
• involved in structuring wiki and forums for product documentation
development of oozie workflows for scheduling and orchestrating the hadoop
large complex data sets, real-time/near real-time analytics, using hadoop
experienced with major hadoop distributions like cloudera, ibm, amazon and
hands-on installing, configuring cloudera hadoop and hortonworks hadoop.
tools, and cloud platforms. able to architect, implement and lead teams in
• 5 years of experience in i.t., specialized in database, data warehouse
• real-time experience in hadoop distributed files system (cloudera,
shell scripting and linux internals.
• procedural knowledge in cleansing and analyzing data using hiveql, pig
database.
• experience in python scripting.
different applications.
based on user requirements, and sad documentation.
interactive testing tools: quality center, quick test professional
• experience in cloudera and emr hadoop distributions.
• knowledge in executing flume to load the log data from multiple
• knowledge in designing both time-driven and data-driven automated
workflows using oozie.
|apis:  rest api, spark api          |database                            |
|network architecture,               |file formats: xml, json, avro,      |
|apache solr, apache drill, presto, apache hue, sqoop, kibana, tableau,    |
• installed and configured different tools like jupyter notebook,
redshift, python libraries, spark etc.
• prepared data for consumption into tableau visualization layer.
the cloud.
• wrote many programs to generate a different type of data for each
schema for analysis.
sqoop, hive and pig jobs that extract the data on a timely manner.
• exported the analyzed data to the relational databases using sqoop for
• followed agile scrum principles in developing the project.
offloading oracle or teradata data warehouses to hadoop data lakes for
and spark clusters in aws instances to generate terabytes of data and
aug 2014    bank of america – charlotte, nc
recommended a course of action.
and jbos; daemon services and respond accordingly to any warning or
analyze life cycle of checking and debit transactions.
• rich experience of database design and hands-on experience of large
vermont technical college, williston, vy
shad
5 years hands-on experience with hadoop and cloud data solutions.
diamond resorts international in las vegas, nv.
resolved significant issues from existing environment in preparation for migration, including data cleansing and file formats and schema.  engineered various data pipelines and transformation on the new system and a well optimized system and set-up a highly efficient administration.
established data lineage for each application for data ingested.
ran webjobs on azure blobs, tables and queue services.
worked on idle timeout issues in azure cosmosdb by adding pool size.
installed kerberos secured kafka cluster with no encryption in all environments.
integrated ldap configuration for securing ambari servers and manage authorization and securing with permissions against users and groups.
designed data flow ingestion chart process.
set up a new grafana dashboard with real-time consumer lags in all environments pulling only consumer lags metrices and sending them to influx db (via a script in corntab).
implemented rack awareness in production environment.
upgraded elasticsearch from to following the rolling upgrade process by using ansible to deploy new packages in all clusters.
successfully made some visualization on kibana and deployed kibana with ansible and connected to elasticsearch cluster.
wrote unit tests to test the view returned by a controller action, to test the view data returned by a controller action, and to test whether or not one controller action redirects the user to a second controller action.
developed web services and consumed web services using wcf and visual studio 2010 with c#, retrieving data in the format of json
utilizing sql server 2008, created a database to handle complex queries, t-sql, triggers, and stored procedures.
phone: (203) 936-6943
design spark scala job to consume information from s3 buckets
define spark data schema and set up development environment inside the cluster
monitor background operations in hortonworks ambari
answerlab
cleveland, oh
ethan karlson  |  hadoop |  702-430-3575 | ethanlkarlson@gmail.com
contents
sql, spark, pig, hive, xml
analytics project manager
participated in strategic planning and responsible for dependencies, deliverables and assignment of projects across managers of diverse groups.
assisted the project manager in preparing the project charter comparing scope of project, and responsible for agile/scrum requirements gathering and planning project scope, dependencies and defining deliverables.
design workbooks and dashboards and publishing to the server and managing the user base and access for user groups.
developed tableau visualizations and dashboards using tableau desktop.
created filter actions, highlight actions & url actions to link worksheets.
comcast | philadelphia, pa	october 2013 – december 2014
business intelligence analyst
generated database design to the highest level of scalability and performance.
define measures• developing reports in business objects.
recommend analytical systems to support the business management process.
worked with drill through, drill down analysis studio reports to report studio reports such as molap- rolap.
developed lists, cross tab, chart, master detail and complex reports involving multiple prompts in report studio.
develop acceptance testing procedures and conduct acceptance testing.
detention officer
the daily goal is to maintain care, custody, and control of the maricopa county inmate population. conducting bi-hourly security checks inspecting the welfare of all individuals incarcerated within the assigned housing. the facility integrity must be examined and reported to keep the operations running. maintain accountability of all inmates via headcounts by identification bands. this ensures that no inmates have escaped or been falsely released. being able to communicate with various individuals in order to build a level of trust. using attention to detail to complete daily logs, incident reports, and various other documents that can be used in legal proceedings.
received a unit citation for returning accreditation to clinical health services
air traffic controller
arizona army national guard – phoenix, arizona	may 1997 – july 2004
(xxx) xxx-xxxx
implemented a dimensional/fact model in snowflake using sql
maintained & debugged scd type i & type ii tables in sales data model design using sql
data transformation from json to tables using snowflake & matillion
migrated a solution from flume to kafka-connect to send high volume of records.
updated kafka-hdfs connector to ingest records coming from claims.
documentation of kstreams program to scale up the solution.
worked with landoop lenses team to expand security options in hive connector.
interfaced with internal servers through putty & filezilla
this virtual medical assistant avatar integrates ai to recommend diagnoses based on patient symptoms using a smartphone.  the platform uses algorithms trained on large volumes of clinical content, such as medical protocols and chronic disease information, to interpret patient symptoms and to recommend an appropriate diagnosis. patients can describe their symptoms to the virtual medical assistant named molly, using speech, text, images and video.  the platform can be integrated with electronic health records to provide continuity of care, allowing clinicians to monitor patients outside of the clinical care setting.  the project required capture of real-time streaming voice data for analysis and response.
june 2015 – december 2016
chemical pricing strategy is complex and often relies on outdated data. with big data and analytics, the company can leverage accurate and timely price information from multiple sources, including sales information, to provide competitive and profitable pricing solutions to customers.
participated in development of big data pipelines online personalization (demand creation), supply chain and sales.  data science algorithms currently in production both directly and indirectly impact revenue.  ti transitioned business in 2009 from a digital signal processing company to an analog and embedded processing company. analog and embedded processing gathers information from the real world, converts it with an analog-to-digital converter, processes it, converts it using a digital-to-analog converter, and outputs it to the real world.  the company had several large data warehouses containing disparate data. the company has established a big data platform to make entire copies of the data sets in warehouses to provide business users with accelerated analytics capability.
marketing analysis framework
james madison university
ibm – big data
ibm – hadoop
jarod a. beardsley  |   phone  (469) 472-2894  |  email: beardsleyjarod8@gmail.commangornongfrancis@gmail.com / 513-586-4775
big data cloud developer 	10.2018 – 10.2019
responsible for enabling and extending a third-party api extension to airflow called clairvoyant api.
this web application served as both a mechanism for users to bring their own workflows as well as a file template generator that created custom airflow workflows that were then deployed to airflow to be orchestrated.
worked with kroger and enterprise cloud teams to assist with enabling and setting up gcp projects and managed services
used a helm chart (series of kubernetes .yaml files) in the deployment process.
key technologies:  hortonworks, hadoop, ibm big insight, kafka, camel, spark, spark streaming, hdfs, flume, zookeeper, rdd
hadoop cloud engineer 	06.2015 – 11.2016
trammo is involved in international commerce, trade, transportation, distribution, and marketing of fertilizer, chemicals, methanol, crude oil, liquefied petroleum gas, and petrochemicals.  big data is intrinsic to the corporate strategy, and my project focused on trade tariffs, trends and supply chain distribution.
fetched live stream data to hbase table using spark streaming and apache kafka.
data engineer  	05.2013 – 02.2014
responsible for documenting each task done.
optimized mapreduce jobs by using practitioners for 1 -to-many joins, saving execution time; designed and tested reliability of m-r jobs using unit testing in the hbase/hdfs dev/qa platforms.
developed and ran map-reduce jobs on yarn clusters to produce daily and monthly reports per requirements.
university of pittsburgh, pittsburgh, pa
rds, emr, redshift
prepared scripts to automate ingestion of data in python as needed through various sources such as api and aws s3 vendor buckets.
met with clients whenever a past project would need additional requirements.
utilized jira to handle tickets for troubleshooting as necessary.
utilized the qubole environment for verification of ingested data against known-good client data.
dell - round rock, tx
developed etl pipeline to process log data from kafka/hdfs.
accurate
kafka, and zookeeper
✓ ingested data from exadata to hadoop using sqoop.
✓ exported data from hadoop to exadata using sqoop.
testing etc.
✓ designed workflows to migrate data from hadoop to aws redshift.
✓ developed and designed spark jobs for data ingestion/aggregates.
✓ written shell script to run e2e
✓ generate messages from different sources using kafka producers in
✓ kafka messages are processed (for fraud detection analysis) using
✓ worked on analyzing hadoop cluster and different big data analytic
✓ analyzed large data sets by running hive queries and pig scripts
using hive queries
✓ developed workflow in oozie to automate the tasks of loading the
✓ involved in running hadoop jobs for processing millions of records
✓ worked on tuning the performance pig queries.
✓ experience in managing and reviewing hadoop log files.
✓ developed the test procedures and test cases using software
test input data using hp alm for functional testing, security testing
clear and precise manner, describing the scenario, expected outcome
✓ used hp alm to implement version control system and change
✓ implemented version control system and change management system for
uft scripts.
✓ created several custom reports from test management tool those were
✓ attended qa daily meetings, proposals for resolving the conflicts.
office, ms excel, unix.
good knowledge in cluster coordination services through zookeeper and kafka.
tableau, qlik view, pentaho
yarn, puppet
worked closely with smes and stakeholders to gather requirements for the current big data project.
data systems administrator/engineer	february 2013 - february 2014
infrastructure:  mapreduce, hdfs, hive, etl, pig, sql, mysql, oracle, spark, data frame, bucketing, partitioning, ssis, yableau, udf, spark context, zookeeper, sqoop, oozie, flume
•	created partitions, buckets based on state to further process using bucket based hive joins.
•	used zookeeper and oozie for coordinating the cluster and scheduling workflows.
responsible for securing network devices and protocols such as such as telnet, ssh, snmp, itl making sure they meet pci security audits and compliance.
purchased and implemented cisco asa 5516x next generation firewalls and configured acls, nat and ipec vpns, as well as cisco firepower sfrs and firesight management system with standard ids/ips policy, web filtering and advanced malware protection.
installed and administered network monitoring tools solarwinds orion, lem log and event manager, npm network performance monitor, voip network quality manager.
gps hospitality, atlanta, ga
performed daily back-ups of all servers using veritas back-up exec 9 and implemented a disaster recovery plan for all eleven servers.
summary
aws ec2 & ebs, s3
kerberos, knox gateway, apache ranger
programming languages
apache drill, presto, elk, elasticsearch, logstash, apache jelly
vpc, s3, ec2, cloudwatch, rds, lambda, emr
created hive internal and external tables and loaded the data in to tables and query data using hql
perform tuning, firewall setup, monitor and troubleshooting cluster-wide as needed.
handling hive queries using spark sql that integrate with spark environment implemented in scala.
austin, tx
march 2014 to march 2015
directly supporting and managing clustered vmware esxi \/5 with vcenter.
loaded data into the cluster from dynamically generated files using flume and from rdbms using sqoop
worked on sequence and orc files, bucketing, partitioning for hive performance and storage improvement
hadoop administrator
chicago transit authority
at cta, we want to continue to offer clean, reliable and timely transit services to our customers and we believe putting to use all our data, not just some, plays critical roles toward our goals. we collect and analyze large amounts of data from sensors and tracking devices on our buses, rail vehicles, rail tracks and other equipment, as well as when our customers interact with our fare machines. raw data from these sources could be structured, semi-structured or unstructured. hadoop distributed platform allows us to store, process and analyze huge amount of data from these disparate sources in entirety like never, as oppose to traditional database systems, where we had to choose what data to keep and what to discard. some of our use cases includes planning and demand model, predictive maintenance, event response and personalized services for our chicago card and chicago card plus loyal subscribers.
performed clusters capacity and growth planning; recommended nodes configuration for test, production and dr clusters based on business needs and workloads.
configure high availability for namenode, resource manager, hiveserver2 and metastore.
strong experience in jvm, hive, mapreduce, operating systems performance tuning.
expert in mapreduce counters tuning for faster and optimal data processing.
modified database schema as needed.
education & certifications
9alexander b. merino carreno
i am a big data engineer with 5 years of professional experience in the it industry. as an experienced big data consultant, i will ensure the successful delivery of high-quality big data solutions. i combine an understanding of the business case, a variety of skills, frameworks, best practices and coding skill.   additionally, i have a strong work ethics, the ability to work well with teams, to create the right platforms, pipelines and reporting tools for clients.
learn and adapt to perform for the cicd tool (github, jenkins) chain that is available at customer environment or proposed to be made available.
used spark to work on streaming analyzed data to hbase and make available for visualization and report generation by the bi team.
experience integrating kafka with avro for serializing and deserializing data.  expertise with kafka producer and consumer.
implemented spark and spark sql for faster testing and processing of data.
knowledgeable of deploying the application jar files into aws instances.
creation of kafka brokers in structured streaming to get structured data by schema.
skilled in hiveql, custom udfs written in hive, and optimizing hive queries, as well as writing incremental imports into hive tables.
database and data warehouse
git, github, mvc, jenkins, ci cd, jira, agile, scrum
flume, spark, kafka, hive, pig, spark streaming, sparksql, data frames, kinesis, spark, spark streaming, spark structured streaming
big data platforms
aws platform
handled large amounts of data utilizing spark.
worked with elasticsearch and logstash (elk) performance and configure tuning.
responsible for kafka operation and monitoring, and handling of messages funneled through kafka topics.
automated aws components like ec2 instances, security groups, elb, rds, lambda and iam through aws cloud formation templates.
implemented security measures aws provides, employing key concepts of aws identity and access management (iam).
aws emr to process big data across hadoop clusters of virtual servers on amazon simple storage service (s3).
launched and configured the amazon ec2 (aws) cloud servers using ami's (linux/ubuntu) and configuring the servers for specified applications.
implemented aws lambda functions to run scripts in response to events in amazon dynamo db table or s3 bucket using amazon api gateway.
united health group
wrote hive queries and optimized the hive queries with hive ql.
used ambari for maintaining heathy cluster.
big data developer	april 2014- aug 2015
worked on hortonworks hadoop distributions (hdp 2.5)
cluster coordination services through zookeeper and kafka.
moved data from spark and persist it to hdfs.
bachelor of science in computer science
spark fundamental (ibm)
nlp – natural language processing with python (udemy).time series analysis and forecasting (udemy)hadoop big data engineer
ability to troubleshoot and tune relevant programming languages like sql, java, python, scala, pig, hive, rdds, dataframes & mapreduce.
able to design elegant solutions through the use of problem statements.
experience collecting real-time log data from different sources like webserver logs and social media data from facebook and twitter using flume, and storing in hdfs for further analysis.
experience developing oozie workflows for scheduling and orchestrating the etl process.
excellent knowledge on hadoop architecture and ecosystems such as hdfs, configuration of nodes, yarn, mapreduce, sentry, spark, falcon, hbase, hive, pig, sentry, ranger.
java, c++, c#, c, python, scala, r, pig/pig latin, hive, hiveql, mapreduce, unix, shell scripting, linux, ant, yarn, spark, spark streaming, storm, kafka
amazon aws (emr, ec2, ec3, sql, s3, dynamodb, cassandra, redshift, cloud formation)
work: generate reports using python and create presentation for the top management
used spark streaming to divide streaming data into batches as an input to spark engine for batch processing.
wrote spark applications for data validation, cleansing, transformation, and custom aggregation.
imported data from disparate sources into spark rdd for processing.
minneapolis, mn
involved in creating udfs in hive like simple udf, udtf, udaf.
illinois institute of technology - great lakes institute of management
▪ performance tune hadoop data systems and pipelines with optimized
▪ worked with systems employing hive, rdds, dataframes.
analytics, and distributed big data platforms.
impala, and hortonworks.
nodes in the hadoop ecosystems.
▪ worked on disaster management with hadoop cluster.
▪ experience in mainframe data and batch migration to hadoop.
data scraping, data warehousing tools
▪ skilled in forensic methods of data cleaning and refining data.
|sql, powershell, ansible, html, php,|aws, impala                         |
|operating systems                   |zookeeper, impala. hdfs, pig, oozie,|
|linux 6-7, red hat, windows 7/10,   |apache solr, lucene, elasticsearch, |
|linux                               |kibana, cloudera impala             |
|software                            |file formats                        |
|metaspoilt, wireshark, vmware,      |file compression                    |
|cloveretl                           |analytics, hue, apache hive, apache |
|data cleansing                      |impala, apache kudu, apache oozie,  |
|database/data stores                |                                    |
|hdfs, sql, plsql, psql/ga,          |                                    |
redbox – chicago, il
pipeline for customer data.
• led poc involving confluence api call to populate wiki with log data
other:  jdk,  maven,  streamsets,  airflow,  confluence,  jira,  pyspark,
senior hadoop data engineer       january 2018- october 2018
• implemented cloudera impala for faster data analysis.
• performed data cleansing and data analytics in hadoop hdgs.
• automated workflow tasks using apache oozie.
• implemented yarn resource pools to share resources of the cluster for
correct serialization & memory tuning.
• created hive generic udf's to process business logic that varies based
• imported data into hdfs and hive using sqoop and kafka. created kafka
topics and distributed to different consumer applications.
• worked on spark sql and dataframes for faster execution of hive
processing of data.
• connected various data centers and transferred data between them using
• extracted the data from rdbms (oracle, mysql) to hdfs using sqoop.
• used nosql databases like mongodb in implementation and integration.
team.
the data.
• worked on installing the cluster, commissioning and decommissioning of
regular jobs like file transfers between different hosts.
• involved in production support, which involved monitoring server and
identifying information to be transferred and processed outside of
future.
gathering.
• configured and managed dlp policies for cloudlock, netskope, and
• quarterly scheduled audits allowed the company to stay compliant with
client contracts and avoid any legal/revenue damages.
party software.
• internal forensic, incident response, and threat hunting using sans
• automated smoke testing for application deployments.
• writing and implementing sql, bash, and powershell scripts for
• team lead for infrastructure itil reporting and documentation.
• linked technology issue ticket creation to knowledge base which
forty-two percent.
• tested, configured, and deployed new monitoring solutions for 2000+
servers, 300 applications, and our internal network using bash, sql,
for transaction analysis, code debugging, and incident management.
• application logging development for elk stack with python for
integrated network monitoring into one dashboard.
• provided application support and guidance which included service
hadoop big data
kevin lanni
expertise in monitoring with elk stack (elasticsearch, logstash and kibana)
experience with real-time streaming data pipelines with kafka, spark streaming and hbase
knowledgeable of cloud security using kerberos and ranger.
administration using oozie workflows and zookeeper.
strong advocate of agile methodologies
may 2019 – present
ai ml big data engineer
worked on collecting and transforming internal and external data from multiple sources for both business reporting and the training of ai models.
created and maintained various infrastructure using cloudformation
helped to mentor more junior developers in learning the aws technology stack.
performed data exploration and metadata collection for the purposes of identifying useful information for financial ai models.
created hive queries to spot emerging trends by comparing hadoop data with historical metrics.
participated into the aws architecture, design and planning from ingestion into reporting.
securely controlling aws users and groups access to aws services and resources by assigning roles and polices using iam.
end-to-end data analytics solutions and support using hadoop systems.
designed and implemented security of hadoop cluster using kerberos.
familiar with hive's analytical functions, extending hive core functionality by writing custom udfs.
etl from databases such as sql server and oracle11g to hadoop hdfs in data lake.
s3, redshift, hadoop hdfs, apache cassandra, apache hbase, hive ql, spark
query language █████████
kibana, tableau, powerbi, excel
the coca-cola company 	may 2018- present
implemented applications on hadoop/spark on kerberos secured cluster.
created complex infrastructure of the pipeline using aws cloud formation.
import/export data from mysql and oracle into hdfs and hive using sqoop.
loaded data into hbase tables and hive tables for consumption purposes.
involved in implementing security on hdp hadoop clusters with kerberos for authentication and ranger for authorization and ldap integration for ambari and ranger
built a scalable, fault-tolerant processing using spark streaming and spark sql.
creating hive scripts for etl, creating hive tables, writing hivequeries.
spark sql for joining multiple hive tables and write them to a final hive table and stored them on s3.
pyspark
aws rds
aws iam
cloudera
cloudera manager
shell script language
hadoop admin
worked with product teams to optimize reads for cassandra from spark
download data through sqoop and hive in hdfs platform
worked with amazon aws iam console to create custom users and groups.
migrated on prom rdbms to aws rds and implemented aws lambda to process some of data to s3.
experienced with installation of aws cli to control various aws services through shell/bash scripting.
used kafka producer to ingest the raw data into kafka topics run the spark streaming app to process event-driven data.
email:  frank.mukendi888@gmail.com
senior data engineer
understanding of distributed systems, hdfs architecture, internal working details of mapreduce and spark processing frameworks.
understanding of big data concepts and use of cloud technologies and tools.
hands-on expertise in hadoop components - hdfs, mapreduce, hive, impala, pig, flume, sqoop and hbase.
reactjs/redux, html5/css3test automation.
amazon stack
file compression
software
use aws sagemaker to write models for predicting the probability of default on credit card debts; usage of aws sagemaker to train those models.
implemented many impala scripts and shell scripts for data validation and data analytics.
real-time data indexing using aws sqs messaging service.
fetching the live stream data from db2 to hbase table using spark streaming and apache kafka.
oozie scheduler to automate the tasks of loading the data into hdfs and pre-processing with pig.
cloud watch monitor for s3 & glacier storage management, access control and policy.
used impala where possible to achieve faster results compared to hive during data analysis.
designed jobs using db2 udb, odbc,.net, join, merge, lookup, remove duplicate, copy, filter, funnel, dataset, lookup file set, change data capture, modify, row merger, aggregator and peek, row generator stages.
cloud formation scripting, security and resources automation.
worked with various compression techniques to save data and optimize data transfer over network using lzo, snappy, etc.
big data developer	september 2014 - january 2016
data developer	august 2013 – september 2014
imported data using sqoop to load data from mysql to hdfs on regular basis.
sr. information technology engineer with more than 5 years of experience, specialized in hadoop ecosystem & big data frameworks.
experienced in ansible, jenkins, and pyspark.
experience with hadoop big data infrastructure for batch data processing and real-time data processing.
ability to manage competing priorities in a complex environment and maintain high productivity.
amazon aws, microsoft azure, anaconda cloud, elasticsearch, solr, lucene, cloudera, databricks, hortonworks
sept 2018 to present
boeing co.
created multithreading streaming application to migrate from disparate sources to a datalake.
caterpillar
custom kafka broker design to reduce message retention from default 7 day retention to 30 minute retention - architected a light weight kafka broker
automated all the jobs for pulling data from hdfs to load data into hive tables, using oozie workflows.
big data/hadoop engineer
created modules using spark streaming in data into data lake using storm and spark.
wells fargo insurance services
involved in transforming data from legacy tables to hdfs and hbase tables using sqoop.
performed code reviews and utilized gitflow for branching and collaboration.
developed application to mine semi-structured json data from restful web-service into mongodb.
certification
cognitive class
4dirk sorkin
good knowledge on spark framework on both batch and real-time data processing.
capable of building data tools to optimize utilization of data, and configure end-to-end systems.
aaws lambda, aws s3, aws rds, aws emr,
apache airflow & camel
poc, architectural planning, hadoop cycle,
log analytics
elk stack or elastic stack
also: maven, ant, flume, apache airflows
s3, hdfs, dhw, datalake
data stores
july 2018-present
the sre - system reliability engineering. this team was responsible for processing all credit card applications - capital one credit card and all third-party brands capital one handles. i was responsible of the development of dashboards to monitor the health of the 20+ apis for the upper management and developers. technology used was kibana, elk, elastic search, data dog. most of the dashboards were created in grafana. kibana dashboards were created when limitations on the grafana were encountered. also created dashboards using new relic insights and data dog.
automated backups of the grafana dashboards using selenium and linux bash shell to download the .json files from grafana.
have worked on jenkins and git tools.
worked with stakeholders and sales and marketing management to gather requirements and determine needs.
documented findings including current environment and technologies, in addition to anticipated use cases.
it neer, inc.,
aggregation, queries and writing data back to oltp system directly or through sqoop.
used hbase to store majority of data which needed to be divided based on region.
december 2012
worked with the client to reduce churn rate, read and translate data from social media websites.
scripts.
system administrator
liaison between staff at all levels of a client organization
resolved open trouble tickets using salesforce.com
utilized ms sql to troubleshoot and resolve customer reporting issues
december 2003-
developed and implemented component testing scripts.
coded the design after approval from design.
certificate in software development life cycle in big data and business intelligence (sdlc-bd & bi)
ad hoc training
architecting data analytics systems and data processing pipelines to
aws, azure, cloudera, hortonworks.
platforms and technologies.
• specializing in big data platform design and implementation and
• effective in hdfs, yarn, pig, hive, impala, sqoop, hbase, cloudera.
• experience in importing and exporting data using sqoop and sftp for
• experience in implementing user defined functions for pig and hive.
• expertise in preparing the test cases, documenting and performing unit
hadoop big data components
web technologies & apis
dynamodb
data storage and files
hadoop, cloudera hadoop, cloudera impala, hortonworks
and camel, apache apache hue, sqoop, kibana, tableau, aws, cloud foundry,
may 2016    hadoop data architect/engineer
change data capture, error handling, restart and refresh strategies.
• worked with different feeds data like json, csv, xml, dat and
measured facts.
customer analytics projects.
other transformations for every data
• • architecture and hands-on production implementation of the big data
• marketing using telecom data, shipment data, point of sale (pos),
• spark sql is used as a part of apache spark big data framework for
• modeled hive partitions extensively for data separation and faster
• caching of rdds for better performance and performing actions on each
which indeed partitioned by iri time dimension key, retailer name,
architected and implemented a cloud platform for data storage and analytics
• worked on developing user defined functions (udfs) in hive to
• involved in writing pig scripts for cleansing the data and implemented
• benchmarked hadoop and spark cluster on a terasort application in aws.
• wrote spark to run a sorting application on the data stored aws.
• implemented images conversion and hosting on a static website using s3
• optimized amazon redshift clusters, apache hadoop clusters, data
• devised etl functions between oracle and amazon redshift.
• involved in collecting, aggregating and moving data from servers to
• experienced in collecting the real-time data from kafka using spark
faster processing of data.
• generated and published reports regarding various predictive analysis
• extensively worked on impala to compare processing time of impala with
apache hive for batch applications to implement the former in project.
hdfs and pre-processing with pig and hive.
• involved in designing web interfaces using html/ jsp as per user
teradata data mart
bachelor of science in interdisciplinary studies/business health
dubois
incremental imports, partitioning and bucketing concepts in hive and spark sql needed for optimization.
harris corporation – columbia, md
collect, aggregate, and move data from servers to hdfs using apache spark & spark streaming.
may 2015 	may 2016
zurich n.a. uses hadoop for big data analytics to query customer activity in real time.  using advanced analytics on data streams like advanced windowing, event correlation, event clustering, anomaly detection, and so on, play a large role in the financial giant's corporate strategy and administration of accounts, products, investments, and customers.
used spark api over hadoop yarn to perform analytics on data in hive. imported data into hdfs and hive using sqoop and kafka. created kafka topics and distributed to different consumer applications.
created hive generic udf's to process business logic that varies based on policy.
environment: hadoop, hdfs, hive, spark, yarn, kafka, pig, mongodb, sqoop, storm, cloudera, impala
configured oozie workflow engine scheduler to run multiple hive, sqoop and pig jobs.
used oozie scheduler system to automate the pipeline workflow and orchestrate extraction of data.
used sqoop to efficiently transfer data between databases and hdfs and used flume to stream the log data from servers.
nov 2000	feb 2003
modeling and simulation: performed orbit analysis and modeled communications paths both bent pipe and multi hop.
5 years’ experience in hadoop big data and 5 years’ experience in i.t.
seasoned experience in project management (agile/scrum, waterfall and various agile processes).
worked with data lakes and big data ecosystems (hadoop, spark, hortonworks, cloudera)
used to working in a production environment, managing migrations, installations, and development.
programming languages:
git, github
apache camel, flume, kafka, talend, pentaho, sqoop
aws lambda, aws s3, aws rds, aws emr, aws redshift, aws s3, aws lambda, aws kinesis, aws elk, aws cloud formation, aws iam
data query:
spark sql, data frames
install and configure kafka cluster and monitoring the cluster; architected a light weight kafka broker; integration of kafka with spark for real time data processing.
handled structured data via spark sql then stored into hive tables for downstream consumption.
accessed hadoop file system (hdfs) using spark and managed data in hadoop data lakes with spark.
integrated kafka with spark streaming for real time data processing
rds, cloud formation, aws iam and security group in public and private subnets in vpc.
created hive tables to store the processed results in a tabular format.
transfered data between a hadoop ecosystem and structured data storage in a rdbms such as mysql using sqoop.
5+ years of experience working in it with 5 years specialized in big data.
provide actionable recommendations to meet hadoop data analytical needs on a continuous basis using hadoop distributed system and cloud systems.
displays of analytics and insights using data visualization tools, tableau, and hadoop tools to generate reports and dashboards to drive key business decisions.
database partitioning, database optimization, building communication channels between structured and unstructured databases.
used spark-sql and hive query language (hql) for getting customer insights, to be used for critical decision making by business users.
fort worth, tx
detroit, mi
manipulated and analyzed complex, high volume, and high dimensional data in aws using various querying tools.
architecting and devops for aws services including in house data center for middleware system and web services. also, managing security review and web compliance management.
fmc corporation
stackdriver monitoring setup to collects metrics, events, and metadata.
bachelor of arts in french and francophone studies
phone:  (703) 659-4419  |   email:  wsmclemore1@gmail.com
knowledge of incremental imports, partitioning, bucketing in hive and spark sql for optimization.
experience in mainframe data and batch migration to hadoop.
extensively used apache flume to collect logs and error messages across the cluster.
agile, kanban, scrum, devops, lean, six sigma
apache ant, apache cassandra, apache flume, apache hadoop, apache hadoop yarn, apache hbase, apache hcatalog, apache hive, apache kafka, apache maven, apache oozie, apache pig, apache spark, spark streaming, spark mllib, graphx, scipy, pandas, rdds, dataframes, datasets, mesos, apache tez, apache zookeeper, cloudera impala, hdfs, hortonworks, apache airflow and camel, apache lucene, kibana, x-pack, apache solr, apache drill, presto, apache hue, sqoop, kibana, tableau, aws, cloud foundry
implemented credit policies for upmarket card segment using drools and java spring. verified policies end-to-end in inner-sourced credit policy knowledge engine application.
contributed to inner-sourced cross-team applications via enterprise github pull requests, increasing stability and performance.
technologies: kie/drools, java spring, apache maven, vba, cucumber, jenkins, amazon dynamodb, postgresql, artifactory, splunk, pagerduty
developed and maintained application which calculates interchange between visa, mastercard, and capital one, grossing hundreds of millions of dollars in revenue. leveraged cloud technologies and devops toolchain to build scalable, performant architecture.
•	served as lead engineer to team of six engineers (3 senior, 3 junior).  worked on-site working across several teams with senior and junior engineers
•	deployed containerized applications using docker, allowing for standardized service infrastructure.
•	led disaster recovery and incident management efforts to minimize downtime and loss of revenue.
updated onboarding process and documentation for new hire engineers, shortening the onboarding process from two weeks to two days.
served as liaison between big data team, java development team and web development teams, resolving conflicts and creating and assigning user stories in jira.
trained analysts on business-driven development and test-driven development methodologies, including a primer on cucumber and gherkin.
designed and diagrammed data models which conformed raw data to a dimensional model to facilitate etl processes in ibm datastage and ibm netezza.
implemented yarn resource pools to share cluster resources for yarn jobs submitted by users.
configured spark streaming to receive real time data from kafka and store the stream data to hdfs.
the city of long beach, california is using smart water meters to detect illegal watering in real time and have been used to help some homeowners cut their water usage by as much as 80 percent. that's vital when the state is going through its worst drought in recorded history and the governor has enacted the first-ever state-wide water restrictions.
built continuous spark streaming etl pipeline with spark, kafka, scala, hdfs and mongodb.
design and develop etl workflows using python and scala for processing data in hdfs & mongodb.
involved in converting hive/sql queries into spark transformations using spark rdds, python and scala.
gulfstream – savannah, ga
offloading oracle or teradata data warehouses to hadoop data lakes for better scaling, more analytics and cost savings.  created multi-node hadoop and spark clusters in aws instances to generate terabytes of data and stored it in aws hdfs.
load and transform large sets of structured, semi structured and unstructured data.
alibaba works with buyers and suppliers through its web portal. apache storm provides it the feature to take into consideration the purchases that are being made during the day while recommending products to users. this plays a key role on special days (holidays) when the activity is unusually high. this is an example where efficient stream processing plays over batch processing
bi developer
software engineer
expertise in preparing the test cases, documenting and performing unit testing and integration testing.
work experience with cloud infrastructure like amazon web services.
experience in using kafka as a messaging system to implement real-time streaming solutions using spark streaming.
apache ant, apache flume, apache hadoop, apache yarn, apache hive, apache kafka, apache maven, apache oozie, apache pig, apache spark, apache tez, apache zookeeper, cloudera impala, hdfs
data visualization tools
pentaho, qlikview, tableau
continuous integration
apache hbase
worked with spark to create structured data from the pool of unstructured data received.
developed spark scripts by using scala shell commands as per the requirement and used pyspark for proof of concept.
used aws cloud formation to ensure successful deployment of database templates. automated cloud deployments using chef, python (boto and fabric), ruby, scripting and aws cloud formation templates.
cloudera manager used to collect metrics
involved in cluster level security, security of perimeter (authentication- cloudera manager, active directory, kerberos/ranger) access (authorization and permissions- sentry) visibility (audit and lineage - navigator) data (data encryption at rest).
implemented capacity schedulers on the yarn resource manager to share the resources of the cluster for the map reduce jobs given by the users.
• hands on pig latin scripts, grunt shells and job scheduling with oozie.
enterprise hadoop.
database system and vice-versa according to client's requirement.
• extend hive and pig core functionality w/custom udf and udt.
programming languages & ides  [pic][pic][pic][pic][pic]
apache cassandra, apache hbase, mapr-db, mongodb, oracle, sql server, db2,
sybase, rdbms, mapreduce, hdfs, parquet, avro, json, snappy, gzip, das,
powerbi, tableau, etl tools, kibana
skills [pic][pic]
tez, apache zookeeper, cloudera impala, hdfs, hortonworks, mapr, mapreduce,
apache airflow and camel, apache lucene, elasticsearch, elastic cloud,
worked on big data analytics systems using hadoop, spark, storm, hive,
analytics consulting company.  clients include big name retail such as home
• worked closely with sme to prepare a tool using map reduce to maintain
scd2 mapper.
storm and kafka to get real time streaming of data into hbase.
• installed ambari on existing hadoop cluster.
• implemented nifi flow topologies to perform cleansing operations
• utilize the give oltp data models for source systems to design a star
and inserts of the records.
• worked closely with the app support team in production deployment and
spring 3.0
processing using kafka.
• extended udfs in pig library piggybank to learn n-gram language model
for parts of speech and corresponding tf-idf models and inverted
to spark engine for batch processing.
interactive querying.
and flume.
• designing and creating hive external tables using shared meta-store
procedures. created alerts for successful or unsuccessful completion
environment: cloudera distribution cdh 5.5.1, oracle 12c, hdfs, map reduce,
and data analysis of financial transactions, investments and risk analysis.
• developed several advanced yarn programs to process received data
• responsible for building scalable distributed data solutions using
• handled importing of data from various data sources, performed
• involved in hdfs maintenance and loading of structured and
• developed workflow in oozie to automate the tasks of loading the data
• data was formatted using hive queries and stored on hdfs
• responsible for managing and reviewing hadoop log files. designed and
• cluster maintenance as well as creation and removal of nodes using
• followed agile methodology, interacted directly with the client
• point of contact to the client for all technical aspects. prepared
beans.
• developed reusable services using bpel to transfer data. participated
and read the logs.
• implemented log4j for logging purpose in the application. track new
change request, analyze requirement and design solutions as part of
email: vicenteayala1888@gmail.com
antonio  saldivar  lezama
universidad popular autónoma del estado de puebla
• microsoft certified solution associate windows server 2012able to work with existing eds platforms and strategic initiatives that are built for future phases of eds/ebi.
worked with apache spark to provide fast engine for large data processing integrated with functional programming languages scala, python, and scripting in hive ql and pig latin.
skilled in architecture of big data systems using:
cloudera hadoop, cloudera impala, hortonworks hadoop, mapr, spark, spark streaming, hive, kafka, nifi, kinesis
data pipeline architecture and construction
skilled in database and data management
big data architect
evaluated the solution and implemented enhancements to support two batch processes within the same emr cluster.
spark data frames joined from multiple sources, and managed metadata of the schemas.
designed and deployed custom solution for team collaboration using github web hooks to deploy jenkins pipelines working in multiple environments
supported multiple teams to design solutions and remediate security compliance, cost efficiency and optimize the platforms.
paladion networks, reston, va
flink cep
apache phoenix
hadoop yarn
pyhton
october 2016 – april 2018
use of spark, python, hive, pig in constructing pipelines and queries.
poc
memory tuning
spark api
ac nielsen, oldsmar, fl
involved in converting hiveql/sql queries into spark transformations.
partitioning
batch processing
optimized hive analytics, sql queries, created tables, views, wrote custom udfs, and hive-based exception processing.
set-up qa environment and updated configurations for implementing scripts with pig.
built and configured a virtual data center in the aws cloud to support enterprise data.
developed end-to-end hive queries to parse the raw data,
designed the spark streaming and kafka producer interfaces - for multithreaded partitions
skills
 created lambda to process the data from s3 to spark for structured streaming to get structured data by schema.
implemented serverless architecture using aws lambda with amazon s3 and amazon dynamo db.
led many critical on-prem data migration to aws cloud, assisting the performance tuning and providing successful path towards redshift cluster and aws rds db engines.
worked on aws s3 bucket integration for application and development projects.
experience in managing and reviewing hadoop log files in aws s3.
designed and developed etl jobs to extract data from salesforce replica and load it in data mart in amazon redshift.
spark and spark sql for faster testing and processing.
performed upgrades, patches and bug fixes in hdp in a cluster environment.
defined data security standards and procedures in hadoop using apache ranger and kerberos.
implemented enterprise security measures on big data products including hdfs encryption/apache ranger.
flume and hiveql scripts to extract, transform, and load the data into database.
accessed hadoop file system (hdfs) using spark and managed data in hadoop data lakes.
hands on experience in etl, data integration and migration and extensively used etl methodology for supporting data extraction, transformations and loading
sql, hiveql, impala
data modeling
apache cassandra, apache cassandra, datastax cassandra, apache hbase, couchbase, db2, mysql, postresql, dynamodb, auroradb, arangodb, simpledb, cosmosdb, amazonrds
implementation of data lake on s3 ,aws and cloud service: batch processing and real time processing.
used kafka producer to ingest the raw data into kafka topics run the spark streaming app.
worked on various file formats avro, orc, text, csv, and parquet using snappy compression.
stratford university
bachelor’s of healthcare administration and policy
experienced in ansible, jenkins, and pyspark, and hadoop streaming applications with spark streaming and kafka.
apache cassandra, aws redshift, amazon rds, apache hbase, sql, nosql, elasticsearch, hdfs, data lake, data warehouse, database, teradata, sql server
spark and kafka
worked with apache spark streaming api on big data distributions in an active cluster environment.
worked on cloudera distributions and configured, installed and managed distributions is a multi-cluster environment.
mapped to hbase tables and implemented sql queries to retrieve data.
familiarity with the entire hadoop ecosystem including, sql, scala, pig, hive, rdds, dataframes.
█   █   █   █   █   █   █   professional skills   █   █   █   █   █   █   █
thirdlove – san francisco, ca
wells fargo extensively redesigned its website; launched a service, now with 1.5 million users, that allows check deposits to be made through smart phones; and added the surepay service, which allows person-to-person payments nationwide through mobile devices using an email address or mobile phone number.
the bank also set-up a big data lab to pioneer the use of emerging technology and data science to drive customer experience, prevent fraud and develop customer insights. they launched a wells fargo app for use on ipad and android tablets, providing access to wells fargo accounts, transfers, payments, mobile deposit, and brokerage access and trading. two weeks after the launch, nearly 200,000 customers downloaded the ipad app, and more than one million downloads have been incurred since. access to wells fargo advisors accounts has been added, as has the ability to trade via mobile device.
setup internal and external load balance for application and manage cloud dns setup
ndices setup, manage template & configuration. indexing data from community, sql & no-sql database, cms tool like aem & teamsite.
used apache flume and kafka for collecting, aggregating data, moving from various sources.
built and configured a virtual environment in the aws cloud to support enterprise data.
developed data queries using hiveql and optimized the hive queries.
created structured data from the pool of unstructured data using spark.
scala, python, sql, hive ql, shell scripting, c#, java, python, sql, vb.net, mysql, vba, html, css, javascript
kibana, tableau
mass mutual
implemented spark and spark sql for faster testing and processing.
integrated kafka with spark for real time data processing.
documented infrastructure design for elk clusters.
experience in managing and reviewing hadoop log files stored in aws s3 buckets.
june 2014 – sept 2015
installation of cloudera hadoop clusters on aws using cloudera manager (cdh3, cdh4 & cdh5).
set-up hortonworks infrastructure from configuring clusters to node security using kerberos.
staff sergeant
various locations
bachelor of science degree - information technology
page 8
rafael loustaunau
senior hadoop engineer designer
years of experience
5 years information technology
wells fargo – aws, spark, scala
proven experience showcasing technical and operational feasibility of hadoop developer solutions.
developed scalable and reliable data solutions to move data across systems from multiple sources in real time as well as batch modes.
hadoop cloud platforms
used spark api over hortonworks hadoop yarn to perform analytics on data in hive.
analyzed the sql scripts and designed the solution to implement using pyspark.
streaming data is another type of data found in the marine and offshore industries. streaming data refers to a sequence of message-oriented  data  in -sequence transport used to transmit or  receive information in a real-time application among the networks.
write code that will take input as log files and parse the logs and structure them in tabular format to facilitate effective querying on the log data.
developed complex streaming jobs using java language that are implemented using hive and pig.
developed data pipeline using flume, sqoop, pig and to ingest customer behavioral data and purchase histories into hdfs for analysis.
implemented flume to import streaming data logs and aggregating the data to hdfs.
environment: linux, hadoop, big insights, hive, puppet, java, c++.derek yang
significant contribution to the development of big data roadmaps.
c++, java, php, python, scala, html/xhtml/css, sql, hive, spark
file management
microsoft office suite
virtualization & network
migrated cluster/data from one datacenter to another
integrated bi tools (spotfire, crystal reports, lumira, tableau) with hadoop
we moved the company data infrastructure to the cloud to be more effective in terms of cost and operation. we used hortonworks hadoop on aws cloud for the analytics platform to support rapid development and launch of new products and services.
implemented amazon elastic compute cloud (amazon ec2) for scalable compute capacity.
transformed log data into data model using pig and wroteudf functions to format the logs data.
used oozie workflows and coordinators for integrating mapreduce workflow- including java rest service consumption and mongodb/neo4j ingress, and scheduling the data flow pipeline.
university of charlotte north carolina at charlotte
concentration: cyber security
caldwell community college & technical institute, hudson, nc
project summary
foot locker // new york, ny
worked in a databricks environment on aws using spark.
senior big data engineer	august 2017 – february 2019
home depot, atlanta, ga
installed and configured apache hadoop and hive environment on the prototype server.
loaded unstructured data into hadoop file system (hdfs).
worked on analyzing hadoop cluster and different big data analytic tools including mapreduce, hive,  hdfs, spark, kafka and apache nifi.
installed, configured and deployed a 30 node cloudera hadoop cluster for development and production
developed multiple mapreduce jobs in java for data cleaning and preprocessing.
phone: (402) 218-2731    |    email: chris.tran430@gmail.com
ad hoc classes in computer science
certification & training
highly motivated data engineer skilled in architecture and implementation of scalable, high availability systems for processing and analytics of very large volume structured and unstructured data.  skilled in the development of custom etl pipelines, query functions, and reporting dashboards for use by data science, data analysts, cross-functional teams and stakeholders.
architecture of elegant hadoop analytics solutions through data modelling, etl pipelines and tools to extract useful and meaningful data from hadoop distributed systems (hdfs).
identifies key initiatives for analytical decisions using various tools such as spark
ide: eclipse, intellij, pycharm
rdbms: oracle, sql, mysql
etl tools:  flume, kafka
desktop software:  microsoft office suite, adobe creative suite
did a poc for predix.io for consideration of future use.
environment: hadoop, cloudera, kibana, mapreduce, hdfs, hive, impala, python, sql, sqoop, kafka, flume
used spark api over yarn to perform analytics on data in hive
responsible for scheduling jobs, alerting and maintaining packages.
summary                                                     +
experience                    +
apache hadoop
data processing (compute) engines
apache spark
created orc tables from text input format tables.
created and modified python and unix scripts.
troubleshooting of oozie workflows, hive queries, and spark jobs.
created yarn queues for each customer and configuring capacity for each queue.
210-960-3326  |  pierrejosh647@gmail.com
moved data from hortonworks cluster to aws emr cluster.
highly available, scalable and fault tolerant systems using amazon web services (aws).
discovery communication, inc.
ingest raw data from s3, process using spark sql and load to oracle db, redshift tables
author airflow dag
involved in converting hive/sql queries into spark transformations using spark rdd and data frame
worked with both batch and real-time processing w/ spark frameworks.
round rock, tx
sept 2016 – feb 2018
aws cloud formation was used to create templates for database development.
mar 2015 – sept 2016
setup the python scripts to create the snapshots on aws s3 buckets and delete the old snapshots.
worked on manage policies for s3 buckets and glacier for storage and backup on aws.
spark api over cloudera hadoop yarn to perform analytics on data in hive.
metropolitan state university
about me
develop and automate reports, and develop analytical dashboards to provide insights at scale.
unix shell scripting, object-oriented design, object-oriented programming, functional programming, ftp,  sql, java,  hiveql, hive, mapreduce, python, scala, html, css, javascript, jquery, xml, blueprint xml, ajax, typescript, rest api, spark api, json, avro, parquet, orc
big data frameworks & tools
hdfs, cassandra, hbase, mongodb, mysql, sql, parquet, orc, avro, json, snappy, gzip
hard skills
linux
skills, tools, software
microsoft visio
usage of dataframes, rdds along with sparksql and sparkcore
leveraged unix shell scripts for the pipeline of projects.
documented unit test of ingestion process using pytest and unittest.
integrated the hive warehouse with hbase
exported the analyzed data to the relational databases using sqoop for virtualization and to generate reports for the bi team.
sincere
interested, engaged
continually learning
etl from databases such as sql server […] oracle11g to hadoop hdfs in data lake.
experience working with various tools.
project management
develop automation and processes to enable teams to deploy, manage, configure, scale, monitor applications in data centers and in aws cloud.
worked on aws to create, manage ec2 instances and hadoop clusters. involved in connecting pentaho 7.0 to target database to get data.
indianapolis, in
wrote database objects like stored procedures, triggers for oracle, ms sql
accountable for data updates into patient health information database for hippa regulatory compliance by logging various aspects of client treatment interventions programs using secure health information management access terminal;
migration of systems data from different databases and platforms to ms-sql databases at a leading regional research institution;
research database administration including installation, configuration, upgrades, capacity planning, performance tuning, backup and recovery in a high-transaction and fast pace environment.
bachelor of arts in communication
proven track record of increasing responsibilities, turning around low performing teams & enhancing operational processes. strong analytical & problem-solving skills
hadoop framework, hadoop distributed file system and parallel processing implementation.
skilled in leadership, team lead, mentoring, time management, budget and resource allocation
flume, apache storm, apache spark, nifi, apache kafka, talend, elk
debug and solve issues with hadoop as on-the-ground subject matter expert. this could include everything from patching components to post-mortem analysis of errors.
experience in importing and exporting data into hdfs and hive using sqoop.
experience in working with various kinds of data sources such as mongodb and oracle.
performed data scrubbing and processing with oozie and spark.
devised and lead the implementation of the next generation architecture for more efficient data ingestion and processing.
experience in managing and reviewing hadoop log files.
created and maintained technical documentation for launching cloudera hadoop clusters and for executing
planning, and slots configuration.
database developer
humberto acosta
experience summary
programming /scripting
application servers
jenkins, hudson, travis
worked on escalated tasks related to interconnectivity issues and complex cloud-based identity management and user authentication, service interruptions with virtual machines (their host nodes) and associated virtual storage (blobs, tables, queues).
extensively used transformations like router, aggregator, normalizer, filter, join, expression, source qualifier, unconnected and connected lookup, update strategy and store procedure, xml transformations along with error handling and performance tuning.
extensively worked on performance optimization of hive queries by using map-side join, parallel execution and cost-based optimization.
kafka consumers used with kafka brokers to facilitate publish/subscribe function.
completed tasks and project on time, per project requirements and quality goals.
implementation of data lake on s3, aws and cloud service: batch processing and real time processing.
cognitive classsenior data engineer
financial fraud prevention
software developer	march 2012 – february 2013
remained current and up-to-date with technology, hardware, software, trends, and new releases.
houston community college; houston, tx
a.a in computer science
phone: (999) 999-9999
python, unix shell scripting
i.t. in data analysis
(999) 999-9999
extending hive functionality by using user defined function's (udf), user defined table-generating functions (udtf) and user defined aggregating functions (udaf) for hive.
constructed a kafka broker with proper configurations for the needs of the organization in using big data.
6 years big data architecture and spark development
storage with nosql databases such as cassandra, mongodb, hbase, arangodb, redis, couchdb
running modern data pipe on cloud using tools like ec2, emr, s3, kinesis, redshift dynamodb, rds, athena, aurora, snowball, glue
experience in implementing security with apache ranger, apache sentry, authentication using kerberos and encryption using ssl certificates
apache hadoop, hdfs, apache spark, mapreduce, apache pig, hive, sqoop, hive, spark, storm, kafka, pig, hbase, sqoop
hands on experience on fetching the live stream data from rest api to kafka cluster
architected pipeline ingestion from kafka, integrating kafka with spark streaming for high speed data processing and enrichment
hicksville, ny
slomin's has engaged third-party data analytic providers to act on slomin's behalf to track and analyze usage of the myshield app. while providers brought the data scientists, i was contracted to slomin’s to architect an hadoop environment on aws and construct pipelines per the specs provided by the analysts based on their needs.
successfully loaded files to hive and hdfs from oracle, sql server using sqoop.
new delhi, india
01/2012-06/2012
(gpa: 3.575/4.0)
awarded academic scholarship for the year 2016 by university of bridgeport, ct, usa
training in apache hadoop with learning in apache spark, map reduce programing, apache pig, hive,sqoop, hbase.
hdfs, data lake, data warehouse, database, postgresql
sql, spark sql, hive ql, cql
built a model of the data processing by using the pyspark programs for proof of concept.
hadoop ecosystem software & tools
data engineer		april, 2017 - present
t-mobile usa		bellevue, wa
us express enterprises		chattanooga, tn
presented findings and recommendations at the end of internship to multiple levels of leadership
i am a developer in one of the multiple teams that develop credit policies. a credit policy in capital one is a set of rules that determines if a credit card application is accepted or rejected. data is collected in an online application form and rules engine, java-coded web service processes and evaluates the information collected according to the product selected by the applicant.
every rule is tested in a local environment using cucumber. every policy requested to the team is developed by 3 engineers using a branch in git, which is merged and submitted to jenkins ci for qa testing using a web interface to match an existing product with the recently developed policy.
all the rulesets are coded in drools, however, their relationship with a policy and a product is stored in a postgresql relational database. that way the rules engine can decide what rules need to be evaluated when applying for a product. we also write the sql scripts with dml statements to match a product with its rules.
participated in reinforcing the security of aws services in order to maintain the security policy and to update the latest stable version of implementing technologies.
internal security provided by sphinx.
jan 2014	hadoop data engineer
environment:  hadoop, spark, hdf, oozie, sqoop, mongodb, hive, pig, storm, kafka, mapreduce, sql, acro, rdd. sqs s3, cloud, mysql, informatica, dynamo db
dec 2013	alibaba – remote
hands-on experience of sun one application server, web logic application server, web sphere application server, web sphere portal server, and j2ee application deployment technology.
aggregate and process data for real-time analytics.  aws, cloudera,
• in-depth understanding/knowledge of hadoop architecture and various
application master and execution of a mapreduce job.
servers and social media- tweets) using flume and storing in hdfs to
• experience in installation, configuration, supporting and managing -
• expertise in developing pig latin scripts and hive query language for
perform actions on your data.
apache storm, apache hive, apache cassandra, apache hadoop, apache
apache tez, apache zookeeper, x-pack
the weather channel uses distributed data processing with apache storm
• used apache storm to power real-time data analytics and predictive
and as for storage.
• implemented high availability of name node, resource manager on the
data modeling.
environment: java, j2ee, spring, hdfs, spark, storm, sqoop, kafka,
pig, hive, oozie, etl, hbase, zookeeper, aws, hadoop, oracle, manager,
ambari, oracle, mysql, cassandra, mongodb, sentry, falcon, spark,
july 2016   luxoft holdings – alexandria, va
redshift, microsoft sql data warehouse)
• created hbase tables to load large sets of structured, semi-structured
• implemented java hbase mapreduce paradigm to load data onto hbase
verified its performance over mr jobs
processing and developed code to process it
in tableau to generate reports and also used sqoop to transfer data
• compared the performance of the hadoop based system to the existing
processes used for preparing the data for analysis
• involved in hadoop administration on cloudera, hortonworks and apache
and store the stream data to hdfs using scala
• wrote hive queries to have a consolidated view of the mortgage and
• loaded the load ready files from mainframes to hadoop and files were
converted to ascii format.
upgrades, backup, and recovery, commissioning and decommissioning data
• used maven for continuous build integration and deployment.
data from different databases.
• exposure to burn-up, burn-down charts, dashboards, velocity reporting
• involved in installation and configuration of jdk, hadoop, pig, sqoop,
effort estimation on code development.
• created hive queries to process large sets of structured, semi-
• shared the knowledge of hadoop concepts with team members.
• sun one application server, web logic application server, web sphere
employee data such as personal information, title, working hours,
• gathered requirements from the client, analyzed and prepared the
• involved in database designing and developing sql server.
improved data consistency and redundancy.
• planned daily injection of products for customer orders by deciding on
orders delivered with less than 2% of complaints by customers over 3
java se-8 programmer
19+ years of experience in i.t.
scrum master trained to lead agile project teams, do sprint planning, manage sprint backlog, daily scrums, manage sprints and assign tasks in an iterative process.
experienced in interpretation of data and presentation of strategic business intelligence and recommendations to executive committees.
have led change management initiatives based on business intelligence gathered from data to improve operational efficiencies and retained earnings.
defined data requirements and report layouts for the business review creating test plans and scripts based on requirements.
migrate and redesign all ssis packages and reports to the new platform.
database technologies
support for existing and new reports in ssrs
responsible for building reports in power bi from the scratch.
redesign of demand management cube sourcing from datamart instead of named queries to improve processing performance and implementing partition process strategy. this cube has over 500 users worldwide.
currently recognized as sme on ssas in the organization.
developed, monitored and deployed ssis packages.
used recently introduced power bi to create self-service bi capabilities and use tabular models.
designed and developed multiple reports in ssrs 2008r2 as a source for roambi mobile solution.
sr. business intelligence specialist                                              feb 2007 - apr 2011
helped the accounting department design and implement the etl process for the school accounting model to analyze incomes and expenses in college divisions.
into target data sources by performing different kinds of transformations (for loop, bulk insert, data flow, data mining, ftp, xml tasks, etc) using sql server integration services (ssis).
designed, developed, and implemented the etl process for enrollment analytic model, which served as the basis of a predictive model used by directors.
provided on-site and remote post-production support to the ola models and analytic tools users.
bachelor in systems of information technologies
implemented advanced procedures of feature engineering for data science team using the in-memory computing capabilities like apache spark written in scala
aws amazon components
worked on spark streaming using amazon kinesis for real time data processing.
executed hadoop/spark jobs on aws emr using programs, data stored in s3 buckets.
populating database tables via aws kinesis firehose and aws redshift.
cluster co-ordination services through zookeeper.
specializing in the hadoop ecosystem.  i am a high performer, innovative,
project direction, focus, organization, and strong leadership and
dataframes.
• use of apache flume; staging data in hdfs for further analysis.
• development of custom large-scale enterprise applications using spark for
|of data analytics, data processing  |lisp • sql • javascript • jquery • c|
|mongodb, hbase, rdbms, hive         |flume, apache hadoop, apache hadoop |
|databricks, hortonworks hadoop      |apache hue, sqoop, kibana,          |
mastercard is heavily involved in analytics research and has several large
data installations throughout the united states.  this project involved
chevron – san ramon, ca
bucketing mechanisms on both the managed and external tables.
using kafka.
worked on spark sql and dataframes for faster execution of hive queries
developed metrics, attributes, filters, reports, dashboards and also
created advanced chart types, visualizations and complex calculations to
hive, spark.
loading data from diff servers to aws s3 bucket and setting appropriate
bucket permissions.
cassandra data modeling for storing and transformation in spark using
load and transform large sets of structured, semi structured and
streaming.
designed a cost-effective archival platform for storing big data using
installed and configured pig for etl jobs and made sure we had pig scripts
collected and aggregated large amounts of log data using apache flume and
hadoop hdfs, nosql,  apache cassandra, apache hbase, mongodb, mysql, mssql, crystal reports, netsuite, great plains, salesforce, sap, oracle
amazon aws, microsoft azure, anaconda cloud, elasticsearch, solr, lucene,
care in the medical industry. used cloudera distribution, apache sqoop,
mechanisms on both the managed and external tables in hadoop
distributed ecosystems.
• debugging and fixing production issues in the hadoop environment.
• implemented workflows using apache oozie framework to automate hadoop
serialization & memory tuning to optimize performance of hadoop
• data ingestion using flume with source as kafka source & sink as
hadoop distributed file system (hdfs).
participated in large-scale hadoop data analytics projects involving
performance and data organization.
processing data in hadoop distributed file system (hdfs).
• used different file formats like text files, sequence files, avro, and
structured and unstructured data.
ntt’s work focuses on next generation healthcare systems suing analytics
to drive better outcomes and lower costs.  used hadoop cluster, hadoop
vice-versa using sqoop.
orchestrate jobs that extract the data on a timely manner.
requirement, and project plan.
requirements gathering.
• responsible for the collection of new data and the refinement of
• evolving the hr data warehouse by working with it and hr leaders to
• experience in hadoop ecosystems with database and etl.
• experienced in ansible, jenkins, and pyspark.
process.
my name is jeffrey reiher, and i am continually amazed at the variety of
← accustomed to working with large complex data sets, real-time/near
compressions (snappy& gzip)
← experience developing oozie workflows for scheduling and
← strong hands-on experience in hadoop framework and its ecosystem
data protectionapache log fileszookeeper
virtualizationcloud
snappydatabasesql, mysql, postgresql, mongodb, hbase, cassandra
monitors job performances, filesystem/disk-space management, cluster & database connectivity, log files, management of backup/security.
monitoring and managing the hadoop cluster using cloudera manager.
may 2016 paybook – austin, tx
mentored analyst and test team for writing hive queries.
created a daily job to compress data from 300gb/day to 42gb/day and coalesce 250,000 files/day to 300 files/day
built a poc cluster in aws; planning to migrate the whole cluster to cloud
used oozie scheduler system to automate the pipeline workflow and extract the data on a timely manner.
developed dashboards using sqlbi and tableau.
designed and developed various weekly and monthly reports showing detailed information that could be used to send information to diverse group of users, clients, and managers.
built tableau visualizations for ad-hoc reporting against various data sources like flat files (
experience in implementation of sql server new features like alwayson availability groups, contained databases, change data capture (cdc), online index rebuilds, central management server, transparent data encryption (tde), user-defined server roles.
expertise in dynamic sql, collections and exception handling.
southern illinois university, carbondale, il
-profile-
etl processes, edp, real-time processing, batch processing, streaming processes, cloud security, cloud filtering, linear regression rather than logistic regression, datacleaner, winpure data cleaning tool, patnab, openrefine, drake
project management & leadership
iheartmedia
analysis and development of java and scala applications using spark for data extraction from elasticsearch data.
created pipeline schedulers in gitlab to execute periodic data pipelines.
development, execution and debugging of aws step functions to automate etl processes from emr hadoop cluster creation, data transformations to daily, weekly, monthly and quarterly data and reports.
development of json based scripts to perform automation steps on data processes like table creation, insertion and executing shell scripts.
responsible for troubleshooting and resolution of application performance issues, connectivity and security issues.
monitored system and application performance with troubleshooting and l3 engineering support and resolution of escalated issues.
i was involved in the modernization of the corporate etl process. i was responsible for analysis, design, and development of a new enterprise etl solution.  the new data etl solution integrates kafka, spark, scala, cassandra, avro, protobuf, and oracle to deliver a high-throughput of data and transformation from a wide variety of applications like apple care, apple music, itunes, icloud, retailers, etc.  i developed event driven solutions with spark streaming from mac os applications and other data sources in various file formats.
the sources of data can be as disparate as csv, protobuf, avro, kafka, teradata, oracle, etc.
apple proprietary tools were also used.
environment: hadoop, hdfs, hive,spark, spark streaming, scala, kafka, rdd. protobuf, protoc, rest, scalatest, csv, avro, maven, apple, cassandra, netezza, hdfs, teradata, oracle, dataframe, load, transform, business rules
i3 solutions
processing the data in hdfs and analyzing the data.
worked on different big data file formats like txt, sequence, avro, parquet and snappy compression.
providence, ri
worked on design, architecture, and implementation of big data pipeline and hdfs ingestion from various sources for efficient process and supporting realtime queries and analysis for financial risk management and decision making.
like hadoop/hbase, zookeeper.
responsible for configuring deployment environment to handle the application using jetty server and web
installed and administered collective[i]'s first hadoop cluster utilizing the cloudera distribution.
rolled out new staging tier by repurposing existing hardware, integrating with puppet and following
written hive jobs to parse the logs and structure them in tabular format to facilitate effective querying on
used amazon web services s3 to store large amounts of data in a repository.
has extensive experience in resource allocation, scheduling project tasks, and risk analysis activities.
expertise in data loading techniques like sqoop, flume. performed transformations of data using hive, pig according to business requirements to hdfs for aggregations.
having daily scrum calls on the status of the deliverables with business users.
documenting the requirements in project approval document for approvals from stakeholders and further reference in subsequent phases of the project.
creating process flow diagrams, to lay out the dependency on the various database systems involved performing impact analysis on various system, for the changes requested.
coordinating for demos with business users to highlight the changes and provide training support.
approve the alternative solutions concerning the areas of opportunity of ict by aligning the applicable regulations in the field coordination
siemens		princeton, nj
consultant name   |   phone  999-999-9999   |  email@gmail.comevan lloyd flint
created external hive tables to store the processed results in a tabular format.
worked on multi clustered environment and set up hortonworks hadoop ecosystem.
elasticesearch
san jose, ca
managed communication with remote teams in subsequent meetings to align and control business processes.
data consumption from webmd stored in google cloud, consisting of network clicks and network impressions. (clickstream) these data sets were to be combined, then flattened so that there was only one row for each unique user id.
handled thousands of files with sizes usually between 1 and 5 gb each compressed.
optimized data using dataframe api migrating from old group with joins to the new datasets api with catalyst optimizer.
worked with the resultant data structure as a data frame with a single column and tab-separated values, which was saved to a tab-separated csv file.
sanmina
las vegas, nv
• experience developing oozie workflows for scheduling and orchestrating
up between all the clusters.
• strong hands on experience in hadoop framework and its ecosystem
present     darden restaurant group – orlando, fl
•     used impala where possible to achieve faster results compared to hive
•     performed performance tuning for spark steaming e.g. setting right
•     implemented spark using scala and spark sql for faster analyzing and
•     configured fair scheduler to allocate resources to all the
applications across the cluster.
•     used zookeeper for various types of centralized configurations, git
•     used hive, spark sql connection to generate tableau bi reports.
•     created hive generic udf's to process business logic that varies
scripts with pig.
•     used zookeeper and oozie for coordinating the cluster and scheduling
advanced data management and analytics for customers.
like data scientists and business partners.
prompted by the need to fully characterize a 10,000-plus undrilled vertical-
required the use of big data platforms to collect, aggregate, process and
• met with stakeholders to gather requirements and define the pocs to
build out for experiments in the big data arena.
• documented requirements gathered from stake holders.
hbase, shell scripting, eclipse, oozie, navigator.
• building, installing, configuring servers from scratch with os of
performed red hat linux kernel tuning, memory upgrades.
• apply os patches and upgrades on a regular basis and upgrade
• installed and configured apache, tomcat, and web logic and web sphere
solutions and provide preventive measures.
paul baez ordonez
more than 10 years’ experience programming and computer science
programming languages:  sql, pl/sql, u-sql, r, pig latin, hiveql, python, py4j, scala, blueprint xml, ajax, typescript, active script, active directory, powershell, .net programming, c, c++, c#
data visualization: kibana, powerbi
data processing:  etl, real-time, stream, batch, data cleansing, data mining, data modeling
developed scala scripts, udffs using both data frames/sql/datasets in spark 2.1.1 for data aggregation, queries and writing data back into oltp system through sqoop.
conducted scrum daily stand up, product backlog, sprint planning, sprint review & sprint retrospective.
11/2011 – 2/2013
installed medium-sized sharepoint 2013 farm –with 3 wfe servers, 2 application servers, and 2 database servers for an internal collaboration portal.
created custom site templates for different departments to be able to use this template to create sites
configured fast search utilizing continuous crawl to ensure optimal fresh data.
performed code reviews of deliverables provided by other developers
used tsql statements, stored procedures and query analyzer to manipulate tables.
created batch files and scripts to move log files once they had expired.
performance tuning across environments including optimization of search, web services, custom code analysis, and warm-up scripts.
ph: (999) 999-9999
jeremie havice
5 years in big data engineering
apache cassandra, apache hbase, oracle, sql server, rdbms, hdfs, parquet, avro, json, snappy, gzip
hive, mapreduce,  sqoop, kafka, spark, elk stack (elasticsearch, logstash, kibana)
security:  kerberos, ranger
techfield atlanta, ga
built real-time streaming data pipelines with kafka, spark streaming and cassandra.
built jenkins jobs for ci/cd infrastructure from github repos.
big data developer july 2015 – jan 2017
cloud county community college
honors
system.
• for creating tables, views, indexes, stored procedures, and functions.
• nosql databases such as hbase, cassandra.
• performed the performance and tuning at the source, target, and data
• performance tuning in the live systems for etl/elt jobs in hive,
• in-depth understanding of data structures and algorithms and
|development, unit testing,          |adobe dreamweaver                   |
|functional testing, design thinking,|operating systems                   |
between hdfs to rdbms.
processes that are the backbone of this process.
management.
using sqoop. environment: hadoop cluster, hdfs, hive, pig, sqoop,
measuring and diagnostic tools to test and modify electronic parts. ensured
nnm).
• informed and advised site representatives, central design
authorities and program managers on the process by which legacy
meet emergent mission requirements.
to the existing engineering team and non-engineer personnel.
• consulted with senior management and customers concerning ia
• worked on uml diagrams for the project use case.
• involved in structuring wiki and forums for product documentation.
software, telephone switching systems (sb-3865, nortel, etc), idnx, atm,
series switches in a dmvpn network.   developed, design and analyze new
defense message system
regulations
elasticsearch.
programming experience in python and java, and experience with data
spark api, avro, scala, python, parquet, orc, microsoft powershell, c, c#,
apache drill, presto, apache hue, sqoop, kibana, tableau, aws, cloud
store the stream data to hadoop distributed file system (hdfs).
• skilled in monitoring servers using nagios, data dog, cloud watch and
using efk stack elasticsearch fluentd kibana.
• created tableau dashboards for tns value manager in using various
tableau features, i.e., custom-sql, multiple tables, blending,
• used apache spark & spark streaming to move data from servers to
hadoop distributed file system (hdfs) to rdbms (database).
writing efficient calculations, data source filters, indexing and
• implemented aws solutions using e2c, s3, rds, ebs, elastic load
using cloudformation templates.
pig, hive, spark, hadoop.
• import/export data into hadoop distributed file system (hdfs). and
|[pic]         |jan 2014- may 2015                                         |
|              |granular, llc – milwaukee, wi                              |
kafka in hadoop system.
•  consumed the data from kafka queue using storm, and deployed the
application jar files into aws instances.
• built a full-service catalog system which has a full workflow using
•  loaded data from various data sources into hadoop distributed file
•  designed a cost-effective archival platform for storing big data using
environment:  hadoop, spark, hdf, oozie, sqoop, mongodb, hive, pig,
storm, kafka, sql, acro, rdd. sqs s3, cloud, mysql, informatica, dynamo
• documented technical specs, dataflow, data models and class models in
university of maryland university college, adelphi, md (gpa 3.3)
developed scripts and automated data management from end to end and sync up
hadoop distributed file system (hdfs), configuration of nodes, yarn, hadoop
data and batch migration to hadoop.
ability to troubleshoot and tune relevant programming languages like sql,
able to design elegant hadoop, spark, hive solutions w/ problem statements.
applications using spark.
• experiences in writing udf's for hive and pig.
latin.
• performed end to end design including the derivation of calculations
• both on-premise and cloud.
• experience in working with github repository.
• capable in developing/writing test plans, test cases, and test scripts
• ability to work in high-pressure environments delivering to and
• good interpersonal skills and ability to work as part of a team.
• ability to develop pig udf's to pre-process the data for analysis.
|soft skills                         |aws, azure, anaconda cloud,         |
|troubleshooting, and creating strong|                                    |
required to implement the prioritized requirements.
• designed and implemented hive tables and loaded data into hive.
a datastax connector.
worked on data warehousing, hadoop hdfs, pipelines. pulling data from
jan 2013    state of vermont – montpelier, vt
israel vertiz
work history
prepared the solution design document and got it reviewed and approved.
provided transparent mechanism for ingesting data for frequently changing source schema, aka schema evolution, and provided point-in time query capability of data with capability to identify and eliminate duplicate records.
set the target always encrypted configuration for database columns using the set-sqlcolumnencryption cmdlet (in the sqlserver powershell module). using the set-sqlcolumnencryption cmdlet modified both the schema of the target database as well as the data stored in the selected columns.
responsible for maintaining the system, reviewing all log files of all daemons.
used workflows and coordinators for integrating app services and web services, including rest api consumption and azure sql database and scheduled the data flow pipeline.
worked on spark transformation process, rdd operations, data frames, validate spark plug-in for avro data format
created a cron job to execute a program that will start the ingestion process. the data is read in, converted to avro, and written to the hdfs files.
tested all services like hadoop, zk, spark, hive server & hive metastore.
worked on heap optimization and configurations for hardware optimization.
installed and configured ambari log search under the hood it required a solr instance, that can collect and index all cluster generated logs in real time and display them in one interface.
tested kibana and elk by creating a test index and injected sample data.
retrieved information from sql server through a web services using c# in an asp.net application.
created a custom windows application to assist project managers on client personal contents using inventory software to provide deep control of all assets, using scan guns to translate machine readable barcodes, and then storing all data (including: scans, photos,  digital signatures) into the sql server database.
(203) 936-6943
xiangliangli101gmail.com
implementation and management of data systems using cloudera hadoop, hortonworks hadoop, hadoop on aws cloud platform or on premise.
data repositories
traveler's insurance
configured linux on multiple hadoop environments setting up dev, test, and prod clusters within the same configuration
work with docker containers launching spark applications with scala
pushing containers into aws ecs
hortonworks used for installation of hortonwork cluster and performance monitoring.
nat’l guard
apache ant, yarn, flume, hcatalog, hive, kafka, maven, oozie, pig, spark, tez, zookeeper, hdfs, kibana, solr, lucene
mgm casinos| las vegas, nv	may 2016 – present
developed tableau workbooks from multiple data sources using data blending.
leading the team for testing the data and liaising with the developers and the stakeholders to ensure quality.
reviewing business requirement document for completeness, analyzes actual, and forecast budgeting and forecasting requirements.
created and delivered presentations to various business teams regarding the current status and key decisions regarding their business issues
created tableau dashboards using stack bars, bar graphs, scattered plots, geographical maps, gantt charts, line charts using show me functionality.
created complex mappings of slowly changing dimensions, and change data capture process.
information systems security
experience in using ide's such as eclipse, intellij for debugging and developing python and spark applications.
slack, san francisco, ca
february 2019- july 2019
created a migration plan to move from cloudera kafka to confluent kafka.
kafka-connect project planning.
tune kafka.
collaborated on lenses integration with kafka-connect.
managed lenses (landoop) integration with kstreams.
explore kstreams use cases.
the chemours company, wilmington, de
used etl to transfer the data from the target database to pentaho to send it to tableau.
january 2014 - june 2015
wrote flume and hiveql scripts to extract, transform, and load the data into database.
ad hoc training in robotics
consultant
netbeans, jupyter notebooks, eclipse, intellij, pycharm, atom, visual studio
apache storm, apache hive, apache cassandra, apache hadoop, apache hadoop, apache hcatalog, spark mllib, graphx, scipy, pandas, mesos, apache tez, apache zookeeper
github, git, gitlab, svn
we practiced agile scrum as our style of software development
worked with security team on cloud accesses for users and services.
hadoop big data engineer 	02.2014 – 06.2015
used scala to implement spark frameworks
c++ - in progress( 571) 934-4460   |   rafael.a.morell1412@gmail.com
importing of data from rdbms into hdfs using sqoop.
maintained version control and organized repositories in github.
performed day-to-day tasks regarding projects in a “semi-agile” environment.
worked with elasticsearch, log stash and kibana (elk).
sept 2015 to nov 2016
tuning of spark jobs for setting batch interval time, level of parallelism, and memory tuning
google's app engine, amazon's aws ec2, and openstack cloud)
(active directory, kerberos, knox, ranger) migrations focusing on
• highly proficient in client / stakeholder management with a penchant
• strong problem solving & technical skills coupled with confident
visual basic, sql, pl/sql, vb script, c/c++, c#, css, php, .net
|distributions                     |data processing                   |
|cloudera, hortonworks, mapr, aws, |apache spark, spark streaming,    |
(hortonworks), apache nifi, lucene/solr indexing, and related open-
✓ designed and implemented solutions on aws cloud.
✓ developed dag's for daily production run.
✓ developed wrapper scripts in shell and python.
✓ used hdfs and aws s3 as storage to build hive tables.
big data architect/engineer
✓ ran the hadoop jobs on 89 node cluster on production environment.
5.1.3)
spark streaming with rdd and spark sql in java.
✓ extensive experience with hadoop and hbase, including multiple
✓ involved in creating hive tables, and loading and analyzing data
data into hdfs and pre-processing with pig.
of text data
✓ implemented test scripts to support test driven development and
continuous integration.
✓ exported the analyzed data to the relational databases using sqoop
versions.
✓ used checkpoints to check the attributes of the application across
management system for uft scripts.
environment for running uft framework.
✓ created several test scripts using uft to create batch tests and
✓ used to setup trouble shooting sessions to resolve the issues.
b.s. in computer science, university of louisiana at lafayette, lafayette,
mckinley kareem
good knowledge on spark framework on both batch and real-time data processing
toad, podium, talend, informatica
administration
apache lucene, elasticsearch, kibana,
apache solr, cloudera search
infrastructure
implementation of data lake on aws s3 and cloud service.
•	used beautifulsoup for extracting data from html and xml files.
•	used oozie scheduler system to automate the pipeline workflow and extract the data on a timely manner.
•	collected and aggregated large amounts of log data using apache flume and staging data in hdfs for further analysis.
used iss to host website with .net framework, batch scripted, script with powershell, bash, vbscript, wmi.
grainer, atlanta, ga
microsoft azure, windows server, vpn, firewall, vsphere, linux, lan, wan, san, nas, asav, vnets,solarwinds
main responsibilities are infrastructure and linux, and windows 2008/2008r2 server theories, principles and concepts. including application infrastructure and standards, networking fundamentals, windows server, san storage systems, clustering, physical server architecture, vsphere environment, lan/wan/firewall/vpn network technologies. ensure all new and proposed software is tested with existing environment prior to deployment. acted as project manager on global projects with a focus on, risk assessment, costs, timing and benefits. provides input based on the research of current software and operating system service packs, patches, updates and fixes for servers and desktops. make recommendations based upon the severity, critical needs and implications to environment and supported applications.
advanced experience supporting, maintaining, deploying windows server platforms - including active directory, file/print services/web services/dns/dhcp/windows clustering/windows group policies.
systems support	sept 2004 - february 2006
required to have a complete knowledge and understanding of windows nt 4, 98, 2000 pro/xp and server 2000/2003 with active directory.
kayode hammed
expert in troubleshooting, diagnosing, performance tuning, and solving oracle database and hadoop related issues in production and sdlc database environments.
skilled in performance tuning to provide robust systems for low latency and fast retrieval on cloud (aws) and on-premise environments.
hortonworks hdp 2.6 & hdf 3.0
operating systems
ambari v2.5
configuration management
search
apache lucene, elasticsearch, elastic cloud, kibana, apache solr
warner music group
provided big data and general distributed system technology, design/development and expertise. this was an aws cloud implementation with heavy duty redshift using the technologies below.
irving, tx
installed and maintained the client’s first elasticsearch (elk) cluster on aws hosting about 4tb of indexes used primarily to ingest log data with information dashboards created with kibana.
rdd, spark streaming, pair rdd operations, check-pointing, and sbt.
implemented poc to migrate map reduce jobs into spark rdd transformation using scala ide for eclipse
involved in cassandra and teradata for nosql database.
involved in designing, installations and maintenance of kafka and talend.
created design documents, architectural documents and technical documents for poc
worked on highly available 120 nodes production cluster running hdp 2.50
worked with highly unstructured and structured data of 1.2 pb in raw size.
strong understanding of the internals and interrelationships of hadoop components and hardware/software infrastructure on which they are built.
experience in working with relational databases and design database schemas using sql.
performed upgrades, patches and fixes using either rolling or express method.
collected and analyzed business requirements to derive conceptual and logical data models.
developed database architectural strategies at the modeling, design and implementation stages.
translated a logical database design or data model into an actual physical database implementation.
skokie, il
worked with mining tools such as rapidminer and orange to unlock trends and patterns and derive insights in complex datasets.
bachelor of science degree in computer science
implemented spark in emr for processing big data across our data lake in aws system
experience in configuring, installing and managing hortonworks & cloudera distributions.
skillset
vba, python (jupyter notebook, pandas, numpy, matplotlib, scikit-
created training program to form professionals as machine learning developers.
used kibana to create custom dashboards, data visualization and reports.
aws big data engineer 	oct 2016- march 2018
developed aws cloud formation templates to create custom infrastructure of our pipeline.
installed, configured and managed aws tools such as elk, cloud watch for resource monitoring.
experienced in amazon web services (aws), and cloud services such as emr, ec2, s3, elb and iam entities, roles, and users.
etl to hadoop file system (hdfs) and wrote hive udfs.
created unix shell scripts to automate the build process, and to perform regular jobs like file transfers.
developed shell scripts, oozie scripts and python scripts.
download data through hive in hdfs platform.
developed job processing scripts using oozie workflow to run multiple spark jobs in sequence for processing data.
managing hadoop clusters via command line, and hortonworks ambari agent.
monitored multiple hadoop clusters environments using ambari.
performed cluster and system performance tuning.
run multiple spark jobs in sequence for processing data.
certifications
email:  manishkathait484@gmail.com
hadoop big data engineer
professional profile
proficient in major vendor hadoop distribution like cloudera, hortonworks, and mapr.
proficient in mapping business requirements, use cases, scenarios, business analysis, and workflow analysis. act as liaison between business units, technology and it support teams.
development environments
amazon cloud
file formats
parquet, avro, orc, json
data engineer	february 2019 - present
work: machine learning model to analyze high severity incident reviews to identify common, recurring “themes”
work: create an internal emailing tool that sends customized mails to the employees.
view history of sent emails
boca raton, fl
closely worked with data science team in building spark mllib applications to build various predictive models.
dearborn, mi
used spark-streaming apis to perform necessary transformations and actions on the real-time data using bedrock data management tool.
architecture - spark clusters exclusively from the aws management console
data engineer		september 2013 – january 2015
us bancorp uses big data to capture transactional data, early fraud detection and monitor consumer habits to make recommendations, as well as generate early warnings.  worked on the data platform to ensure the delivery of the right data, manipulating data from various sources and cleaning to provide a basis for quality real-time fraud prevention.
developed etls to pull data from various sources and transform it for reporting applications using pl/sql
technologies.
▪ strong in planning and development of data governance, security, and
▪ collection of log data from different sources like web server logs and
▪ experience deploying large multiple nodes of a hadoop and spark
using etl, and data cleansing.
▪ experience developing oozie workflows for scheduling and
distribution.
▪ extending hive and pig core functionality by writing custom udfs.
technical skills
|apache camel, flume, apache kafka,  |data visualization                  |
customer data from 1010data to hdfs.
• worked with data team to integrate streamset into deployed projects.
security data, and poc benchmark testing between 1010data and apache
• taught a spark class at a p&g “data engineering university” in warsaw,
• wrote/updated company documentation for use case processes
compress to parquet format, and reorganize in s3
jupyter notebooks, knime
this data is also used to direct innovation across georgia pacific.
• data import, export with sqoop to go between hdfs and rdbms.
• migrated etl jobs to pig scripts for transformations, joins,
yarn jobs submitted by users.
batch interval time, correct level of parallelism, selection of
analytics performed on the data.
• worked on disaster management with hadoop cluster.
manager, ambari, oracle, mysql, cassandra, sentry, falcon, spark, yarn
evolent health is involved in the analysis of high-risk patient data for
data in hdfs & mongodb.
bucket based hive joins.
on policy.
• worked with clients to better understand their reporting and
to manipulate the data.
facility hygiene, and cleaning.  this project focused on the development of
• imported data from disparate sources into spark rdd for processing.
kafka.
• involved in design and development of technical specifications using
unstructured data.
from social media websites.
• loaded data from various data sources into hdfs using kafka.
• used shell scripts to dump the data from mysql to hdfs.
and pig jobs as per the requirements.
gas data to minimize drilling risks and to create optimal production
results.
hadoop.
• involved in loading the created files into hbase for faster access of
all the products in all the stores without taking the performance hit.
and staging data in hdfs for further analysis.
• implemented partitioning, bucketing in hive for better organization of
data node, namenode recovery, capacity planning, and slots
escalating issue when necessary.
research now – plano, tx
• developed a double-blind comparative encryption utility with scipy, c,
• eliminated the company's liability for client and company data because
the application renders all data unidentifiable to the human eye.
• moved fraud investigations from reactive to proactive.
microsoft exchange.
• discovered several terminated employees with active accounts to third-
• decreased the load of scans ninety percent.
• discovered several vulnerabilities and exploit code that our propriety
• automation of deployments with ansible.
minutes.
using html, css script, and atlassian confluence.
o decreased onboarding time by thirty-seven percent.
• linked all alerts to knowledge base playbooks.
• generated appdynamics byte code to run with proprietary applications
presentation & facilitation
poland.  the class was part of the larger big data workshop.  the spark
create a data frame in spark and query it using sparksql.
master of business administration and business computer information systems
-----------------------
▪ engineering
▪ processing
hands-on aws emr and s3, and redshift clusters in aws.
continuous integration tools jenkins cicd) and automated jar file deployment.
planned and architected serverless architecture using aws lambda, sqs and dynamodb
march 2018 – present
migrated log management system from flume to kafka.
optimized spark jobs migrating from spark rdd’s api to data frames.
configured elastic search, log stash and kibana (elk) for log analytics, full text search, application monitoring in integration with aws lambda and cloud watch.
td ameritrade
medimmune
collaborated with the hadoop team to add and decommission nodes from the hadoop cluster
spark streaming to receive real time data using kafka.
configured spark streaming to receive real-time data to store in hdfs.
bachelor of arts
(470) 407-2467
5 years’ experience in big data engineering
primary technical skills in spark, spark streaming, sparksql, kafka, kibana
works as team member, individual contributor or team lead providing mentoring to engineers.
knowledgeable of hadoop architecture and various components such as hdfs, job tracker, task tracker, name node, data node and legacy systems using mapreduce.
hands-on experience developing pl/sql procedures and functions and sql tuning of large databases.
misc development █████████
handled structured data with spark sql to process in real time from spark micro-batching.
spark jobs, spark sql and data frames api to transform structured data into emr.
macy’s                                                                       april 2017 - may 2018
handled importing of data from rdbms into hdfs using sqoop.
used cloudera manager for maintaining heathy cluster.
big data engineering
big data administration
utilized spark data frame and data set extensively for processing.
used spark-sql to load json data and create schema rdd and load into hive tables and handles structured data using spark sql (spark structured streaming).
optimized cost of aws cloud through reserved instances, selection and changing of ec2 instance types based on resource need, s3 storage classes and s3 lifecycle policies, leveraging autoscaling.
spark structured streaming
elk
aws data engineer
ranger
performed streaming data ingestion to the spark as a consumer from kafka.
implemented kibana to provide data visualization dashboards and reports.
created a kafka broker in structured streaming to get structured data by schema.
improvising the tuning options using hive functions such as partitioning, bucketing, index and udf
expertise in hive queries and have extensive knowledge on joins.
genuine parts, atlanta, ga	march 2017 – may 2018
automated cloud deployments using aws cloud formation templates.
knowledge of setting up kafka cluster.
phone: (319) 682- 6838
well-versed in installation, configuration, administration, and tuning hadoop cluster of major hadoop distributions (cloudera cdh 3/4/5, hortonworks hdp 2.3/2.4, and amazon web services (aws).
worked with various file formats (delimited text files, click stream log files, apache log files, avro files, json files, xml files).
expert in big data ecosystem using hadoop, spark, kafka with column-oriented big data systems on cloud platforms such as amazon cloud (aws), microsoft azure and google cloud platform.
distributions
hadoop, hive, spark, maven, ant, kafka, hbase, yarn, flume, zookeeper, impala. hdfs, pig, oozie, tez, zookeeper, apache airflows
apache solr/lucene, elasticsearch/ kibana
rapidminer, ibm spss modeler, oracle data mining
apache camel, flume, apache kafka, apatar, atom, fivetran, heka, logstash, talend,
wrote design documentation for code and data versioning of ml models.
created data catalogue using glue crawlers.
automated the creation of ec2 instance and autoscaling zones.
wrote python programs to convert sas data into parquet format.
developed workflow in oozie to automate the tasks of loading data into hdfs and pre-processing with pig and hive.
wrote shell scripts for automating the process of data loading.
handled the real time streaming data from different sources using flume and set destination as hdfs.
integrating kafka with spark streaming for high speed data processing.
context relevant	seattle, wa
extracted the data from rdbms (oracle, mysql) to hdfs using sqoop.
ernesto samano
i’ve been working as a hadoop engineer, business intelligence analyst and dba
importing real-time logs to hadoop distributed file system (hdfs) using flume.
designed, developed and tested extract transform load (etl) applications with different types of sources.
coded a java application to parse json input files into pojos as part of the distributed copy process.
partnered closely with a team of data technologists, data scientists, and business analysts leading boeing’s big data strategies.
october 2016 to sept 2018
implement kafka messaging consumer to access data from kafka brokers and from spark streaming.
may 2014 to may 2015
deployed the big data hadoop application on aws cloud.
spark streaming to divide streaming data into batches as an input to spark engine for batch processing.
cloudera cdh, hortonworks hdp
apache spark, spark streaming, storm
data pipeline
hadoop administation
scripting
eclipse, oracle j developer, visual studio, sql navigator, intellij
frameworks
storage
file formats & compression
clip project
sre project
june 2018
led implementation and participated in hands-on data system engineering.
implemented the business rules in spark/scala to get the business logic in place to run the rating engine.
used ambari ui to observe the running of a submitted spark job at the node level.
loading rdbms of large datasets to big data by using sqoop.
used hibernate orm framework with spring framework for data persistence and transaction management.
used the lightweight container of the spring framework to provide architectural flexibility for inversion of controller (ioc).
implemented partitioning, dynamic partitions and buckets in hive for optimized data retrieval.
it neer inc.
manage hosting and administration of website
analyzed defects in test director with pl/sql using sql plus, toad and sql navigator 4
created, suggested and produced documents with one or more suggestions on how to fix defect.
misc project tasks, accomplishments and responsibilities
tested code change and make sure changes made did not affect other parts of application.
analyze defects assigned to myself in test director with pl/sql using sql plus, toad and sql
code the jsp, j2ee, html or java script solution approved by team lead.
looked up customers in oracle crm using sql plus
updated peoplesoft rem ticket and e-mailed bellsouth csr that issues were resolved.
installed peoplesoft, oracle, and many other software application tools for users
delegated work to others on team to optimize efficiency
dirk sorkin | phone: (703) 910-3326  email: dirksorkin0@gmail.com
optimize data analytics processes using spark, hive, storm, pig, sqoop,
• expert in writing complex sql queries with databases like db2, mysql, sql
testing and integration testing.
• extensive knowledge in development, analysis and design of etl
xml, blueprint xml, ajax, rest api, spark api, jso
hdfs, data lake, data warehouse, redshift, parquet, avro, json, snappy,
cloud platforms and tools
cloud
maven, apache oozie, apache pig, apache spark, spark streaming, spark
washington, d.c.
|
• created mapping documents to outline data flow from sources to targets
• involved in dimensional modeling (star schema) of the data warehouse
framework's, implementation building and communication of a range of
• • explore clickstream events data with sparksql.
• • optimized the configuration of amazon redshift clusters, data
udtf, udaf.
structured and unstructured data and analyzed them by running hive
• created multi-node hadoop and spark clusters in aws instances to
sqoop, oracle, oozie, aws- ec2, s3, sqs, dynamodb, yarn
• imported and exported data from different relational data sources like
and some pre-aggregations before storing the data into hdfs.
times of them using the etl tools like qlikview and pentaho
the sentiment/emotions & opinion of the company/product in the social
impala, sqoop, pig, hbase, git, sqoop, phoenix, eclipse, stratosphere, sql,
nusi, created appropriate nusi for smooth (fast and easy) access of
systems) to teradata rdbms using bteq, fastload, multiload, and tpump.
• developed scripts to load high volume data into empty tables using
command.
• identified and tracked the slowly changing dimensions, heterogeneous
mappings.
• based on the business requirement.
university of new orleans, new orleans, la
software engineer with 30+ years of experience developing software for operational systems, product analysis, big data analysis, real time applications, signals analysis, network solutions, and image processing.
energetic and enthusiastic with 7+ years’ experience as a data professional with a passion for data processing, systems, data cleaning and preparing big data for use in various analytics.  experience designing, architecting implementing and using hadoop big data ecosystems.  proficient in etl and data pipeline methods and tools, and querying and cleaning data.  key tools are: hadoop, spark, amazon aws, cloudera, hortonworks.
20+ years of experience: c++, linux/unix shell scripting, perl, java, python, sql, mysql, nosql, html5, css3, visual basic, hive ql, python, scala, cobol, xml, blueprint xml, ajax, rest api, spark api, json, avro, parquet, orc, jupyter notebooks, eclipse, intellij, pycharm
data & file management
sql and nosql, rdbms, apache cassandra, apache hbase, mapr-db, mongodb, oracle, sql server, db2, sybase, rdbms, hdfs, parquet, avro, json, snappy, gzip, das, nas, san, data analysis and reporting, skilled in digital reporting, dashboards, and making presentations.
capitalone – mclean, va
created artifactories for storage of proprietary libraries.
influential in further modularization and unit testing inputs.
migrated etl jobs to pig scripts for transformations, joins, aggregations before hdfs.
performed both major and minor upgrades to the existing cloudera hadoop cluster.
involved in creating hive tables, loading with data and writing hive queries.
worked on spark sql and dataframes for faster execution of hive queries using spark and aws emr
credit card companies are data savvy organizations basing operations and strategy on enormous numbers of credit card holders and merchants.  this project is based on capital one’s analysis of credit profiles and user patterns in predictive analytics to manage risk and to attract the best and most lucrative customers.
loaded data from various data sources into hdfs using kafka.
used nosql databases like mongodb in implementation and integration.
worked on streaming the analyzed data to hive tables using sqoop for making it available for visualization and report generation by the bi team.
moving data from oracle to hdfs and vice-versa using sqoop.
used linux shell scripts to automate the build process, and to perform regular jobs like file transfers between different hosts.
responsible for building scalable distributed data solutions using hadoop.
performed on console work monitoring the operational system for both goes weather satellites.
monitored the system / looking at the algorithm output products for image quality and frequency of the products. performed overall product counts to verify the ground system was running as expected (python).
harris corporation – long beach, ca
ibm – moving data into hadooprahib amin
proven success in team leadership, focusing on mentoring team members, and managing task for efficiency.
analyzed the ms-sql data model and provided inputs for converting the existing dashboards that used excel as a data source.
expert with design of custom reports using data extraction and reporting tools, and development of algorithms based on business cases.
keeps managers and stakeholders apprised of project status and informed of any pertinent events.
ide:
hadoop distributions:
spark, spark streaming, java, python, scala, pyspark, pytorch
aws:
developed poc using scala & deployed on yarn cluster, compared the performance of spark, with hive and sql.
hive for queries and incremental imports with spark and spark jobs for data processing and analytics.
build a spark proof of concept with python using pyspark
spark applications using spark core, spark sql and spark streaming api
cloudera manager used for installation of cloudera cluster and performance monitoring.
writing hive queries for analyzing data in hive warehouse using hive query language.
experience in migrating the data using sqoop from hdfs to relational database system and vice-versa according to client's requirement.
cassandra, hbase, redshift, dynamodb, mongodb, ms access, sql, mysql, oracle, pl/sql, postgres sql, rdbms
august 2017 – present
ally financial
march 2015 – april 2016
vpc setup for multiple project.
creation of udf functions in python or scala.
data governance, security & operations experience.
big data cloud systems engineer iii
•	managed binary artifacts using jfrog artifactory to ensure reliable and secure builds with convenient access to snapshot and release versions.
technologies: apache spark, hadoop, scala, aws (simple storage service (s3), elastic mapreduce (emr), elastic cloud compute (ec2), elastic container service (ecs), lambda, and elastic load balancing (elb)), java, jenkins, github, jira, confluence, terraform, maven, gradle, artifactory, splunk, pagerduty, kibana
trained engineering staff on engineering best practices and technologies, including tdd, bdd, automated testing, change impact analysis, hadoop/hdfs, apache hive.
set-up automated continuous integration processes using private jenkins server.
discovered and documented bugs in the podium data platform, and worked with podium technical representatives to fix the bugs and develop a patch for the client.
trained analysts on how to securely transmit and share documents in the cloud and bank intranet.
coordinated work between on-site and offshore resources in order to maximize productivity and reduce analyst downtime
identified workflow defects and cataloged them in jira for resolution in future sprints
technologies:  apache hadoop, apache pig, apache hive, podium data, java, er studio data architect, atlassian jira, amazon aws, cucumber/gherkin, subversion (svn), github
spark streaming with kafka & hdfs & mongodb to build a continuous etl pipeline. this is used for real time analytics performed on the data.
may 2015- may 2016
involved in creating hive tables, loading the data and writing hive queries.
apache kafka to transform live streaming with the batch processing to generate reports
jan 2014-may 2015
involved in analyzing system failures, identifying root causes and recommended course of actions.
developed, tested, and implemented financial-services application to bring multiple clients into standard database format.
as a sole developer, created application allowing workers to scan received product and inventory, speeding up handling and eliminating human error.
phone: (651) 371-1285
hands-on use of spark and scala api's to compare the performance of spark with hive and sql, and spark sql to manipulate data frames in scala.
sqoop, oozie, yarn
spark, scala, pyspark, pytorch, java
parquet, avro & json, orc
professional experience profile
3m, maplewood, mn        		july 2017 - present
developed etl pipelines using spark and hive for performing various business specific transformations.
managing hadoop clusters via cloudera manager, command line, and hortonworks ambari agent.
developed oozie workflow for scheduling and orchestrating the etl process within the cloudera hadoop system.
used spark api over cloudera hadoop yarn to perform analytics on data in hive.
map reduce, hdfs, hive, pig, pentaho, hbase, zookeeper, sqoop, oozie,
flume, storm, yarn, spark, scala and avro.
• self-starter, lifelong learner, team player, excellent communicator.
• well organized with great interpersonal skills.
• hands-on pig latin script migrating into java spark code.
functional programming, sql, java, java swing, javascript, hive ql,
big data platforms, software, & tools [pic][pic][pic][pic][pic]
• implemented partitioning and bucketing in hive based on the
• used spring ioc, autowired pojo and dao classes with spring controller
• prepared pig scripts and spark sql/spark streaming to handle all the
transformations specified in the s2tm's and to handle scd2 and scd1
• involved in converting hive/sql queries into spark transformations
• worked on apache spark writing python applications to convert txt, xls
• involved in collecting metrics for hadoop clusters using ambari.
• experienced in implementing spark rdd transformations, actions to
broadcast variables.
• created shell scripts to parameterize the pig, hive actions in oozie
• worked with eqm and uat teams for fixing the defects immediately by
environment: cdh 5.5.1, hadoop, map reduce, hdfs, nifi, hive, pig, sqoop,
may 2015    hadoop data architect/engineer
strategic business intelligence regarding online properties, domains and
• worked on hive to create numerous internal and external tables.
in hive.
• utilized spark dataframe and spark sql api extensively for processing.
• developed dynamic parameter file and environment variables to run jobs
• executed tasks for upgrading clusters on the staging platform before
• installed and configured various components of the hadoop ecosystem.
• used spark engine, spark sql for data analysis and provided to the
may 2015    citizens financial group– providence, ri
structured and unstructured sources into hdfs (aws cloud) using sqoop
• collected and aggregated large amounts of data from different sources
algorithms.
• worked on converting pl/sql code into scala code and also converted
• hadoop and cassandra as part of a next-generation platform
• built-in request builder, developed in scala to facilitate running of
unstructured data
report generation and running the jars in hadoop. coordinated with
the banking industry.
• implemented static and dynamic web pages using jsp, javascript, css.
xml, neatbeans
aug 2010    blue cross blue shield of alabama – birmingham, al
• used hibernate orm tool as persistence layer - using the database and
objects) to the application.
stack developer and skilled java programmer, able to leverage skills with
fluent in architecture and engineering of the hadoop, cloudera, hortonworks, amazon aws, azure, mapr hadoop ecosystem.
skilled in the use of mapreduce, mapreduce jobs and generating tools like pig or hive.
uses expert skills across a number of platforms and tools, and working with multiple teams in high visibility roles.
experience in data modeling and architecture involving realtime database, sql, no sql, hdfs, data warehouse and data lakes
apache camel, flume, apache kafka, apatar, atom, fivetran, heka, logstash, scriptella, stitch, talend, ketl, kettle, jaspersoft, cloveretl
zookeeper, oozie, cloudera manager, ambari
apis and microservices deployments
designed a contribution model to allow multiple teams to collaborate with our platform.
worked with sns, ecs cluster and lambda functions in order to create tasks and move files from s3 location in one aws account to another.
technologies:  aws emr, ecs, cloudformation, cloudwatch, lambda, kinesis, sns, sqs, rds, pstgresql, java, scala, terraform, github, jira, confluence, jenkins, apache kafka, spark, hdfs, yarn, maven, gradle, cucumber tests, junit, splunk, sping, springboot, groovy, bash, ksh, sonarqube, jmeter.
april 2018 – october 2018
worked with flink state, process functions, aggregator, coprocessfuntion and window function.
set kafka brokers and topics with the proper replication factor and partitions to provide exactly once kafka semantics.
linux centos operating system.
developed object-oriented java application to create the main engine and python scripts as kafka producer.
xml
apache nifi
aws
ec2
hive ql
optimization
bid data architect & engineer
created a poc involved in loading data from linux file system to cloudera platform and hdfs.
aws hadoop cloud data architect
citizens insurance, tallahassee, fl
big data cloud engineer / architect
involved in developing pig scripts for change data capture and delta record processing between newly arrived data and already existing data in hdfs.
data lake
transformations
partitioning/bucketing
clusters
transitioned windows server from usa to méxico taking operations from customer infrastructure to the current environment.
automate cicd on qa/prod environments, helping out developers/qa automation team to achieve their day to day goals.
apache flume and kafka for collecting, aggregating, moving from various sources.
 development and debugging experience on python, scala and java.
 set the spark job to process the data to redshift and emr hdfs(hadoop).
expertise in aws data migration between different database platforms like local sql server to amazon rds and emr hive.
taxaudit - citrus heights, ca
university of california, riverside
page 6
use of amazon cloud (aws) using elastic mapreduce, elasticsearch, cloudera impala.
etl tools/frameworks
worked directly with the big data architecture team which created the foundation of this enterprise analytics initiative in a hadoop-based data lake.
design included a variation of the lambda architecture
extracted real time feed using kafka and spark streaming and convert it to rdd and process data in the form of data frame and save the data as parquet format in hdfs.
hadoop architect & engineer
spark notebooks.
handled the real time streaming data from different sources such as cassandra db, cosmodb and sql, using flume and set destination as hdfs.
designed jobs using oracle, odbs, join, merge, lookup, remove, duplicate, copy, filter, funnel, dataset, file set, change data, capture, modify, role merger, aggregator and peek, role generator stages.
design of kibana dashboard over elasticsearch for log monitoring
wrote mapreduce code to process and parse data from various sources and store parsed data into hbase and hive using hbase-hive integration.
involved in creating hive tables, loading with data and writing hive queries, which will internally run a mapreduce job.
amazon university
phone: (513) 999-9741
big data engineer with experience in aws cloud and on-prem systems design and implementation. experience with aws, hadoop, hortonworks, cloudera, cassandra and redshift. able to use spark data frame and data set from spark sql api for data processing.  fine-tuned resources for long-running spark applications to utilize better parallelism and executor memory for more caching. data pipelines, transformations and custom development with visualizations in kibana.
hadoop big data infrastructure for batch data processing and real-time data processing.
extending hive and pig core functionality by using custom user defined function's (udf), user defined table-generating functions (udtf) and user defined aggregating functions (udaf) for hive and pig.
scala, python, bash
created kafka broker for structured streaming to get structured data by schema.
used cloudera manager for maintaining healthy cluster.
installed and configured tableau desktop to connect to the hive framework (database) which contains the bandwidth data
streaming events from hbase to solr using hbase indexer.
vincent schneider
5 years of experience with the hadoop ecosystem and big data tools and frameworks.
amazon ec2, amazon s3, amazon simpledb, amazon rds, amazon elastic load balancing, amazon sqs, and other services of the aws family
extensively worked on datatsage sever and parallel job controls and sequencers. designed and developed parallel jobs by using different types of stages such as transformers, aggregator, merge, join, lookup, sort, remove duplicate, funnel, filter, pivot, shared container for developing jobs.
█   █   █   █   █   █   █  education & training  █   █   █   █   █   █   █
experience with multiple terabytes of data stored in aws s3 using elastic map reduce (emr) and redshift for processing.
database/datastore
amazon cloud platform (aws)
elt pipelines
hadoop & big data
“configured and managed an aws pipeline.  established processes to utilize team strengths and maintain best practices.”
“many spark jobs were running at once, so we created scripts for oozie to control the workflow.”
big data administrator
july 2000 – june 2014
provided solutions for issues that were unresolved by level 1, 2, 3, and 4 analysts.
military:
ibm cognitive class - big data
email    rafael.loustaunau03@gmail.com
the university of texas, san marcos
san marcos, tx
experience in apache nifi which is a hadoop technology and also integrating apache nifi and apache kafka.
strong knowledge on hadoop architecture and various components such as hdfs, job and task tracker, name and data node, secondary name node and map reduce programming.
used scala libraries to process xml data that was stored in hdfs and processed data was stored in hdfs.
load the data into spark rdd and do in memory data computation to generate the output response.
used impala for querying hdfs data to achieve better performance.
implemented apache pig scripts to load data from and to store data into hive.
involved in converting hive/sql queries into spark transformations using spark rdds, scala and python.
deployed to amazon web services (aws) cloud services like ec2, s3, ebs, rds and vpc.
involved in moving all log files generated from various sources to hdfs for further processing through flume.
implemented spark using scala and utilizing spark core, spark streaming and spark sql api for faster processing of data instead of in java.
used spark-sql to load json data and create schema rdd and loaded it into hive tables and handled structured data using spark sql.
use of cassandra architecture, replication strategy, gossip, snitch etc.
involved in hadoop cluster environment administration that includes adding and removing cluster nodes, cluster capacity planning, performance tuning, cluster monitoring.
designed and implemented partitioning (static, dynamic), buckets in hive.
environment: hadoop, cloudera manager, linux, redhat, centos, ubuntu operating system, map reduce, hbase, sqoop, pig, hdfs, flume, pig, python.
medline industries, inc., is a private american company based in northfield, illinois, it is the largest privately held manufacturer and distributor of medical supplies providing products, education and services across the continuum of care with offices in 20 different countries.
installation and configuration of big insight cluster with help of ibm engineers.
edit and configure hdfs and tracker parameters.
script the requirements using bigsql and provide time statistics of running jobs.
contact:  phone:  203-815-1483  |  email: derek.yangdy@gmail.com
responsible for building data expertise and data quality for the transfer pipelines for transformation and moving of data using flume, spark, spark streaming, hadoop.
file types
sql injection, data ftk imager, xxs
spark & hive
i was in charge of the development team for spark, building the skeleton for spark jobs using technologies like s3, to merge datasets using complex transformations.
in charge of hadoop administrative tasks like security configuration, rack awareness, integration with multiple technologies to secure the hadoop cluster, also creation of containerized applications using technologies to automate the process of instantiation.
create sudo files white and black listing to add additional layer of security to hadoop environment
create standardized documents for company all usage
use rally for tracking task and kanban
lead development by managing and coaching not spark developers in the hadoop team
work with off-shore team to troubleshoot oozie jobs, solr, hive, ranger in all environment
the company keeps a close eye on its trade spend, analyzing large volumes of data and running complex simulations to predict which promotional activities will be the most effective. kellogg’s had been using a traditional relational database on premises for data analysis and modeling, but by 2013, that solution was no longer keeping up with the pace of demand.
recommended a solution to meet needs to accommodate petabytes of data, scale according to infrastructure needs, and stay within budget.
implemented test and development environments for all of the company’s u.s. operations.
spotify 	hadoop engineer
performance tuned and troubleshot issues related with service components and the cluster
setup and configured ranger for handling the authorization of the service components
netgear. designs and manufactures network-attached storage (nas) and network video recorder (nvr) solutions for small, medium, and enterprise-level businesses. the company had to become more efficient at extracting insights from customer data. the company was already using aws for its myqnapcloud service. this service enabled customers to back up their local nas data to amazon simple storage service (amazon s3) and amazon glacier. qnap collected event logs and usage statistics from the service, but wanted to make better use of the information.
implemented the analytics platform using amazon virtual private cloud (amazon vpc), allowing it to use security groups to control access to the platform.
loaded and transformed large sets of structured and semi structured data from hdfs.
teksystems, inc.	data administrator
involved in scheduling oozie workflow engine to run multiple hive, sqoop and spark jobs.
vincent nwobodo
komodo health // san francisco, ca
training
project mgmnt
drupal, ux design
all relevant documentation created and recorded in confluence pages.
built complex sql functions to fetched data and returned results used in reports and other api’s.
imported data from aws s3 and into spark rdd and performed transformations and actions on rdd’s.
foot locker, new york, ny
responsible for that day-to-day operation and support of hadoop environments.
collaborate with corporate it function around integrating hadoop ecosystems with critical enterprise systems.
analyzed and interpreted transactions behaviors and clickstream data with hadoop and hdp to predict what customers might buy in the future.
configured hive meta store with mysql, which stores the metadata of hive tables
configured flume for efficiently collecting, aggregating and moving large amounts of log data.
implemented fair scheduler on the job tracker to allocate the fair amount of resources to small jobs.
extraction to and from multiple data sources, such as rdbms, sql, nosql data warehouses (teradata, oracle, aws), hadoop data lake environments (hdfs, hive, hbase, cassandra, impala, mongodb, dynamodb, etc.).
custom etl, data query and visualization systems for corporate data collection, ingestion, processing, access, and visualization all built on a modern, cloud-based tech stack with best-in-class tools. this project required streaming real-time data from iot devices.
visa | foster city, ca
performed streaming data ingestion to the spark distribution environment, using kinesis.
responsible for fine tuning of the database, troubleshooting, memory management.
created hadoop poc to consider changing over the whole system when available, if we like it.
dec 2013-jan 2015
210-960-3326
education                     +
ftp, html5, css3
unix/linux, windows 10, windows8, windows xp, ubuntu, apple os x (yosemite, mavericks)
qlikview, tableau, kibana
senior hadoop engineering consultant
responsible for defining and understanding the key business problems to be solved.
gathered, integrated and prepared data for consumption in machine learning and advanced analytics usages.
developed a data pipeline using kafka, spark and hive to ingest, transform and analyzing data.
involved in managing and reviewing hadoop log files.
extended hive core functionality by using custom user defined function's (udf), user defined table-generating functions (udtf) and user defined aggregating functions (udaf) for hive
kraft foods uses big data to stay agile and repsonsive to ever changing consumer demands.  tio do this, it has established a self-sufficient hadoop ecosystem on an aws cloud platform, which it harnesses and captures powerful first-party data, and mines it for insights that enables it to build compelling content.
used oozie to orchestrate the map reduce jobs that extract the data on a timely manner.
responsible for writing hive queries for analyzing data in hive warehouse using hive query language (hql).
capital group- los angeles, ca
coordinated with different teams sme (unix, vmware) for os configuration/unix server hung issues.
with the help of ibm clearquest & bmc remedy, picking up incidents for non prod and prod env raised by hadoop developers/testers and troubleshoot, resolve the same on our own and sometimes seeking help from hortonwoks engineers.
email: ibrahimamir213@gmail.com   phone: (571) 934-4496
email: ibrahimamir213@gmail.com
experience integrating kafka and spark by using avro for serializing and deserializing data, and for kafka producer and consumer.
work with cross-functional teams to develop data-driven application
setup email notification for google and apple subscription job submission
code lambda serverless application to transfer datasets from s3 to s3
feb 2018 – june 2019
used git version control and jenkins continuous integration server for cicd.
mar 2014 – mar 2015
able to develop custom pipelines and dashboards, clean data and manage all aspects of data processing.
experience with data visualization tools.
helped create data ingestion framework for multiple source systems using pyspark.
performed spark optimizations based on shuffling reduction.
arcelormittal | burns harbor, in	aug 2017 – may 2019
comprehensive knowledge and experience in process improvement, normalization/de-normalization, data extraction, data cleansing, data manipulation
knowledge in incremental imports, partitioning, windowing, and bucketing concepts in hive and spark sql needed for optimization.
developed mapreduce programs to process raw data, populated staging tables and stored the refined data in partitioned tables.
used hive optimization tools like partitioning, bucketing, map side join etc.
ust global
ingalls memorial hospital
wsw health partners
in depth understanding/knowledge of hadoop architecture and various components such as hdfs, job
used apache hadoop for working with big data to analyze large data sets efficiently.
experience in importing and exporting terabytes of data between hdfs and relational database systems using sqoop.
performed the performance and tuning at source, target and data stage job levels using indexes, hints and partitioning in db2, oracle and datastage.
adept with various distributions such as cloudera hadoop, hortonworks, mapr and elastic cloud, elasticsearch
hortonworks, mapr, mapreduce
cloudera, hortonworks, mapr, aws, elastic, elastic cloud, elasticsearch, cloudera cdh 4/5, hortonworks hdp 2.3/2.4, amazon web services (aws)
used zookeeper for various types of centralized configurations, git for version control, maven as a build
documentation of the tasks and the issues is done.
involved in collecting, aggregating and moving data from servers to hdfs using apache flume
created and maintained technical documentation for launching hadoop clusters and for executing pig scripts.
jan 2012 – may 2013
good knowledge in impala, spark/scala, shark, storm.
accountable for data updates into health information database for hippa compliance by logging treatment interventions using secure patient health information management access terminal.
managed the in-house operational database for electronic health records;
performed other mission critical business needs functions as assigned.
provided exceptional levels of support and customer service to all university guests, visiting researchers and exchange faculty / students within the divisions of research and cooperation;
certificate in management information systems
distributed systems, large-scale non-relational data stores, rdbms, nosql map-reduce systems, data modeling, database performance, and multi-terabyte data warehouses.
master’s in applied computer science
january 2015 – june 2016
load and transform large sets of structured, semi structured and unstructured data working with data on amazon redshift, apache cassandra, and hdfs in hadoop data lake.
responsible to manage data coming from different sources.
august 2012 – september 2013
installed and configured pig and also wrote pig latin scripts.
worked on loading log data directly into hdfs using flume.
worked on developing custom mapreduce programs and user defined functions (udfs) in hive to transform large volumes of data with respect to business requirements.
netbeans, jupyter notebooks, eclipse, intellij, pycharm
ajax, rest api, spark api,
version control
kibana, tableau, sqoop, apache drill, presto, apache flume, apache airflow and camel, apache hue, yarn, apache hive, apache kafka, apache maven, apache oozie, apache pig, apache spark, spark streaming,
responsible to review and understand how quantum is used to ingest and process batch and real-time data using apache spark, scala and sql.
the project involved sources of data as disparate as csv, parquet, avro, kafka, snowflake tables, etc.
created and configured yaml files to set input, output cluster parameters, centrify groups, jar file versions and ec2 instance information, like security groups and subnet ids.
key technologies:  quantum (big data framework), spark, sql, python, aws, s3 buckets, aws lambda, emr cluster.
created a lambda architecture variety consisting of near real-time data processing with spark streaming, spark sql; and spark clusters.
used spark dataframe api over azure hdconnect platform to perform analytics on hive data.
the advent of the internet of things (iot), the cloud and big data, it now is possible to use sensors to collect the telltale signatures and send them to big data analytic engines in off-site datacenters for analysis. predictive analytics can be used to improve manufacturing quality and maintenance.
optimized hive analytics sql queries, created tables/views, wrote custom udfs and hive-based exception processing.
wrote custom udfs in pig and hive in accordance with business requirements.
application development and systems analysis
understanding of hadoop big data architectures, data movement technologies, database partitioning, database optimization, and building communication channels between structured and unstructured databases.
apache camel, flume, apache kafka, apatar, atom, fivetran, heka, logstash, scriptella, stitch, talend, talend, ketl, pentaho data integration (kettle), jaspersoft, cloveretl
senior big data engineer		march 2017 - present
siemens constructed big data pipelines using hadoop and aws platforms for the analysis of water consumption for the purpose of improving efficiency in water management.  siemens installed pressure and flow rate sensors in water pipes in test cities. data was consumed from internet of things (iot) sensors in various stages of the water service in key cities of various countries around the world.  data gathered included the season, the time of day, the weather, and special events such as a soccer games. water utilities always have to supply enough water while at the same time striving to consume as little energy as possible.  this big data project succeeded in reducing energy consumption by eight to twelve percent.
smart water meter data
data engineer	february 2013 – september 2014
email: consultant@gmail.com
agile, continuous integration, test-driven development, unit testing, functional testing, gradle, git, github, svn, jenkins, jira
sql, hiveql, apache solr, kibana, elasticsearch
skilled in aws, redshift, dynamodb and various cloud tools.
provide end-to-end data analytics solutions and support using hadoop big data systems and tools on aws cloud services as well as on-premise nodes.
uses flume, kafka, and hiveql scripts to extract, transform, and load the data into database.
migrated data through sqoop and hive in hdfs platform.
migrating the data using sqoop from hdfs to relational database system and vice-versa according to client's requirement.
2002 – 2004
northern trust bank – chicago, il
understanding of etl architectures, data movement technologies, database partitioning, database optimization, and building communication channels between structured and unstructured databases.
client-side scripting
data science
neural networks
hadoop hdfs, parquet, avro, orc, json
vmware, dockers
11/2016-present
set up stream analytics using spark streaming for analysis of iot time data.
worked closely with the architect and assisted with aspects of the project involving stakeholders, planning and documentation.
southern company is an american gas and electric utility holding company based in the southern united states. it is headquartered in atlanta, georgia, with executive offices also located in birmingham, alabama. the company is currently the second largest utility company in the u.s., in terms of customer base. through its subsidiaries it serves 9 million gas and electric utility customers in nine states.  the use of big data analytics drives the business strategy for acquisitions, expansion, implantation of power grids, stations and service areas.
summary of qualifications
machine learning with tensorflow on google cloud platform
development environment
kerberos and ranger
performed upgrades, patches and bug fixes in hdp and cdh clusters.
import the data from different sources like hdfs/hbase into spark rdd.
ability to conceptualize innovative data models for complex products and create design patterns.
involved in benchmarking hadoop and spark cluster on a tera sort application in aws.
data engineer		september 2013 – november 2015
costco		issaquah, wa
unix shell scripting, object-oriented design, object-oriented programming, functional programming,  sql, java, hive ql, mapreduce, python, scala, xml, blueprint xml, ajax, rest api, spark api, json, avro, parquet, orc, jupyter notebooks, eclipse, intellij, pycharm
i was part of the team in charge of developing a batch processing tool that reconciles every credit card transaction with visa and mastercard companies. the application processes 5 files provided in csv, parquet, avro and json formats. the size of these files is tenths of gigabytes and all of them are provided in s3 buckets. the application processes the files using spark sql in an emr cluster of 3 nodes of 8 cores each. after joining and transforming them, the application delivers a 20 gb file to visa and mastercard via sftp containing details of accounts having made any transaction on a given day, each day. the log files are monitored using splunk. we present stats about the daily execution of the app in dashboards and charts in splunk. also, it triggers alerts onboarded with pagerduty that notify team members by e-mail, sms and phone automatically if intervention is needed during or after the execution. the execution of this app is generating a revenue to the bank of 10 million dollars per year.
the rules engine uses a second web service that returns fico scores from equifax, transunion and experian credit bureaus for factoring into the determination. i write the rules in a business rules management system (brms) called drools.
write and created output log files monitored by splunk
worked with cluster management in aws to ensure all data infrastructure to be replicated in east and west regions in aws. high availability ensured through an activity that switched regions every 3 months in case of failure.
agile methodology followed with daily standups, and use of jira.
for one of the use case, used spark streaming with kafka & hdfs & cassandra to build a continuous etl pipeline. this is used for real time analytics performed on the data.
wrote shell scripts to execute scripts (pig, hive, and mapreduce) and move the data files to/from hdfs.
used oozie to automate/schedule business workflows which invoke sqoop, mapreduce and pig jobs as per the requirements.
jan 2012	grupo salinas, mexico
queries, creating user groups and assigning permissions and authority to them. i managed the backups and
db2 for i. i was a member of the circle of excellence of the systems management.
national autonomous university of mexico
• excellent knowledge in understanding big data infrastructure,
framework and complete hadoop ecosystem - hive, hue, pig, hbase,
components such as hdfs, job tracker, task tracker, name node, data
• experience in collecting the log data from different sources (web
zookeeper, sqoop, kafka-storm, spark, pig, impala, and flume.
• implementing rails migrations, active record, action pack and action
and jquery for ui to get a complete end to end system
development, unit testing, functional testing, design thinking, lean,
maven, apache oozie, apache pig, apache spark, spark streaming,
analyze, clean, normalize, and resolve large amounts of non-unique
• experience in optimizing the data storage in hive using partitioning
and bucketing mechanisms on both the managed and external tables.
• worked on importing and exporting data using sqoop between hdfs to
rdbms.
analytics for weather forecasting.
during data analysis.
• integrated hadoop with active directory and enabled kerberos for
• storm integrated with infrastructure, including cassandra, the kestrel
java.
data infrastructure using scala and spark
and used core java technologies to create hive/pig udfs to use in the
project
the bi tools, etc.
transactional and application specific data sources
• exported the analyzed data onto rdbms using sqoop, in order to be used
this project involved analytics systems targeting how the world's
largest brands and media companies measure the impact of their pr
• created hive tables, loaded retail transactional data from teradata
hdfs using scoop.
load data from mongodb into data warehouse.
queries using apache oozie workflows and sub-workflows.
various metrics for reporting.
suitable for mainframes cics consumption.
• participated in daily scrum meetings and iterative development.
report generation and running the jars in hadoop. co-ordinated with
• collaborated with bi teams to ensure data quality and availability
• shared responsibility and assistance for administration of hadoop,
jul 2011    bank of america – charlotte, nc
application server, web sphere portal server, and j2ee application
• configured application connectivity using jdbc
• used html, css, jsp, and javascript for front end user interface
• worked with the collection libraries.
october 2009     epoca plasticos – sao paulo, brazil
10 years’ experience database, java, and distributed processing systems.
business intelligence, data analytics, data management, and reporting
developed several end to end bi solutions (db objects, etl, reports)
migrated ssrs reports to power bi
created power bi reports for ups quantum view.
redesign of demand management cube sourcing from datamart instead of named queries to improve processing performance and implementing partition process strategy. this cube is critical for forecasting demand and production.
responsible for creating reports based on the requirements using ssrs.
designing and implementing a variety of ssrs reports such as parameterized, drilldown, ad hoc and subreports using report designer and report builder based on the requirements.
maintain integrity of data within the production environment, including the execution of proper backup and restoration policies and procedures
environment: ms sql server 2014/2012, team foundation server, microsoft sharepoint server, crystalreports, ssis, power bi, c#, sql server reporting services (ssrs), visual studio 2013, microsoft azure portal, windows 10, t-sql.
design, development, implementation, and maintenance of repots and dashboards of ssrs reports
create ssrs reports sourcing from sql and ssas sources.
designed and developed multiple reports using ssrs, including multiple levels such as global, detailed, trend, comparative, etc.
designed and implemented sql server database objects (complex stored procedures, functions, views and complex t-sql/queries) to support reporting and various business processes.
developed interactive dashboards and reports.
deploying ssis packages from various sources like bids, management studio, file systems etc.
trained and executed pentaho, a graphical etl tool to load and process big data sources.
mcts sql server 2005 bi certification
hdfs, hive, pig, zookeeper, sqoop, oozie, yarn
sql, spark sql, hive ql
vmware, virtualbox, osi, docker
ranger, kerberos
spark clusters exclusively from the aws management console.
created custom test, design and production spark clusters
• proficient in major vendor hadoop distribution like cloudera,
|continuous integration, test-driven |sql, ajax, json, gson, orc          |
|software                            |yarn, apache hbase, apache hcatalog,|
construction engineerig and civli engineering has lagged behind in
management and enhancement of systems, and development of processes
this project also focused on advanced analytics platforms to predict
zurich – schaumburg, il
involved in creating hive tables, loading with data and writing hive
submitted by users.
imported data into hdfs and hive using sqoop and kafka. created kafka
using spark and aws emr
created hive generic udf's to process business logic that varies based on
hdfs and mongodb.
design and develop etl workflows using python and scala for processing data
wrote shell scripts to execute pig and hive scripts in etl processes.
and migration.
data scientists and business partners.
used the image files of an instance to create instances containing hadoop
orchestrate the jobs that extract the data on a timely manner.
regular basis.
standards.
used linux shell scripts to automate the build process, and to perform
when necessary.
materials and equipment. the objective being to logistical tracking and
work.
installed elasticsearch, logstash, kibana, kafka, zookeeper etc.
participated in managing big data solutions to help cut costs and improve
system (hdfs) to rdbms database.
• collect, aggregate, and move data from servers to hadoop distributed
• created conditional logics in pages using jsf tags and jstl.
aggregations before hadoop distributed file system (hdfs).
system tasks.
• disaster management with hadoop cluster.
cloudera impala
• partitioning, dynamic partitions, and buckets in hive for increasing
bucket based hive joins in hadoop systems.
• cassandra data modeling for storing and transformation in spark using
spark and aws emr.
hive queries, spark sql queries and udfs.
processing of data in hadoop.
file system (hdfs.
hadoop, spark, spark streaming, kafka, hive, sqoop, and related
kafka in hadoop.
• transferred data using informatica tool from aws.
analysis.
• worked with several clients with day to day requests and
recommended course of actions.
senior leaders and ais resources.
text files.
functional specifications and then executing delivery.
• performance tuning of spark jobs in hadoop for setting batch interval
syncsort-dmx-h, subversion (svn), and sql and nosql databases
• design and build scalable hadoop distributed data solutions using
capabilities, broadcasts, joins, transformations in the ingestion
qubole, providing data export capability through api and ui.
hbase, mongodb, cassandra, oozie, spark rdds, spark dataframes, spark
hadoop, cloudera, hortonworks, impalainformatica, talendnodes, aws, ec2, emr, azure, anaconda, elasticcloud, anaconda cloudsqoop, flume, yarn, spark, hive, hueetldatabase, hdfs, data warehouse, data storage, scrapersapache camel, flume, apache kafka, apatar, atom, fivetran, heka, logstash, scriptella, stitch, talend, talend, ketl, kettle pdi, jaspersoft, cloveretl, sqoop
elkarchitecturenodes
hadoop cycle
extraction
wrote custom api driver for zeppelin’s rest apis in python.  committed the driver to apple’s code repository and used the api driver in a custom application program.  this program moves zeppelin notes from one instance to another, uploads zip files to azkaban with a python script that runs, schedules and polls zeppelin notes.
designing and upgrading cdh 4 to cdh 5
harden the cluster to support use cases and self-service in 24x7 model and apply advanced troubleshooting techniques to on critical.
worked extensively with sqoop for importing data.
designed a data warehouse using hive.
developed oozie workflows for daily incremental loads, which gets data from teradata and then imported into hive tables.
executed scripts hive scripts and move the data files to/from hdfs.
worked with management to determine the optimal way to report on datasets
balanced and tuned hdfs, hive, impala, and oozie workflows
environment: tableau, dashboards, ssis, sql queries, system testing, sql
may 2007 bi developer
aug 2010 florida power and light– juno beach, fl
experience in documenting and implementing disaster recovery plans using always on, database mirroring, log shipping, and replication with different topologies for production databases.
environment:  sql server, vmware, cdc, tde, monitoring, security, privileges, replication,
anthonypina0417@gmail.com
210-807-8313
more than 8 years professional experience in it, programming and data analysis/bi.
good experience in importing and exporting data between hdfs and relational database management systems using sqoop.
expertise with hive queries and pig scripts to load data from local file system and hdfs to hive.
work with cloud environments like amazon web services, ec2 and s3.
experienced in loading data to hive partitions and creating buckets in hive.
experienced in relational databases like mysql, oracle and nosql databases like hbase and cassandra.
apache hue, apache sqoop, spark, storm, pig, hive, hdfs, zookeeper, tez, oozie, maven, ant, hcatalog, apache drill, presto, airflow, camel,
servers & storage
sharepoint 2016 enterprise, sharepoint server, 2010/2013/sharepoint 2016, online/office 365, team foundation server (tfs), windows server, nas, san, das, hdfs, parquet, orc
-professional experience-
the solutions we built were highly resilient, scalable using cloud-based technologies and automated using continuous integration and delivery (ci/cd) techniques and procedures.
worked closely with business stakeholders and the business analyst to understand and analyze requirements.
creation and automation of aws hadoop clusters to execute step functions for data pipelines.
benchmark and query database tables created using redshift, redshift spectrum and aurora.
provided 24/7 on-call rotational support of applications to users including issue resolution.
the project initially a poc, to demonstrate parallel data processing using big data ecosystem tools like spark implemented with scala.
responsible for development of a scala application that sends a rest request, with gmt timestamp and gets a response from a web server.
experience in moving data to amazon s3.
creating scripts in hive and pig for processing the data.
responsible for building hadoop clusters with hortonworks/cloudera distribution and integrate with pentaho
them better suites the current requirement.
experience in hbase database manipulation with structured, unstructured and semi-structured types of data.
involved in architecture and design of distributed time-series database platform using nosql technologies
logic 10 and postgres database at the back-end.
followed scrum methodology for the application development.
extracted data from netezza databases to hadoop framework.
environment: hadoop, hdfs, hbase, zookeeper, mvc, database, queries, jetty, orm, netezza, postgres sql, spring, pig, hive, sqoop, api, json
built and supported several aws, multi-server environment's using amazon ec2, ebs, and redshift
programmed etl functions between oracle and amazon redshift.
experienced in collecting the real-time data from kafka using spark streaming.
worked on poc for iot devices data, with spark.
environment: hadoop, hdfs, hive, pig, sqoop, hbase, oozie, my sql, svn, avro, zookeeper, unix, shell scripting, hiveql, nosql database (hbase), rdbms, eclipse, oracle.
aggregations and analysis were done on a large set of log data, a collection of log data done using custom-built input adapters and sqoop.  used tableau for data presentations and reports.
responsible for developing data pipeline using sqoop, mr, and hive to extract the data from weblogs and store the results for downstream consumption.
apache pig scripts were written to process the hdfs data.
extensively worked on hive, created numerous of internal and external tables.
design generic unit test queries for data reconciliation on raw data received by business users and entire transformed data.
created documentation, user manuals, and test cases for various applications and solutions.
-certification-
strong knowledge of pig and hive's analytical functions, extending hive and pig core functionality by writing custom udfs.
expertise in developing pig latin scripts and hive query language for data analytics.  well-versed in and implemented partitioning, dynamic-partitioning and bucketing concepts in hive to compute data metrics.
created classes that simulate real-life objects, and write loops to perform actions on your data.
python, scala, php • python • bash • lisp • sql • javascript • jquery • c • c++ • xml • html • css, visual basic, vba, .net, spark, hiveql, spark api, rest api
hdfs, avro, parquet, snappy, gzip, sql, ajax, json, gson, orc
created multiple reports using data residing in hive per the request of the client.
constructed a kafka broker with proper configurations for the needs of the organization.
experience with multiple terabytes of data stored in aws using elastic map reduce (emr) and redshift.
coding
hadoop, elk
kerberos, ranger
adobe
this project involved taking data provided by webmd and formatting it for ingestion into adobe audience manager, which accepts a tab-separated format. handling schema changes and schema evolution.
used private git repository for code base and version control and the team used salesforce for tracking tasks and reporting.
optimized code to run locally in 1 minute 51 seconds. with a total execution time running on emr and ingesting and outputting to the cloud, in 3 minutes 45 seconds.
home depot
oct 2015 – dec 2016
monitored hadoop cluster using tools like ambari
dedicated and seasoned big data professional with skill in implementing and
• 9 years of experience in the field of data analytics, data processing and
• experience collecting log data from various sources and integrating it
the etl process.
• extending hive and pig core functionality by writing custom udfs.
apache cassandra, apache hbase, mongodb, oracle, sql server, rdbms, hdfs,
databricks, hortonworks
cloud foundry
•     administered hadoop cluster(cdh) and reviewed log files of all
environment: hdfs, pig, hive, sqoop, oozie, hbase, zoo keeper, cloudera
•     scheduled and executed workflows in oozie to run hive and pig jobs
•     worked on installing clusters, commissioning & decommissioning of
•     extraction of data from different databases and scheduling oozie
•     wrote shell scripts to execute scripts (pig, hive) and move the data
boarding needs and present solutions using structured waterfall and agile
•     managed jobs using fair scheduler to allocate processing resources.
•     worked on spark sql and dataframes for faster execution of hive
queries using spark and aws emr
may 2015    rackspace/accenture – san antonio, tx
configurations, along with custom applications.
• created database tables with various constraints for clients accessing
administrative tools and utilities, configure or add new services if
• remote system administration using tools like ssh, telnet and rlogin.
2007: word, excel, outlook, powerpoint.
volume groups and logical volumes to manage the storage resources.
• participated and supported in the migration of production servers from
• resolved critical problems, performing root cause analysis, document
able to evaluate clients existing hadoop infrastructure, performance issues and bottlenecks.
extensive knowledge on performance tuning, cluster monitoring & troubleshooting.
defining job flows in hadoop environment using tools like oozie for data scrubbing and processing.
loading logs from multiple sources directly into hdfs using tools like flume.
familiar in commissioning and decommissioning of nodes on hadoop cluster.
web services & technologies:  jquery, rest json, soap, xml, jquery, ajax, html, css, javascript, xml
hadoop distributions:  hadoop, cloudera hadoop chd, hortonworks, mapr
database and data storage:  sql, nosql, rdbms, cassandra, hbase, data lake, data warehouse, hdfs, nas, san, das, parquet, orc, avro
4/2016 – present
implemented schema extraction for parquet and avro file formats in hive.
aws, elastic search, impala, cassandra, tableau, talend, oozie, horton works, cloudera, oracle 12c, linux,
1/2015-3/2016
developing scripts to perform business transformations on the data using hive and pig.
experienced in collecting, aggregating, and moving large amounts of streaming data into hdfs using flume.
involved with reporting team to generating reports from data mart using cognos.
leading the team, we designed architected and implemented the migrating from legacy normalized sql taxonomy data, customer portfolio data and other data to a modern high-performance big data warehouses running on multiple dw appliances.
defined the data governance strategy, designed security patterns, implemented data standards and procedures across the enterprise; drafted business specific methodology to establish business stakeholder-driven data stewardship through mdm.
created and associated reusable sharepoint designer 2013 workflows on lists and libraries.
created the deployment instruction manual with scripts.
created custom page layouts and templates and custom feature stapling to support automatic site provisioning
08/2008 – 11/2010
big data platforms & distributions
created an application that collects data that can be analyzed to show how the company is received in the general public.  was able to collect tweets that mentioned the companies name or any like terms and any spatial data.
hadoop cloudera platform (hive,hdfs & spark).
junction city, kansas
• skilled in hadoop architecture and various components such as hdfs,
flume, oozie. with pig and hive's analytical functions, extending hive
• working with databases like teradata and proficiency in writing
writer.
hadoop ecosystem.
integration.
• self-motivated, excellent team player, with a positive attitude and
|programming                         |database                            |
|html/xhtml, sql, python, scala,     |file management                     |
|lean, six sigma                     |linux/unix, windows                 |
april 2016  hadoop data architect/engineer
time enables users to view usage and manage accordingly.  data of various
types is categorized and fed to specific custom pipelines which can extract
specific types of data for multiple analytics purposes.
wide as needed. worked on importing and exporting data using sqoop
economic factors, internal business drivers, and seasonality. the focus of
installation, configuration, supporting, patching, version upgrade and
• used cloud infrastructures:  aws emr distribution for hadoop, aws s3
• loaded terabytes of different level raw data into spark rdd for data
• import the data from hdfs into spark rdd
easy to use, and satisfies application requirements, data processing
nov 2009    electronic technician
• ensured effective communication between the different shifts and
• proactively checked the hp enterprise applications, and took
• documented all system outages using remedy; ensured all
• determined a proposed solution for navy legacy applications.
• ensured disposition of systems/applications/networks is properly
enterprise enclaves.
certification and accreditation process (diacap), risk management
obtain required iatt and ato certifications.
• collaborated with and provide cyber security ia technical direction
environment: javascript, html, php, css, eclipse
with a military exception. helped integrated web applications in; visual
mar 1990    career military
replace, and troubleshoot aircraft power plants. repair fuel cell, flight
systems st louis, missouri.
maintenance repairs, network concepts, operating systems/applications
iztechnologies fob delaram, afghanistan (network admin)
collated and processed to position hundreds of survey nodes to accuracies
software, telephone switching systems (sb-3865, nortel, etc.), idnx, atm,
handling and directory service standards and protocol.
university of central missouri university, warrensburg, missouri
cisco certification network associate, ccda, a+, mcp, ciw, and mcsa.
seasoned hadoop data analyst specialized in the cloudera hadoop
distribution with use of various analytic tools such as elasticsearch.  big
spark, spark streaming, data warehousing, and data processing.
hortonworks hadoop and cloudera hadoop distributions, hadoop distributed
including entity resolution, entity reconciliation, named entity
recognition, co-reference, anaphora; basic knowledge of embedding models
the economic research service provides key indicators, outlook analysis,
and a wealth of data on the u.s. food and agricultural system. along with
(hdfs).
• used cloudera hadoop (cdh) distribution with elasticsearch.
source filters, hierarchies, filter actions, maps etc.
• implemented workflows using apache oozie framework to automate tasks
in the hadoop system.
the consulting agency utilizes hadoop big data systems, technologies and
• worked with clients to better understand their reporting and dash
boarding needs and present solutions using structured waterfall and
strengthening security by implementing and maintaining network address
translation in company's network.
using spark rdds.
granular provides digital marketing analytics and consulting. analysis of
amounts of social media data from various sources, search data, and data
•  configured oozie workflow engine scheduler to run multiple hive, sqoop
•  developed a task execution framework on ec2 instances using sql and
file system (hdfs).
and clinical trials, and reporting that leads to successful submission.
compression tool.
distributed file system (hdfs). on regular basis.
|               |datameer – san francisco, ca                              |
these calculations. the data also lives in numerous places and is owned by
• rhands-on experience of sun one application server, web logic
|[pic]          |may 2008 - aug 2010                                       |
university of maryland university college (umuc), adelphi, md
bachelor of science in business administration
sentry, spark, spark streaming, spark mllib, falcon, hbase, hive, pig,
sql for optimization.
distributed data ecosystems.
hadoop cluster disaster management
hadoop data engineer and bi analyst.  proficient in hadoop ecosystem,
• experience with aws cloud iam, data pipeline, emr, s3, ec2, aws cli,
• wide-ranging experience working in oracle, sql server and my sql
with reverse-engineering.
managing stakeholder expectations.
• strong analytical and problem-solving skills.
|network skills                      |storage: das, nas, san              |
relational database systems to an all cloud-based data storage and
pipelines using hive, pig, spark and apache storm.  used aws and redshift
online as well as implemented hadoop data lakes using hdfs.
and prioritize requirements.
• assessed and requested for any infrastructure and environments
heterogeneous tables to make it suitable for ingestion into hive
• used hive optimization tools like partitioning, bucketing, map side
• extensively worked on pig scripts for data filtering, cleansing, and
stored it in aws hdfs.
and pig jobs  per the requirements.
• assisted in designing, building, and maintaining the database to
• developed static and dynamic web pages html, and css.
experimental program to stimulate competitive research (epscor)
lead hadoop engineer /big data developer
may 2016 to present
developed a new ingestion service is developed with functional capability to manage data structure using azure data factory.
designed and implemented a de-duplicate records and end date the final records functions.
mar 2013 to may 2016
digested and consumed data from diverse data sources such as sharepoint, call log, and used to create powerbi dashboards for stakeholders and decision makers to reduce the number of support calls.
exception handling, collection api's to implement various features and enhancements.
worked on azure blob using to duplicate on blob configuration on azure.
worked on json format data and manipulated using python application.
created influxdb for kafka metrices to monitor consumer lags in grafana.
involved with hortonworks support team on grafana consumer lags issues. (currently no consumer lags are generating in grafana visualization within hdp).
installed ansible in all environments.
successfully tested kafka acl's with anonymous users and with different hostnames.
created hbase tables to store variable data formats of data
june 2010 to dec 2011
assisted in the resolution of tickets submitted by end users.
utilized c# and created custom routes and constraints to handle incoming requests for the asp.net mvc blog application.
created new view master page and view content page based on the master page, placing logo, navigation links, and banner advertisements in the view master page.
data ingestion, extraction and transformation using etl processes developed using hive, sqoop, kafka, firehose, flume, and kinesis.
sql, hiveql, spark sql, elasticsearch
apache airflow, apache oozie, nifi
managed hive connection with tables, databases and external tables
installed, configured and managed the elk (elasticsearch, logstash and kibana) for log management within aws ec2 /elastic loabalancer for elasticsearch involving in cloud automation with configuration management system ansible.
populated database tables via aws kinesis firehose and aws redshift.
monitored job performances, file system/disk-space management, cluster & database connectivity, log files, management of backup/security and troubleshooting various user issues.
expedia
created process flow diagrams, to lay out the dependency on the various database systems involved.
performed impact analysis on various systems, for the changes requested.
tableau desktop connecting to hadoop hive tables.
environment: hadoop, cloudera, tableau, hdfs, hive, pig, impala, sql, sqoop, kafka, flume, r-studio, sas.
lead business intelligence analyst /project coordinator
experian | costa mesa, ca	january 2015 – april 2016
managing extracts, schedules and users on tableau server.
managing extracts, schedules and users on tableau server
requirement analysis
defined data marts as per requirement.
published power bi reports to server and power bi mobile.
developed key performance indicators covering all aspects of operations.
design workbooks and dashboards and publishing to the server and managing the user base and access for user groups
bachelor of science in information technology
university of phoenix  gpa: 3.58jarod a. beardsley
jarod a. beardsley
apache cassandra, datastax cassandra, apache hbase, apache phoenix, bigsql, couchbase, db2, mariadb, mongodb, ms access, oracle, rdbms, sql, apache toad
more than 5 years big data experience
implemented a hive to snowflake migration plan using matillion
explore possible machine learning use case for dynamic anomaly-detection
wrote several comprehensive testing suites to verify data quality in new data models
implemented a flume to kafka-connect migration plan.
kafka architecture design and implementation.
recommended spark for data processing, decided not to use for heavyweight computation requirements
collaborated with the infrastructure, network, database, application and analysis teams to ensure data quality and availability.
accelerated analytics with big data for signal processing
esurance’s target demographic was 25-to-49 year-olds who manage their lives online and ideally, are coupled or married and thus more likely to want auto or home insurance.  esurance used big data analytics to determine its ideal target audience.  it is successfully using big data derived from social media to engage the target audience with highly effective social media marketing campaigns.  the platform to support this highly successful analytic activity is built on hadoop and aws cloud services.  as real-time social media sentiment analysis reveals changes in attitudes of the target market, the company builds new pipelines nimbly to capture and analyze specific niche areas ripe for marketing.  current pipelines focused on text analysis and sentiment analysis.
worked on reporting, data extraction, data cleansing, replication, data mapping and data uploading programs related to retail and ecommerce using etl tools.
agile, kanban, scrum, devops, continuous integration, test-driven development, unit testing, functional testing, design thinking
we created an integration with this enterprise cluster provisioner through custom airflow operators.
implemented docker to containerize our application and build in any dependencies.
teamcity used to configure the kubernetes environment to contain all proper accesses and required entities -- i.e. kubernetes secrets and persistent volume claims (pvc) to persist data from ephemeral kubernetes pods.
deployed the application using teamcity by running helm (kubernetes install client) install commands.
successfully took a poc of a deployed web application in the cloud to a full-fledged rest api that enabled integrations to an externally owned/controlled ui and airflow.
transitioned through poc’s and pos’s of different managed versions of airflow (i.e. google’s composer (gcp’s managed airflow) and astronomer (third-party solution for a customized airflow).
developed the application as a series of easily manageable microservices that could be readily deployed to kubernetes.
hadoop cloud architect 	11.2016 – 10.2018
coordinated kafka operation and monitoring (via jmx) with dev ops personnel; formulated balancing leadership strategies and impact of producer and consumer message(topic) consumption to prevent overruns;
established java based shell; reconfigured zookeeper znodes -ephemeral, sequential and persistent nodes;
loaded and transformed large sets of structured and semi structured data from hdfs through sqoop and placed in hdfs for further processing.
gilbane building company, atlanta, ga
big data analytics has been transformative for construction engineering, civil engineering and facilities management. data analytics and real-time data from “smart building” sensors provide facilities management data that has strategic value to future lease and capital investment.  for projects data analysis is used to streamline processes, increase efficiencies and increase roi.
developed pig scripts to arrange incoming data into suitable and structured data before piping it out for analysis.
key technologies:  hdfs, hive, pig, mapreduce, yarn, mysql, oracle, sqoop, hbase, yarn
certifications:
wrote, compiled, and executed programs as necessary for apache spark in scala to perform etl jobs with ingested data.
managed and launched amazon emr instances for development use when end-to-end testing was not feasible.
april 2014 to sept 2015
• 6 years total professional experience in information technology
• using cloudera hadoop, hortonworks, native hadoop, and various
• communication effectiveness with team and stakeholders.  efficient and
• proven expertise in implementing big data projects, managing business
cassandra, zookeeper, sqoop, tableau, kibana, hive bucketing and
apache ant, apache cassandra, apache flume, apache hadoop, apache yarn,
hdfs, hortonworks, mapr, mapreduce
amazon aws – emr, ec2, sqs, s3, dynamodb, redshift, cloudformation, azure,
|oracle, sql server, ms access, db2|web logic server, iis, java web   |
|windows, unix/linux               |testing tools                     |
|scripting                         |hp quality center, hp alm, hp qtp,|
|pig/pig latin, hiveql, mapreduce, |hp uft, jira, soap ui, selenium   |
hartford financial, windsor, ct    april 2016-present
✓ implemented micro-service pub/sub architecture leveraging yarn,
✓ involved in loading data from oracle exadata to hadoop data lake.
✓ running jobs on aws emr.
✓ worked on yarn (mr2) framework.
✓ helped the tableau team integrate with hiveserver2 and postgres for
✓ data is inserted into ibm big sql (cold-storage) and ms sql server
public presentations about these technologies.
✓ experience with hands-on data analysis and performing under
pressure.
wipro, o’fallon, mo    january 2012-may 2013
✓ involved in testing and testing design for the application after
✓ implemented a script to transmit information from oracle to hbase
✓ implemented best income logic using pig scripts and udfs.
✓ cluster coordination services through zookeeper.
✓ troubleshooting, manage and review data backups, manage and review
✓ worked with gui checkpoints, database checkpoints while doing the
functional test on the web application.
✓ utilized the defect tracking system hp alm to report defects in a
✓ developed uft automation framework from scratch for new application
performed exception handling.
entire project.
hadoop architect / engineer
participated in design, development and system migration of high performant metadata-driven data pipeline with kafka and hive/presto on qubole, providing data export capability through api and ui.
experienced in collecting metrics for hadoop clusters using ambari & cloudera manager.
data query engines
created pipelines to ingest source data from a variety of source repositories and streamed services into hadoop hdfs using hive, spark, spark streaming and flume.
analyzed data by performing hive queries (hiveql), impala and running pig latin scripts to study customer behavior.
i helped to develop the solr/lucene search deployed to azure. the indexing was done directly on top of the metadata extracted from various files.
•	installed and configured pig for etl jobs and made sure we had pig scripts with regular expression for data cleaning.
•	successfully loaded files to hdfs from teradata and loaded from hdfs to hive.
infrastructure: windows server 2012, microsoft exchange, active directory, internet information server (iis), system center operations manager (scom), system center configuration manager(sccm), sql server
responsible for deployment, maintenance, and troubleshooting of windows 2012 servers, intel servers, and san.
advanced knowledge of telecommunication network design, topology, systems interfaces, and protocols to meet support requirements.
bachelor of computer science, i.t. security
certified expert in hortonworks hadoop platform architecture, engineering and administration.
use of apache oozie to schedule workflows for streamlined processes with efficiency and automation.
successfully support 99.99% database availability uptime within different lobs of financial domain.
cloudera hadoop (cdh) 4.1.1 – 5.13.0
requirements gathering, documentation, data modeling, data system design, pipeline and etl architecture, search architecture, dashboard architecture, toad data modeler, lambda functions, data modelling, data structures, algorithms, and problem solving
nosql data store
visualization
pipeline & etl
new york, ny
key role in migrating production and development hortonworks hadoop clusters to a new cloud based cluster solution.
cluster consolidation saved administrative overhead cost and service contract cost and reduce technical debt.
used spark streaming api with kafka to build live dashboards; worked on transformations & actions in
exported analyzed data to the relational databases using sqoop for visualization & report generation
workflow management: developed workflow in oozie to automate the tasks of loading the data into hdfs and pre-processing with pig
built and supported several aws, multi-server environment's using amazon ec2, ebs and redshift
worked on db2 to extract data and transfer it to hadoop.
managed and reviewed hadoop log files to identify issues when job fails
hadoop clusters design, installation, configuration and monitoring, troubleshooting, security, backup, re-sizing, addition/deletion of nodes, performance monitoring, hdfs balancing and fine-tuning for mapreduce applications.
setup apache ranger for cluster acl’s and audits to meet compliance specifications.
helped design back-up and disaster recovery methodologies involving hadoop clusters and related databases.
mysql database administrator
responsible for mysql installations, upgrades, performance tuning, etc.
mentored and worked with developers and analysts to review scripts and better querying.
used spark sql and data frame api extensively to build spark applications.
used spark structured streaming to structure real time data frame and update it in real time.
professional technical skills
cassandra, hbase, amazon redshift, dynamodb, mongodb, oracle, postgresql, mysql, hive
javascript, html, sql, c, c++, c#, shell script, html, css
development tools, automation, cicd
elk logging & search
data visualization
used spark sql to perform transformations and actions on data residing in hive.
log monitoring and generating visual representations of logs using elk stack.
pulled data and populated the data in kibana.
implemented aws iam user roles and policies to authenticate and control access.
cloud formation, aws iam and security group in public and private subnets in vpc.
worked with aws lambda functions for event-driven processing to various aws resources.
responsible for designing logical and physical data modelling for various data sources on aws  redshift.
developed aws cloud formation templates to for redshift.
moved relational database data using spark to transform in and move into hive dynamic partition tables using staging tables.
experience in configuring, installing and managing hortonworks (hdp) distributions.
coordinates with monitors cluster upgrade needs, and monitors cluster health and builds proactive tools to look for anomalous behaviors.
managed and scheduled batch jobs on a hadoop cluster using oozie.
monitored hadoop cluster using tools like ambari.
pontifical catholic university of peru
lima, peru
tableau desktop specialist (tableau)
experience in recruiting/human resources data environments and financial environments.
accustomed to working with large complex data sets, real-time/near real-time analytics, and distributed big data platforms.
involved in building a multi-tenant cluster, with disaster management with hadoop cluster.
hands on experience in installing, configuring cloudera's and horton distribution.
solid understanding of statistical analysis, predictive analysis, machine learning, data mining, quantitative analytics, multivariate testing and optimization algorithms.
eclipse, intellij, netbeans, pycharm, visual studio, atom, bluej
team: analytics
send customized emails to individual internal recipients, including custom variables within the message and individual specific cc options
look up manager information from ldap to include in cc if requested
create templates to use for frequently sent emails
capture statistics on email opens and link clicks
core tools and libraries:  django, unicorn, nginx, mochila(inner source tool for sending emails out via ponyex), secret-sauce (inner source tool for reading secrets from the vault), django-sso-oauth(inner django source tool for logging into sso in a django application)
candidate guru was established as a predictive analysis company that matches job candidates with employers by culture fit.  it uses artificial intelligence and machine learning to help companies better vet and rank their massive candidate pipeline.  candidate guru just acquired elevated careers, by eharmony, which does a deeper analysis internally and externally with an employee engagement solution that assesses how its employees feel about their organization and a candidate-matching feature to determine whether job candidates match up by personality, culture and skills.  the project focused on implementing the eharmony pipelines, applications and algorithm into the candidate guru framework.
designed batch processing jobs using apache spark to increase speed compared to that of mapreduce jobs.
utilized spark dataframe and spark sql api extensively for processing.
used spark engine, spark sql for data analysis and provided to the data scientists for further analysis.
wrote and spark codes in java to run a sorting application on the data stored on aws.
chattanooga, tn
us xpress, provider of a wide variety of transportation solutions collects about a thousand data elements ranging from fuel usage to tire condition to truck engine operations to gps information, and uses this data for optimal fleet management and to drive productivity saving millions of dollars in operating costs.
development spark cluster(8 nodes); separation of computer versus storage aws frameworks; designed a persistent versus transient architecture - raw linux server with spark jobs
worked on a hadoop big data platform with native ecosystem frameworks and tools.
involved in creating hive tables, loading with data and writing hive queries, which will invoke and run mapreduce jobs in the backend.
hands-on experience extracting data from different databases and scheduling oozie workflows to execute the task daily.
post  graduate  program  in  business analytics and business intelligence
post  graduate  diploma  in  management-marketing
a variety of data processing and transformation tools and data storage
▪ comfortable working with hadoop distributions, cloudera, cloudera
▪ skilled at staging data in hdfs for further analysis.
▪ expertise at zookeeper for the administration of the hadoop system,
spark, falcon, hbase, hive, pig, sentry, ranger.
|aws, kali, cisco, palo alto         |kafka, hbase, yarn, flume,          |
|nimsoft, nagios, cloudclock         |rapidminer, ibm spss modeler, oracle|
|data pipelines/etl                  |data mining                         |
|postgresql, nosql, cassandra, amazon|                                    |
|s3                                  |                                    |
wiki page. also attached to agile project to create etl workflows for event
• tested viability of kinesis-to-redshift workflow without storage layer
• replicated deployed kinesis-to-redshift capability in apache airflow.
supported data science and sales analytics departments on big-data use
• used tendo cli to export customer data from 1010data to hdfs
• performed analytics using 1010data’s macro language
bigquery tables and export to s3
1010data, sql server, arcadia, postgresql, google analytics/bigquery,
• responsible for collecting, aggregating and etl of data from rdbms to
hadoop cluster.
• performed storage capacity management, performance tuning and
• performed performance tuning for spark steaming e.g. setting right
as hdfs.
• performed import and export of dataset transfer between traditional
databases and hdfs using sqoop.
store the stream data to hdfs.
evolent health – arlington, va
• worked with spark context, spark -sql, dataframe and pair rdds.
integration, and migration.
using datastax connector.
store to hdfs using scale.
• transferred data using informatica tool from aws s3.
• deployed the application jar files into aws instances.
• used the image files of an instance to create instances containing
• developed a task execution framework on ec2 instances using sql and
• load and transform large sets of structured, semi-structured and
• configured oozie workflow engine scheduler to run multiple hive, sqoop
storm, kafka, sql, acro, rdd. sqs s3, cloud, mysql, informatica, dynamo db
on them for data analysis in order to meet the business requirements.
• used zookeeper for providing coordinating services to the cluster.
configuration.
• involved in loading data from linux file system to hdfs.
• documented technical specs, dataflow, data models and class models.
data privacy specialist      september 2010- july 2012
and fortran for comparing
cases.
• framed employee access roles (role based access control) across all
• provided a significant step to iso 2701 certification
institute sift workstation to identify breaches and bad actors.
infrastructure containing rca reports of outages, updates of problem
• performed a proof of concept for seven commonly accepted open source
• wrote plugins, crons, and configuration files in bash for monitoring.
• used solarwinds to determine packet loss, gateway issues, and node
• diagnosed root cause of outages and directing other teams needed for
packs, upgrades, application code promotions (hotfixes), report
segment was an introductory course consisting of understanding spark, the
candidate:  mba/mbs
honors and awards
invented a platform-independent double-blind comparative encryption utility
▪ analysis
kevin.lanni4691@gmail.com | (610) 379-2483
5 years experience as a big data professional with experience in big data engineering, big data development and bid data administration.
spark to optimize etl jobs, and spark to create structured data from the pool of unstructured data received.
able to work with a variety of data stores including sql database like mysql and oracle sql and nosql such as cassandra and hbase.
adept at shell language scripting
vanguard
ingested realtime data using kinesis streams
planned and implemented dynamodb data modelling for various tables.
worked infrastructure and lambdas used to create an interactive faq chatbot backed by an amazon elasticsearch cluster.
created iam roles and policies for various resources used by team to maintain security on pii sensitive data.
played a key role in installation and configuration of the various big data ecosystem tools such as elastic search, logstash, kibana, kafka and cassandra.
handled structured data with spark sql to process in real time from spark structured streaming.
feb 2017 – march 2018
discovery
jan 2016 – jan 2017
initiated data migration from/to traditional rdbms with apache sqoop.
gaithersburg, md
5 years total experience i.t. in data systems with the last 5 years’ experience in big data architecture and engineering;
importing and exporting terabytes of data between hdfs and relational database systems using sqoop.
database/datastore █████████
apache flume, ranger, ambari, yarn, cluster management, cluster security, zookeeper, oozie, airflow
aws big data engineer	cincinnati, ohio
hands-on data extraction from different databases on aws and scheduling oozie workflows to execute the task daily.
worked on managing policies for s3 buckets and glacier for storage and backup on aws.
experience in working with flume to load the log data from multiple sources directly into hdfs on aws platform.
managed aws redshift clusters such as launching the cluster by specifying the nodes and performing the data analysis queries.
installed oozie workflow engine to run multiple jobs with hive hql.
job management using airflow, and development of job processing scripts using airflow workflow to run multiple spark jobs in sequence for processing data.
data developer	gainsville, florida
coordinated with monitors cluster upgrade needs, and monitored cluster health and builds proactive tools to look for anomalous behaviors.
integrated maven build and designed workflows to automate the build and deploy process.
experience with jenkins ci and various version control tools like git.
development and debugging experience on python and scala
spark sql
kafka
aws kinesis
oozie
cluster
workflow
kerberos
fmc corporation, philadelphia, pa 	may 2018 - present
designed hive queries to perform data analysis, data transfer and table design.
involved in implementing ansible configuration management and maintaining them in several
created custom amazon machine images (amis) to automate server build during for auto scaling during peak times. also, deployed applications in aws using elastic beanstalk.
blackrock, new york, ny	jan 2016 – march 2017
used spark to process data on top of yarn and performed analytics on data in hive.
transfered streaming log data from different data sources into hdfs and hbase using apache flume.
developed spark code utilizing scala and spark-sql/streaming for quicker testing and handling of information.
ppl, allentown, pa	may 2014 – jan 2016
support multiple clusters at petabyte scales.
enhanced cluster performance with regular maintenance and upgrades through ambari tower.
worked on configuring kerberos authentication in the cluster.
frank mukendi
knowledgeable of hadoop architecture and hadoop components (hdfs, mapreduce, jobtracker, tasktracker, namenode, datanode, resourcemanager, nodemanager.
uses flume and hiveql scripts to extract, transform, and load the data into database.
skills & expertise
ide
git, github, bitbucket, sourcetree
debugging
aws, emr, ec2, ec3, sqs, s3, dynamodb, redshift, cloud formation
cloudera, hortonworks. aws, mapr
f
project experience
configure spark jobs to be executed on emr clusters.
enacted design solutions for creating python environments across emr clusters.
usage of dvc, mlflow and pychaderm to version code and data as a unit.
wrote shell scripts too be used in ec2 bootstrap actions.
usage of datasync for data transfer from on prem to aws.
rockwell collins	cedar rapids, ia
built re-usable hive udf libraries for business requirements which enabled users to use these udf's in hive querying.
indices setup, manage template & configuration. indexing data from community, sql & no-sql database, cms tool like aem & teamsite.
developed the build script for the project using maven framework.
using aws redshift for storing the data on cloud.
319-382-6838  |  frank.mukendi888@gmail.comernesto samano
skilled in phases of data processing (collecting, aggregating, moving from various sources) using apache flume and kafka.
write hadoop streaming applications with spark streaming and kafka.
database management
sql, oracle, mysql, toad, nosql, rdbms, apache cassandra, apache hbase in hadoop big data ecosystems.
file systems & formats
hadoop distributed file system (hdfs); avro, parquet, orc for hadoop.
automated copy using a samba utility through a distributed copy workflow using an oozie coordinator workflow.
developed job processing scripts using oozie workflow to run multiple actions for processing data such as shell, java, hive, and email.
designed software solutions to meet end user requirements using agile methods and wrote supporting documents.
integrated a variety of database systems including oracle, hadoop big data, and sql server.
colorado springs, co
worked on social media analysis platform build on hadoop clusters. customized data pipelines and reporting for new media analytics projects.
apache open source version with mesos job scheduler; developed, designed tested spark sql clients with scala, pyspark and java clients.
hadoop etl process engineer
involved in developing the application using java/j2ee platform.
responsible for enhancing the portal ui using html, javascript, xml, jsp, java, css per the requirements, and for providing the client-side javascript validations, and the server-side.
implemented log4j for logging purpose to debug the application.
wrote code to facilitate the integration of applications with backend restful web services.
more than 7 years hadoop big data experience;
responsible for building quality for data transfer pipelines for data transformation using flume, spark, spark streaming, and hadoop.
expert in big data ecosystem using hadoop, spark, kafka with column-oriented big data systems such as cassandra and hbase.
technical skills summary
(elasticsearch, logstash, and kibana)
elasticsearch, lambda functions, toad,
scala, python, sql
spark, kafka
i worked on two different big data teams at capital one, clip and sre.
clip – credit limit increase program. this project involved moving the clip process from teradata on-premise infrastructure to aws using technologies like s3, lambda, redshift, and other aws technologies. throughout each day data is collected for thousands of potential capital one credit card customers who want their credit limit increased and for customers who qualify for automated credit increase. once a day a process is run to collect and run business rules against this data.  this process contacts several databases and data sources with millions of records and determines if a credit increase is approved or declined, and by how much.
demoed each dashboard to leadership team.
familiarized using sql server management studio and sql server 2014 to develop the business rules.
january 2013-
integrated web scrapers and pulled data from web and social media.
involved in benchmarking hadoop and spark cluster on a terasort application in aws.
used the image files of an instance to create instances containing hadoop installed and running.
data engineer
synergy software
evaluated needs and engineered a data analytics platform for marketing for this software company headquartered in great britain.
worked with cross-functional teams and project management and collaborated with british architect to help architect and engineer additions and improvement to facilitate a marketing initiative using hadoop analytics platform.
worked with phoenix, a sql layer on top of hbase to provide sql interface on top of no-sql database.
implemented the workflows using apache oozie framework to automate tasks
involved in setup and benchmark of hadoop /hbase clusters for internal use.
developed unix shell scripts to run the batch jobs.
september 2007
responsible for maintaining, supporting, and troubleshooting network and peripheral systems.
administrator of security system including issuing badges
provided end user production support
trained customers on how to use the knowlagent applications
managed test environment for reproducing customer issues
oracle support
created a design document from customer requirement document.
maintained bellsouth inventory items on oracle crm database.
responsible for training resources for setups team.
maintained matrix of what setup done on any one of the twelve environments.
identified the root cause of defect.
helped maintain code version control using harvest for multiple environments.
helped assemble tested application by running test scripts written by developers on team.
cca (cloudera certified associate) spark and hadoop developer exam (cca175)
master data engineer and developer: expert in cloud-based data services,
• 5 years of experience in the field of data analytics, data processing and
development of custom data pipelines for analytics use cases
server and ms sql server.
• proficient in extracting and generating analysis using business
redshift, rackspace cloud, intel nervana cloud, open stack, google computer
data reporting and visualization
mllib, graphx, scipy, pandas, rdds, dataframes, datasets, mesos, apache
github, bitbucket
• prepared etl design document which consists of the database structure,
platform (hadoop loader framework, big data spark framework)
structured, shipment, pos, consumer, household, individual digital
• imported the data from hdfs into spark rdd.
• created hive fact tables on top of raw data from different retailers
sept 2015   hadoop data engineer
sept 2016   wms – jersey city, nj
variety of industries.  this project involved the custom development of
requirement. involved in creating udfs in hive like - simple udf,
generate terabytes of data and store it in aws hdfs.
• deployed application jar files into aws instances.
• created instances which contain hadoop installed and running.
developed a task execution framework on ec2 instances using sql and
jan 2014    hadoop data engineer
cassandra api.
queries that will run internally.
• used scala to store streaming data to hdfs and to implement spark for
• involved in developing spark sql queries, data frames, import data
processing pipelines to enable pulling data from various sources and file
retrieve data from the cluster.
sqoop and various etl tools. extracted the data from rdbms (oracle,
data from social media websites.
circle.
web log analytics.
fastload utility.
• used various transformations like source qualifier, lookup, update
• generator and joiner on the extracted source data according to the
• done various optimization techniques in aggregator, lookup, joiner
• developed and implemented informatica parameter files to filter the
• performed scheduling techniques with etl jobs using scheduling tools,
environment: informatica powercenter 7.1.3, ncr's unix servers 5100, ibm pc
sean roussel   202-640-5960 | seanmichaelroussel@gmail.com
7 years – big data development
used the terraform command with variables to initialize a working directory containing terraform configuration files.
used visual studio as ide running on macbook.
created securelogonscript
worked with a team of developers analyzing various large data sets for trends and insider threat analysis using hadoop, machine learning, and predictive analytics.
integrated hadoop with active directory and enabled kerberos for authentication.
administered hadoop cluster(cdh) and reviewed log files of all daemons.
performed storage capacity management, performance tuning and benchmarking of clusters.
implemented workflows using apache oozie framework to automate tasks.
worked on importing and exporting data using sqoop between hdfs to rdbms.
designed and presented a poc on introducing impala in project architecture.
involved in creating hive tables, loading with data and writing hive queries
created partitions, buckets based on state to further process using bucket based hive joins.
used different file formats like text files, sequence files, avro.
documented requirements gathered from stakeholders.
collected and aggregated large amounts of log data using apache flume and staging data in hdfs for further analysis.
worked on the dost team (day in the life of the goes ground station).
april 2005	nov 2007
personal trust clearance with the doj for noaa
rahib amin
cloud platforms:
hive, pig, mapreduce, sql, spark sql, shell scripting
versioning:
tableau, kibana
performance tuning of spark jobs for setting batch interval time, level of parallelism, and memory tuning.
extracted the needed data from the server into hadoop file system (hdfs) and bulk loaded the cleaned data into hbase using spark.
participated in various phases of data processing (collecting, aggregating, moving from various sources) using apache spark.
ncr corporation, atlanta, ga	june 2016- september 2017
built the hive views on top of the source data tables, and built a secured provisioning framework for users to access the data through hive based views.
lean six sigma yellow beltolson josue dimanche
470-344-0391
qualifications
experience with hyperledger exposure in hadoop data projects.
highly knowledgeable in data concepts and technologies including aws pipelines, cloud repositories (amazon aws, mapr, cloudera).
apache hadoop, spark, storm, tez, flink
collecting the real-time data from kafka using spark streaming and perform transformations and aggregation on the fly to build the common learner data model and persists the data into hbase.
experience developing custom large-scale enterprise applications using spark for data processing.
cloud services & distributions
big data platforms, software, & tools
implemented counteroffer credit policy for upmarket card segment, estimated to approve up to 50% of customers who otherwise would be ineligible for a card product, increasing revenues by tens of millions of dollars.
•	created highly scalable, resilient, and performant architecture using amazon aws cloud technologies such as simple storage service (s3), elastic mapreduce (emr), elastic cloud compute (ec2), elastic container service (ecs), lambda, and elastic load balancing (elb).
•	built and maintained end-to-end ci/cd jenkins pipeline across multiple environments.
•	created infrastructure-as-code using hashicorp terraform to easily automate, deploy and evolve application infrastructure on demand.
•	upgraded application architecture from spark 2.2.0 to spark 2.4.0 to take advantage of new built-in spark-avro functionality.
•	maintained and updated database schemata for total systems (tsys) pci data warehouse.
march 2018- august 2018
led effort in refreshing outdated automated test suite, bringing failing test count from over 150 down to zero.
technologies: mapr, hadoop/hdfs, hive, hbase, spark, pig, java spring, java ee, java esb, rest web services, shell, python
coordinated cross-team projects and tasks with engineers and business personnel
supported team efforts through daily standup meetings and planning sessions, and weekly retrospective meetings.
met frequently with engineers and business personnel to make decisions on project architecture, business and technical requirements, and project scope.
trained analysts on how to migrate podium data workflows during upgrade from version 2 to 3.
may 2016- august 2017
involved in creating hive tables, loading with data and writing hive queries, which will invoke and run jobs in the backend.
implemented data ingestion and cluster handling in real time processing using kafka.
import and export of dataset transfer between traditional databases and hdfs using sqoop.
configured spark streaming to receive real time data from kafka and store to hdfs using scale.
imported data into hdfs and hive using sqoop and kafka. created kafka topics and distributed to different consumer applications.
technologies:  hadoop, spark, hdf, oozie, sqoop, mongodb, hive, pig, storm, kafka, sql, acro, rdd. sqs s3, cloud, mysql, informatica, dynamo db
alibaba – remote
used oozie scheduler system to automate the pipeline workflow and orchestrate the jobs that extract the data on a timely manner.
technologies:  hadoop cluster, hdfs, hive, pig, sqoop, linux, hbase, shell scripting, eclipse, oozie, navigator.
involved in building our data warehousing solutions for banking industry, pulling data from various sources and file formats.
email: williamthorndikeiii@gmail.com
effective in hdfs, yarn, pig, hive, impala, sqoop, hbase, cloudera.
experience in developing shell scripts, oozie scripts and python scripts.
knowledge in implementing advanced procedures like text analytics and processing using apache spark with python language.
apache cassandra
used aws redshift clusters to sync data from hoot and used aws rds to store the data for retrieval to dashboard.
implemented aws lambda functions to run scripts in response to events in amazon dynamo db table or s3 bucket or to http requests using amazon api gateway.
installed and configured tableau desktop to connect to the hortonworks hive framework (database) which contains the bandwidth data form the locomotive through the hortonworks odbc connector for further analytics of the data.
• 10+ years’ experience on major components in hadoop ecosystem like hadoop
• worked with apache spark which provides fast and general engine for large
• experience with cloud space: aws, azure, emr and s3.
mapreduce, python, scala, xml, blueprint xml, ajax, rest api, spark api,
databricks, hortonworks, elastic mapreduce
• wrote different udf's to convert the date format and to create hash
piggybanks and other sources.
scenarios.
• managing cluster using ambari.
may 2016    verisign – reston, va
specifications to write a project plan and architecture schematic.
transformation, and custom aggregation.
data scientists for further analysis.
jan 2014    hadoop distributed data engineer
instead of derby with partitioning, dynamic partitioning, and buckets.
• developed custom processors in java using maven to add the
• created external tables pointing to hbase to access table with a huge
number of columns.
• wrote python code using a happybase library of python to connect to
of scheduled jobs.
aws s3, maven, junit, mrunit.
• bulk loaded data into cassandra using stable loader
java team in creating mapreduce programs.
jul 2012    suntrust bank – atlanta, ga
worked on risk management applications and data processing applications for
• gathered requirements, designed and implemented the application that
• implemented j2ee design patterns like mvc and front controller.
involved in requirement analysis, design and provide the estimation.
• involved in writing pl/sql queries and stored procedures. responsible
may 2008    jr. java developer
in analysis, interface design and development of jsp.
• configured log4j to enable/disable logging in the application.
• developed rich user interface using html, jsp, ajax, jstl, javascript,
• implemented pl/sql queries, procedures to perform data base
• designed new database tables and modified existing database tables to
incorporate new statics and alarms in mysql database.
• involved in peer code review processes and inspections. implemented
technologies: java, spring, hibernate, jms, ejb, web logic server,
institute of university studies, mexico
electronics
ieee july 31, 2008
programmer-analyst and big data professional.  passionate about creating
at this point - unknown
antonio saldivar lezama
11 yrs information tech
ability to conceptualize innovative data models for complex products, and create design patterns.
hadoop and tez
spark batch processing
and i developed an ecs task and lambda function to move the file from the source to our s3buckets.
responsible for planning and developing two spark applications to process big data batches.
cross-functional collaboration with teams contributing to application development (i.e., kafka producer for kafka topics, web interface, mobile interface).
migrated expiring security tokens to new ones without impacting customers or other platforms.
worked with parallel tasks to provide high throughput and flink windows to hold data in memory and provide low latency response.
integrated the back-end with the front-end to create rule match dynamically.
flink-yarn session setting the jobmanager, taskmanager and slots resources and be able to start our job with parallelism.
windows azure cluster.
hdfs, zookeeper and yarn to work with the cluster.
flink table api
architected the big data architecture to create the foundation of this enterprise analytics initiative in a hadoop-based data lake.
worked on aws to create, manage ec2 instances, and hadoop clusters.
followed itil best practices for ensure data and infrastructure integrity.
followed six sigma for process efficiency and quality performance.
performance tuned spark jobs for setting batch interval time, level of parallelism, and memory tuning.
sql
august 2015 – october 2016
exported analyzed data to relational databases using sqoop for visualization, and to generate reports for the bi team.
bucketing
big data, hadoop, aws, hive, pig, scala, python, spark, sqoop, mapreduceresume:
used spark extensively to do data processing like collecting, aggregating, moving
hive to populate internal/external tables and stored the refined data in partitioned external tables w/ hive.
feb 2018 – present
 documented the requirements including the available code which should be implemented using spark, amazon dynamodb, redshift and elastic search.
 kibana for dashboards and reporting to provide visualization of log data and streaming data.
xpress global systems - chattanooga, tn
implemented different components on the cloud for the kafka application messaging
george tarpeh
hadoop platforms
vizualization tools
databases
toad database management toolset, podium, informatica, talend
savannah, ga
menards
designed a cost-effective archival platform for storing big data using  mapreduce jobs.
august 2011 – july 2012
managed company databases using sql to create queries, tables, views and procedures.
designed structure for data warehouse (dwh) tables (reference books and classifiers, fact tables) and aggregate tables (data marts).
build and execute analytics and reporting across platforms to identify user behavior and analyze trends, patterns, and shifts in user behavior, both independently and in collaboration with product managers and data analytics resources.
sqoop to import/export data from database to hdfs and data lake on aws.
installed, configured, and tested an aws lambda function workflow in python
managed and monitored aws ec2 instances through aws management console
hive queries for analyzing data in hive warehouse using hive query language.
apache ant, apache cassandra, apache flume, apache hadoop, apache hadoop yarn, apache hbase, apache hcatalog, apache hive, apache kafka, apache maven, apache oozie, apache pig, apache spark, spark streaming, spark mllib, graphx, scipy, pandas, rdds, dataframes, datasets, mesos, apache tez, apache zookeeper, cloudera impala, hdfs, hortonworks, mapr, mapreduce, apache airflow and camel, apache lucene, elasticsearch, elastic cloud, kibana, x-pack, apache solr, apache drill, presto, apache hue, sqoop, kibana, tableau, aws, cloud foundry, github, bit bucket
█   █   █   █   █   █   █  professional experience  █   █   █   █   █   █   █
thirdlove also used online tracking and the resulting big data to improve user experience and optimize the marketing expense.  using online tracking the company knows if a customer if ready to buy, is comparing prices; it knows where they came from. they know if the customer saw an ad on facebook or clicked on an email offer. they know if that customer has visited the site before, put something in their cart, and then backed out. it’s all available.
developed sqoop jobs to populate hive external tables using incremental loads.
and aggregation on the fly to build the common learner data model and persists the data into hbase.
integrated kafka with spark streaming for real time data processing.
november 2014 – may 2016
+ 15 years experience information technology  |  +5 years experience big data
experience processing multiple terabytes of data stored in aws s3.
██████████
apache flume, kerberos, ranger, ambari, yarn, cluster management, cluster security, zookeeper, oozie
applied broadcast variables in spark, effective & efficient joins, transformations.
queried hive tables and incremental imports with spark and spark jobs for data processing and analytics.
extracted the data from application servers into hadoop file system (hdfs) and bulk loaded the cleaned data into hbase using spark.
experience in working with flume to load the log data from multiple sources directly into hdfs on an aws emr cluster.
ambari to monitor workload, job performance and capacity planning.
lead data repair analyst
created applications that reduced database conversions from 45 minutes to 5 minutes, saving the company over 1000 man hours a year.
april 1992 – july 2000
worked with active directory managing military personnel and civilians.
in charge of a team of 15 marines tasked with deploying throughout the world to setup local area networks for government use.
american intercontinental university
american bureau of shipping abs – hadoop, spark, cassandra
hands-on experience on major components in hadoop echo systems like spark, hdfs, hive, pig, hbase, zookeeper, sqoop, oozie, flume, kafka.
proficient with sql server, sql server reporting services, sql server integration services, tsql, plsql
wells fargo san francisco, ca
imported the data from different sources like aws s3, lfs into spark rdd.
involved in hbase setup and storing data into hbase, which will be used for analysis.
develop spark jobs to parse the json or xml data.
implemented the workflows using apache oozie framework to automate tasks.
gns healthcare cambridge, ma
involved in hdfs maintenance and loading of structured and unstructured data.
medline industries northfield, il
involved in architectural design cluster infrastructure, resource mobilization, risk analysis and reporting.
cluster monitoring using big insights ionosphere tool.
6 years of experience engineering big data environments on premises and migration to cloud environments using amazon web services (aws) for hosting cloud-based data warehouses, and data based using redshift, cassandra, arangodb and rdbms sources.
design, development and system migration of high performant metadata-driven data pipeline with kafka and hive, providing data export capability through api and ui.
apache drill, apache kafka, apache maven, apache oozie, apache pig, apache hue, apache sqoop, apache flume, apache hadoop, apache hbase, apache hcatalog, apache ant, apache cassandra, apache lucene, apache solr, apache airflow, apache camel, apache mesos, apache tez, apache zookeeper
bi visualization
apis
agile, kanban, scrum, continuous integration, tdd, unit testing, functional testing, design thinking, lean, six sigma
travelers	hadoop admin engineer
design spark scala pot to consume information from s3 buckets
managed hive beeline connection with tables, databases and external tables
work on docker containers launching spark applications with scala
work on setting mysql enterprise to link with puppet enterprise
technologies:  spark + hortonworks + knox + ranger + hive + solr + ambari + mysql + linux access management (posix)
battle creek, mi	january 2017 – october 2018
we needed to eliminate waste and invest more in the trade spend that drives faster time to market and greater revenue. they also needed the ability to run multiple trade spend simulations simultaneously every day.  it was clear that kellogg needed to move away from its traditional on-premises infrastructure.
used amazon elastic compute cloud (amazon ec2) instances to process 16 tb of sales data weekly from promotions in the u.s., modeling dozens of data simulations a day.
provisioned over 20 node hadoop clusters on microsoft azure cloud infrastructure using cloudbreak service
configured over thirty node cluster using ambari 2.1.2/hdp 2.3.2 on at&t cloud services
developed oozie workflow for scheduling and orchestrating the etl process
netgear	hadoop admin & support
san jose, ca	january 2014 – june 2015
1 | pagevincent nwobodo  |  phone: 978-708-1080  |   email:  vincent.nwobodo888@gmail.com
analysis
hadoop system administrator
bachelor of science in information systems
experience working in hadoop-as-a-service (haas) environments with syncsort-dmx-h, subversion (svn), and sql and nosql databases
c++, visual basic,  javascript, r
loaded the transformed data into staging tables for data analysis with database functions.
technologies:  spark, spark streaming, spark sql, spark scala, databricks, aws, s3, sql, sumologic, etl, datamart, intellij idea, aws buckets, jenkins, circleci, sdlc, agile scrum, sql, jira, confluence
loaded data into spark rdd and do in memory data computation to generate the output response.
experience in analyzing data using hiveql 2.1.1 and custom mapreduce programs in java.
responsible for creating roadmaps for ongoing cluster deployment and growth.
function as expert consulting resources around hadoop integration points with any etl and bi teams.
import data from mysql to hdfs, using sqoop to load data.
cargill, minnetonka, minnesota
developed hive queries and udfs to analyze in hdfs and performed analyzing/transforming data with hive and pig.
environment: hadoop, hdfs, map reduce, hive pig, sqoop, oozie, hbase, linux, java, xml.christopher tran
experience-5yrs
experienced in cloud architecture using aws ecosystem and tools and native hadoop and distributions (cloudera hadoop and hortonworks hadoop)
hadoop data pipeline and aws pipeline tools used to process data with apache spark, spark streaming, spark, for predictive analytics on hadoop data projects.
data viziualization tools: tableau, kibana
feb 2016- june 2017
implemented hadoop streaming jobs to process terabytes of xml format data.
architected and implemented custom data analytic dashboards using kibana, allowing executive management to view past, current and forecast sales data.
created advanced analytical dashboards using reference lines, bands, and trend lines.
josh pierre
profile                                                                                                               +
technical skills summary                                                                    +
microsoft project, vmware, microsoft word, excel, outlook, power point; technical documentation skills
work experiences                                                                                +
identify and translate business requirements into data analysis and data acquisition requirements
implemented technologies to optimize query performance, such as spark-sql.
big data hadoop engineer
load the data into spark rdd and performed in-memory data computation to generate the output response.
developing parser and loader map reduce application to retrieve data from hdfs and store to hbase and hive.
automated all the jobs for extracting the data from different data sources like mysql to pushing the result set data to hadoop distributed file system.
(november 2013 – march 2015)
configured policies for all components.
have good understanding on frameworks (mr, spark, tez).
amir ibrahim
work experience with cloud infrastructure like amazon web services (aws).
perform data analysis on historical record using redshift spectrum
improved performance on tables joins
performance tuning for spark writing to s3 in parquet format.
monitored spark jobs using log4j to track spark job performance
archer daniels midland
eagan, mi
email: mainefelder222@gmail.com
5+ years experience in the hadoop  and big data ecosystems
5+ years of experience in information technology
experience in scripting with sql extracting large sets of data, and design of etl flows.
cloudera hadoop (cdh)
modeled data based on business requirements using pyspark
tested scripts of pyspark, shell and spark
wrote partitioned data into hive external tables using parquet format.
worked in an agile production environment and advised on spark best practices.
worked with a team of developers with specialty in rdbms, mainframe, unix scripting, java, sqoop, pyspark.
supports hadoop business data and analytics projects using spark and hive.
hands-on experience in hadoop distribution in hortonworks
duke energy | charlotte, north carolina	june 2015 – aug 2017
identified and ingested source data from different systems into hadoop hdfs using sqoop, flume and python, creating hive tables to store variable data formats.
used python pandas to import data from web service into hdfs and transformed data using spark rdds.
implemented test scripts to support test driven development and continuous integration.
developed pig scripts for analyzing large data sets in the hdfs.
written hive ql scripts for business needs.
saint peter’s univeristy, jersey city, njcollins enoh  813-882-5638   collinspenoh@gmail.com
supportive and helpful
wellcare
primary technical skills in hdfs, yarn, pig, hive, sqoop, hbase, flume, oozie, zookeeper.
experience managing data analytics projects over $32 million.
knowledge of pig and hive's analytical functions, extending hive and pig core functionality by writing custom
udfs.
strong experience in working with databases like teradata and proficiency in writing complex sql, pl/sql for creating tables, views, indexes, stored procedures and functions.
experience in handling xml files and related technologies.
strengths
database & data structures, apache cassandra, datastax cassandra, amazon redshift, dynamodb, apache hbase, hcatalog
march 2016 – present
migration of content from old data warehouse to aws redshift data ware house for columnar data storage.
tool for deploying the code.
the data to generate the desired output files.
created both internal and external tables in hive and developed pig scripts to preprocess the data for analysis.
built a full-service catalog system which has a full workflow using elasticsearch, logstash, kibana, kinesis, cloudwatch.
aliso viejo, ca
performance tuning in the live systems for etl/elt jobs.
jan 2011 – jan 2012
data systems specialist
built scalable, cost-effective solutions using cloud technologies using data lake, data warehouse and database; experience with cassandra, redshift, mongodb sql and more.
470-728-5232
frostburg, maryland
provided proof of concepts converting avro data into parquet format to improve query processing by using hive.
experience in working with flume to load the log data from multiple sources directly into hdfs.
ups
exported the analyzed data to the relational databases using sqoop for ingestion and tableau for data visualization to generate reports for the bi team.
worked with hive on tez, and various configuration options for improving query performance.
create columnar format in hive like parquet, orc for storing and for use with file compression tools such as gzip and snappy.
experience with hands on data analysis and performing under pressure.
job management using fair scheduler. involved in scheduling oozie workflow engine to run multiple hive, sqoop and pig jobs.
developed sqoop jobs to populate hive external tables using incremental loads
experience in automation testing, software development life cycle (sdlc) using the waterfall model and good understanding of agile methodology.
merck, inc.
implemented jms for asynchronous auditing purposes.
may 2012 – august 2012
6+   years hadoop architecture
jboss, weblogic, websphere, das, nas, san
hdfs, parquet, avro, json, snappy, gzip
mapreduce, hive, hive ql, sql, rdds, dataframes, datasets,
build tools
apache ant, apache maven
verified and validated that ability-to-pay lambda triggered jobs appropriately to execute the cluster and process the accounts.
thermo fisher scientific inc. waltham, ma
met with stakeholders, smes, and data scientists to gather, determine and document requirements.
ppg industries inc. pittsburgh, pa
optimized the data storage in hive using partitioning and bucketing on both the managed and external tables.
setup and config data lake on bigdata platforms like cloudera, hortonworks, and mapr.
proficient with building tools like apache ant and apache maven.
data engineer		january 2016 – march 2017
energy future holdings corporation	dallas, tx
analyzed and documented results in a comprehensive written report and delivered conclusions.
worked with senior software developers to analyze and streamline processes, identify inconsistencies, and execute on-time practical solutions.
university of houston; houston, tx
“hadoop, cloudera, hortonworks, aws”
java, spark, python, scala
eclipse, intellij, pycharm, visual studio, atom
apache spark, spark streaming
rensselaer polytechnic institute
bachelor of computer systems engineering
capable of building big data tools to optimize utilization of data, and configure end-to-end systems.
able to perform cluster and system performance tuning on big data systems.
developed the bunch contents to bring the information from aws s3 and do required changes in spark emr.
created aws lambda function for extracting the data from kinesis firehose and post the data to aws s3 bucket on scheduled basis (every 4 hours) using aws cloud watch.
hadoop metadata management by extracting and maintaining metadata from hive tables and hdfs.
enabled security to the cluster using kerberos and integrated clusters with ldap at enterprise level.kislay jha
big data engineer skilled in architecting and implementing big data analytics platforms using data lakes, data warehouse, sql and nosql databases with batch and steam data.
kislayjha000@gmail.com
setting up modern data centers empowered by mesos, kubernetes, docker swarm to implement container orchestration and run microservices in big data ecosystem
experience with data visualization tools such as zeppelin notebooks, kibana, grafana, tableau, informatica, talend, kettle
experience with searching tools like elk stack (elasticsearch, logstash and kibana), apache solr integration with cassandra for search capabilities.
amazon/aws, google computer cloud, azure cloud
viacom
extensively worked on performance optimization of all project by optimizing hive queries by using map-side join, parallel execution and cost-based optimization.
implementation of hadoop data lake on amazon s3, with hadoop etl processing using amazon aws cloud services for batch processing and real-time processing.
technologies: big data, hadoop, spark, spark streaming, spark rdd, kafka, aws, data pipeline, kerberos, sqoop, zookeeper, flume, hive, hdfs, etl processes, cassandra, redshift, arangodb
05/2015-11/2016
performance optimized the iot data pipelines using hive partitioning and bucketing.
technologies: big data, hadoop, aws, iot, data pipeline, kafka, sqoop, zookeeper, flume, spark, spark streaming, spark rdd, hive, hdfs partitioning, bucketing, data cleansing, data transformation, real-time streaming, batch processing, data ingestion, etl processes
02/2014-05/2015
utilized cloud dataflow cloud-based data processing service for both batch and real-time data streaming applications.
useed bigquery restful web service for interactive analysis of massively large datasets working in conjunction with google storage.
06/2012-02/14
aggregator, merge, join, lookup, sort, remove duplicate, funnel, filter, pivot, shared container for developing jobs.
worked with linux systems and rdbms database on a regular basis in order to ingest data using sqoop.
defense research & development
bachelor of engineering in nanotechnology
disciplines
email: chingizkhalifazada0816@gmail.com
bachelor’s degree in mathematics
university of british columbia – vancouver, bc, canada
cirrus logic, austin, tx	february 2016-august 2017
worked with amazon web services (aws), and cloud services such as emr, ec2, s3, ebs and iam entities, roles, and users.
collected metrics for hadoop clusters using ambari & cloudera manager.
assisted in upgrading, configuration and maintenance of various hadoop infrastructures like pig, hive, and hbase.
xml, blueprint xml, ajax, rest api, json,  pytorch, tensorflow, tableau, qlik view, pentaho
successfully loaded files to hdfs from teradata and loaded from hdfs to hive.
data analyst		may 2012 – september 2013
hadoop/big data training
dedicated and seasoned big data professional with skill in implementing and improving big data ecosystems using hadoop, spark, microsoft azure, amazon aws, cloudera, hortonworks, mapr, anaconda, jupyter notebooks, and elastic search.  proficient in etl and data pipeline methods and tools.
apache cassandra, apache hbase, mapr-db, mongodb, oracle, sql server, db2, sybase, rdbms, mapreduce, hdfs, parquet, avro, json, snappy, gzip, das, nas, san, postgresql, mysql, memsql, oltp.
credit batch processing
tests are programmed using cucumber for acceptance test-driven-development (atdd) quality process.
deployed manually with terraform commands and used jenkins for automated deployment.
technologies:  aws ec2, emr, terraform, scala, hadoop, spark sql, splunk, pagerduty, jira, drools, cucumber
analyzed hadoop cluster using big data analytic tools including kafka, pig, hive, spark, mapreduce.
aug 2012	hadoop data engineer
sep 2011	systems administrator
power server performance through programs encoded in ile-rpg and stored procedures in
may 2011	devant it, mexico
with visualization
• knowledge in installing, configuring, and using hadoop ecosystem
components like hadoop map reduce, hdfs, hbase, oozie, hive,
• strong knowledge of pig and hive's analytical functions, extending
relational database systems/ non- relational database systems and vice-
bucketing concepts in hive to compute data metrics.
software solutions using java and j2ee technologies with developing
apache cassandra, apache hbase, mapr-db, mongodb, oracle, sql server,
kibana, tableau, sqoop, apache drill, presto, apache flume, apache
data points with low latency and high throughput.  the hadoop hdfs is
• migrated complex mapreduce programs into apache spark rdd operations.
• implemented data ingestion and cluster handling in real-time
• performed both major and minor upgrades to the existing cloudera
the development of and integration with data processing systems.
scripts, hive queries, and process for executing them etc.
• experience in using and tuning relational databases (e.g. microsoft
portfolios
• involved in installation of cdh4 hadoop, configuration of the cluster
• worked closely with the data modelers to model the new incoming data
sets and developed and maintained hive ql, pig latin scripts, scala
• imported data from local file system, rdbms into hdfs and sqoop and
illustrative reports and dashboards along for ad-hoc analysis
tools like jira agile and git stash
customers
• gained good experience in amazon web services (ec2, s3, emr) and
environment: cdh5, hadoop, hdfs, mapreduce, yarn, hive, oozie, sqoop,
oracle, linux, shell scripting, java, spark, scala, sbt, storm, kafka,
for these clients.
• worked on poc and implementation & integration of cloudera &
• build and maintained a bill forecasting product that will help in
• created etl jobs to load twitter json data into mongodb and jobs to
• data is loaded back to the teradata for the basel reporting and for
distributions (hortonworks & cloudera).
avro, parquet, etl, hortonworks, datameer, teradata, sql server, ibm
the project involved the "claims" module that handles adjudication,
hive, hbase on linux environment. assisted with performance tuning and
• worked on creating pig scripts for most modules to give a comparison
• created reports for the bi team using sqoop to export data into hdfs
with live visualization
tables
salary, department info, etc. the system allows management to keep
• object-oriented design using uml and ibm's rational rose used in
implementing uml.
environment: java/j2ee (jsp, servlet), eclipse, struts, hibernate,
• restructured the ms access database by normalizing tables resulting in
human errors.
the quantity, the order and on which machine to inject.
environment: java, servlets, spring, jsp, javascript, html, php, css,
12 years of experience in development of bi solutions including design and development of olap cubes 9olap, molap, holap, rolap), star schema, snowflake schema, scorecard, balanced scorecard, and analytics reporting dashboards.
use of .net and c programming to customize reports and data pipelines transformation and cleansing.
worked with development teams to build database design and implementation.
windows 2000/server 2003/server 2008/xp/vista/7, linux suse
cloud storage & processing
sr. business intelligence developer                                                 mar 2018 - present
preventice solutions– houston, tx
worked without direct supervision 95% of time attending requests from users directly and coordinating priorities with vp’s and directors.
performed bi analyst and developer functions
optimized existing ssrs reports
experience on publishing power bi reports on dashboards in power bi server.
created power bi reports for uccx cisco.
technologies:  msbi stack, power bi, proprietary systems
design and create demand management datamart.
develop/deploy best practices and procedures, including: utilizing visual studio for development, utilizing source control for code/script management, and following change control and code review processes
sr. business intelligence specialist                                             dec 2012 - sep 2014
designed, developed and implemented an insurance model, creating ssis etl process for dimensions update, fact table calculations, to improve sales based on trend analysis.
develop and implement new and existing database solutions
mentored my team on principles, best practices and analysis of olap cubes and dimensions.
maintain and improve current bi solutions
designed and implemented an etl process based on 80 excel files located in 30 different locations and for medical insurance analysis using ssis.
created multiple reports using ssrs report suites for the world financial portal.
developed multiple reports with a dashboard look and feel for the medical insurance analysis using ssrs.
provided maintenance of corporate reports using ssrs report suites for gruma headquarters and its different branches.
designed and implemented roambi mobile dashboards.
tecnologico de monterrey- monterrey, mexico
developed and provided maintenance for multiple reports using ssrs reports suite for the school hypercube model for desktop and mobile users.
designed a real time olap cube using ssas for the school accounting model.
designed etl packages dealing with different data sources (sql server, flat files) and loaded the data
asad khan
5 years of big data engineering and information technology experience in hadoop environment both on-prem and cloud.  expertise in hadoop components and ecosystem, and migrating legacy technologies to aws cloud.
documented the requirements including the available code  implemented using spark, amazon dynamodb, redshift and elastic search.
apache kafka, apache maven, apache oozie, apache pig, apache sqoop, apache flume, apache hadoop, apache hbase, apache cassandra, apache lucene, apache solr, apache airflow, apache camel, apache mesos, apache tez, apache zookeeper
involved in creating frameworks which utilized a large number of spark and hadoop applications running in series to create one cohesive e2e big data pipeline.
implemented aws lambda functions to run scripts in response to events in amazon dynamo db table or s3.
worked on analyzing hadoop cluster and different big data analytic tools including hive and spark, and managed hadoop log files.
used oozie and workflow scheduler to manage hadoop jobs by direct acyclic graph (dag) of actions with control flows.
holder construction, atlanta, ga	sept 2013-dec 2014
background
• ability to troubleshoot and tune sql, python, scala, pig, hive, rdds,
• deep knowledge in incremental imports, partitioning and bucketing
• deployed large multiple nodes of hadoop and spark clusters.
• scheduling workflows and etl processes with apache oozie.
|experience                          |programming                         |
|10 years of experience in the field |python, scala, php • python • bash •|
|5 years of experience with the      |basic, vba, .net, spark, hiveql,    |
|hadoop ecosystem and big data tools |spark api, rest api                 |
|                                    |tableau, microsoft power bi         |
|autocad • matlab • revit • ltspice •|apache hive, apache kafka, apache   |
hadoop data engineer   jan 2016 - present
innovative architects – atlanta, ga
utilization o f big data.  now, data analysis and machine learning are
and maintenacne of water systems and more.
risks, threat assessment and strategy.
integrated hadoop with active directory and enabled kerberos for
interval time, correct level of parallelism, selection of correct
for one of the use case, used spark streaming with kafka & hdfs & mongodb
spark streaming.
before hdfs.
implemented data ingestion and cluster handling in real time processing
performed storage capacity management, performance tuning and benchmarking
increasing performance benefit and helping in organizing data in a logical
scheduled and executed workflows in oozie to run hive and pig jobs.
developed various data connections from data sourced to ssis, and tableau
configured spark streaming to receive real time data from kafka and store
spark rdds, python and scala.
environment.
generate reports.
datastax connector.
used oozie to automate/schedule business workflows which invoke sqoop, and
pig jobs as per the requirements.
creating hive external tables to store the pig script output. working on
them for data analysis in order to meet the business requirements.
involved in loading the created files into hbase for faster access of all
used sqoop to efficiently transfer data between databases and hdfs and used
involved in production support, which involved monitoring server and error
used zookeeper and oozie for coordinating the cluster and scheduling
primary project was a program to manage inventory of goods from raw
preparation and execution of unit test cases. reviewing and committing the
involved in structuring wiki and forums for product documentation
used stored procedures to crunch data and creating view for fetching data
database management.    my expertise focuses on design and development of
cloudera, databricks, hortonworks
file system (hdfs) using apache spark & spark streaming.
(cloudera distribution hadoop) (cdh) and reviewed log files of all
• coding sql statements for back end communication using jdbc.
(hdfs) using sqoop.
• configured spark streaming to receive real time data from kafka and
qualmetrix  miramar, fl
• worked with spark context, spark sql, dataframe and pair rdds in
hadoop data processing.
generate reports based on hadoop.
• imported data into hadoop distributed file system (hdfs and hive using
consumer applications.
worked with humana’s data analytics team in a project that focused on
spark, kafka, hdf, oozie, sqoop, mongodb, hive, pig, storm, kafka, hadoop,
• extracted rdbms data (oracle, mysql) to hadoop distributed file system
• integrated kafka with spark streaming for real time data processing in
the hadoop system.
• developed data pipelines to process the data and create necessary
files.
• imported data using sqoop to load data from mysql and oracle to hadoop
• worked on installing cluster, commissioning, and decommissioning of
• involved in gathering requirements and responsible for documenting
the hr data warehouse. used maven, sql, oracle, xml.
• involved in analyzing system failures, identifying root causes and
bi developer     may 2008 - aug 2010
blue cross blue shield of north carolina     charlotte, nc
changes were proposed or implemented in source systems.
• consulted with other teams to help identify solutions to solve
• worked with teams to translate business requirements into a data
model.
• expanded the content of the hr data warehouse based on business needs
data warehouse and hub common format files.
• write hadoop streaming applications with spark streaming and kafka.
← participated in design, development and system migration of high
← use of python for big data pipelines, and customizations.
spark, spark streaming, hadoop, and vertica.
← experience developing custom large-scale enterprise applications
← excellent knowledge on hadoop architecture and ecosystems such as
hdfs, configuration of nodes, yarn, sentry, spark, falcon, hbase,
datasets, spark
on-premmicrosoft visio
smartdraw
installed zeppelin on an application node in the cluster.
installed mysql on a cluster.
may 2016 hadoop data engineer
enabled sentry and kerberos to ensure data protection
may 2015 ovation travel– new york, ny
involved in converting hiveql/sql queries into spark transformations using spark rdds, python, and scala.
installed, configured, and monitored hadoop clusters using cloudera
fashioned a new sandbox cluster to test upgrades and beta services
wrote and executed sql queries (subqueries and join conditions, correlated subqueries).
worked on ssis configuration and troubleshooting
i managed database servers, and ran queries to compile data on utilization and billing metrics derived from sensors and from accounts.
experience in using performance monitor/profiler to resolve dead locks/long running queries.
substantial experience with pig, flume, tez, zookeeper and hive and storm.
experienced in application development using hadoop, rdbms and linux shell scripting and performance tuning.
sql server integration services (ssis)
sharepoint technologies
designed, developed and implemented cutting-edge big data analytics application to support the royalties business function.  this project involved migrating legacy applications to new technologies, as well as building fault tolerant, highly available, high performing applications that provide a seamless experience to our end-users.
building spark applications on scala and java with docker images to perform benchmarks on spark with no hadoop on a single node.
accounts, roles and policies management on aws iam (identity access management) for development and business team members.
involved in installing aws emr framework.
citizens financial
imported and exported data from different relational data sources like db2, sql server, teradata to hdfs
involved in developing spark sql queries, data frames, import data from data sources, perform
developed pig udf's for manipulating the data according to business requirements and also worked on
migrated etl jobs to pig scripts to do transformations, even joins and some pre-aggregations before storing the data onto hdfs.
worked on different file formats like sequence files, xml files.
los angeles, ca
developed internal and external tables, good exposure on hive ddls to create, alter and drop tables.
experience in using zookeeper and oozie for coordinating the cluster and scheduling workflows.
managed hadoop log files.
written custom udf's in pig and hive according to business requirements.
on time completion of tasks and the project per quality goals.
softtek
reviewing and approving the test cases and scenarios so that the testing team can validate the changes against the requirements listed.
gis project coordinator, jan 2009 – june 2010
research and development of geographical applications to facilitate applications to facilitate strategic decision-making
developed .net batch applications which were used for processing a high-volume of data.
developed custom .net web applications using mvc 4, entity framework 4.0 and linq
-education-
leon institute of technology, mexico
experience in importing and exporting data using sqoop from hdfs to relational database systems/ non- relational database systems and vice-versa.
hadoop ecoystem components & tools
together with our global network of experts, we are a strategic partner to siemens’ operative units and provide important services along the entire value chain – from research and development to production and quality assurance, as well as optimized business processes. our support provided to the businesses in their research and development activities is ideally balanced with our own future-oriented research.ject description
hortonworks
big data enterprise architect
participated in project planning for webmd’s business needs and technical challenges of the project.
worked with and mentored two junior developers.
proficient in etl and data pipeline methods and tools.
flume, and storing in hdfs for further analysis.
including but not limited to hdfs architecture, hive, pig, sqoop, hbase,
• involved in building a multi-tenant cluster.
• experience in mainframe data and batch migration to hadoop.
and camel, apache lucene, elasticsearch, elastic cloud, kibana, x-pack,
apache solr, apache drill, presto, apache hue, sqoop, kibana, tableau, aws,
consulted on project to get real-time insights about customer experience,
what is driving customer experience, and the impact of collaborative offers
•     designed and presented a poc on introducing impala in project
•     configured spark streaming to receive real time data from kafka and
•     used spark sql and dataframes api to load structured and semi
forecasting.  analysis of date supported inventory, logistics and
•     loading data from diff servers to aws s3 bucket and setting
•     analyzed hadoop cluster using big data analytic tools including
•     involved in converting hive/sql queries into spark transformations
•     worked with clients to better understand their reporting and dash
•     wrote complex hive queries, spark sql queries and udfs.
•     cassandra data modeling for storing and transformation in spark using
•     installed and configured various components of the hadoop ecosystem.
•     implemented partitioning, dynamic partitions and buckets in hive for
pocs and research to try to design an innovative big data system for
dec 2013    pioneeer natural resources – irving, tx
processes used to statistically value pioneer's undeveloped potential
sep 2010    linux system administrator
• daily assistance with users using microsoft office suites 2003 and
phone: 404-220-9581 email:. paulbordonez@gmail.com
experience in configuring zookeeper to provide cluster coordination services.
provided guidelines and helped in setting up development, staging and production environments for sharepoint
experience in job management using fair scheduler and developed job processing scripts using oozie workflow.
performed advanced procedures like text analytics and processing, using the in-memory computing capabilities of spark.
experienced in handling large datasets using partitions, spark in memory capabilities, broadcasts in spark.
responsible for developing data pipeline with amazon aws to extract the data from weblogs and store in hdfs, and cassandra implementation using datastax.
data analysis through pig, hive.
led the design and development of a sql server ssas analysis cube utilizing star schema with complex mdx calculated measures, named sets and kpis to present an analytical view for the data and data quality with multiple dimensions.
created lists & libraries and implemented claims based security for all the existing users.
implemented policies on the lists and libraries to enable expiration.
created custom sharepoint hosted apps using javascript and jquery within the promise pattern
created the system release plan.
lead scrum meetings with developers and managed tasks within an agile framework
involved in all phases of the sdlc and deployment and maintenance of the web application.
used ms sql.net data providers and ado.net to connect to and work with sql database servers.
zacapoaxtlan superior technological institute,
advanced ad-hoc courses
installation and configuration of the various big data ecosystem tools such as elastic search, logstash, kibana, kafka and cassandra.
implemented spark streaming for real-time data processing with kafka and handled large amounts of data with spark.
itt exelis
kansas state university
seasoned i.t. professional with experience in all phases of i.t./data
flume, oozie, zookeeper.
• import/export terabytes of data between hdfs and rdbms using sqoop.
optimization.
|apache                              |big data                            |
|scripting                           |mongodb, oracle, sql server         |
|hive, spark                         |hdfs, snappy, gzip, das, nas, san   |
|agile, kanban, scrum, devops,       |cloud                               |
|continuous integration, test-driven |web development software            |
georgia power uses smart meters to eliminate the need to send
mar 2016    bank of america – charlotte, nc
multiple-migration approach.
implementation for an investment risk platform (irp).
constructed, maintained, and tested electrical systems and components. used
systems and components meet established specifications. employed knowledge
rca of all problem records for the mceits environment.
handoff of the issue to the next shift.
• conducted data analysis and security review of solutions to migrate
determined appropriate end state architecture (elimination,
significantly diminished due to the introduction of legacy
applications/legacy systems.
• provided security design and security measures when related to
• performed vulnerability scans and assessments.
military
multiplexes, information systems operations and information systems
air national guard 3e3x1 camp pendleton, virginia beach
operating system, and microsoft exchange. it also includes x.400 messages
computational linguistics/natural language processing (nlp), machine
approaches, e.g. neural networks, svms, probabilistic graphical models;
clustering, p/r, auroc
qlik view, pentaho
databricks, hortonworks hadoop, or hadoop environments.
sas, others microsoft cain & abel, microsoft, microsoft baseline security
the environment, and the rural economy.
• created indexes for various statistical parameters on elasticsearch
and generated visualization using kibana.
• wrote sql queries for hadoop data validation of tableau reports and
• worked on importing and exporting data (etl) using sqoop between
directing planning based on insights.
• involved in the process of designing hadoop architecture including
with spark streaming & kafka.
increasing hadoop system performance.
to manipulate the data from hadoop system.
• used the aws-cli to suspend an aws lambda function and automate
• imported data into hadoop distributed file system (hdfs) and hive
queries for hadoop data system.
|              |hadoop data engineer                                       |
•  used nosql databases like mongodb in implementation and integration.
•  integrated kafka with spark streaming for real time data processing in
processing. in hadoop
strategy.  this necessitates compilation of data from longitudinal studies
the hadoop distributed file system (hdfs) data.
intense computational power and lots of storage is required to perform
• wrote shell scripts to monitor health check of apache tomcat and jbos;
• computed trillions of credit value calculations per day on a cost-
• worked on client-side data validation.
extensively used apache flume to collect logs and error messages across the
proficient in major vendor hadoop distribution like cloudera, hortonworks,
incremental imports, partitioning and bucketing concepts in hive and spark
hadoop distributed file system (hdfs) using flume; staging data in hadoop
and big data platforms and analytics.
sns & other services.
• extensively trained on big data processing and hadoop development.
• knowledge in building the framework model architect from scratch or
develop brd's (business
• extensively used rdbms like oracle and sql server for developing
risks, issues, schedules, and deliverables.
sources directly into hdfs.
|programming                         |data repositories                   |
|ides: jupyter notebooks, eclipse,   |server, rdbms, hdfs, data lake, data|
|virtualization, and back-end        |parquet, orc, ajax                  |
|architecture                        |cloud data                          |
|team lead, requirement gathering,   |elasticsearch, solr, lucene,        |
|communication with stakeholders and |cloudera, databricks, hortonworks   |
|cross-functional team, mentoring    |                                    |
|developers, problem-solving,        |                                    |
|apache ant, apache cassandra, apache flume, apache hadoop, apache hadoop  |
|yarn, apache hbase, apache hcatalog, apache hive, apache kafka, apache    |
|tez, apache zookeeper, cloudera impala, hdfs, hortonworks, apache airflow |
|and camel, apache lucene, elasticsearch, elastic cloud, kibana, x-pack,   |
present     delta airlines – atlanta, ga
involved in the project to migrate data from data warehouses and sql
• part of a critical model, design, for decision making and maintain
business need.
which bi analysts could use for quick queries and tableau reports.
• wrote shell scripts to execute scripts (pig, hive), and move the data
• handled 20 tb of data volume with 120-node cluster in a production
• developed, tested, and implemented the financial-services application
implemented, set-up and worked on various hadoop distributions (cloudera, hortonworks, amazon aws, and microsoft azure hdinsight and related tools and environments).
vast understanding of different development methodologies including waterfall, rapid application development, agile, scrum and kanban.
worked with team to develop a plan for a new cloud environment based on needs and use, gathering and documenting requirements.
for easy migration and management of various data types and use cases, established databases on the hdinsight cloud azure cosmos db for distributed storage and arangodb to provide multi-modal storage replacing both sql and nosql data stores.
created service to ingest structured data from oracle, db2, sybase, data in delimited files like csv, tsv, fixed width, pdf files, and to catalog the ingested data using azure data factory.
developed system ability to infer schema for delimited files, csv, tsv, psv, fixed width and provide relational or query structure on ingested data.
managed project and tasks using microsoft team foundation server (tfs).
used azure data factory to create new data pipelines for extracting, transforming and loading data from repositories such as azure data lakes and azure data warehouse.
excellent management communication and leadership skills working with direct report and cross-functional teams stakeholders and business units.
implemented knox, ranger, spark and smartsence in hadoop cluster.
experience in gzip data compression data to avro data into hdfs files.
succeeded in deploying elasticsearch, influx db in a docker container.
installed ansible by installing elasticsearch nodes in multiple environments with automated scripts.
worked on nagios monitoring tool.
successfully generated consumer group lags from kafka using api.
participated on a migration effort from sharepoint 2007 to sharepoint 2010 using detach-attach content database approach.
built whole department sites using out of the box elements in sharepoint such as web parts and workflows.
created a content type hub in a sharepoint 2010 site collection and start publishing reusable content types.
set up and configure document sets for new contracts and attach custom approval workflow to the library.
customized the treeview control to reduce the number of post-backs and increase performance of the application by handling the major events on the client by intercepting the server-side events on the client's browser.
hive, mapreduce, zookeeper, yarn
bloomin' brands
" i was working on a project that allows the company to track data about their sourced and transported goods. we also used kinesis and aws emr to do sentiment analysis on social media and visualize it using aws elk.”
developed oozie workflow for scheduling and orchestrating the etl process within the hortonworks hadoop system.
hadoop fundamentals ibm
educationprofessional profile
distributions:  hortonworks, cloudera
database:  sql, mysql, oracle, cassandra, hbase
jenkins, hudson, travis, continuum
git, svn, perforce, clearcase, rtc
prepared project charter comparing scope of the project.
created the system requirement documents, to detail the functional requirements, and changes expected from various technical teams.
imported data from web service into hdfs and transformed data using pig.
reviewed and approved the test cases, and scenarios so that the testing team can validate the changes against the requirements listed.
tools:  microsoft powerbi, microsoft visio, microsoft powerpoint, microsoft excel, microsoft word
requirement gathering for data warehouse
worked extensively with advance analysis actions, calculations, parameters, background images, and maps in tableau desktop.
analysis, design, coding, performance tuning, and implementation of new bi solutions using power bi and tableau.
built and maintained power bi dashboards, and used power query, power pivot, power q & a, and power map, with various transformations using dax language.
developed complex list reports, cross tab reports, chart reports, and drill through reports with advanced multidimensional reporting.
environment:  tableau, power bi, gantt, power map, hadoop,  agile, scrum
created numerous processes through ms visual studio (ssis, ssas, and ssrs) and ms sql server 2012 to develop a data warehouse project.
contributed to build business dashboards to conduct trend analysis on retail metrics, especially relating to driving sales revenue, customer retention and sales person productivity.
create use case documents based on research of similar products or scenarios.
migrated a solution from hive to snowflake for high-volume data analytics in a centralized data model that emphasizes cloud scalability
worked with bi analysts to define aggregate tables, migrate logic from hive to snowflake and implement new logic for visualization
developed tables & etl jobs in matillion to replace heroku connect for salesforce data moving into data warehouse
i was involved in updating allstate’s data analysis system. i was responsible for analysis, design, and development of the new data etl solution which integrates kafka, kstreams, java, hdfs, avro, json, and landoop lenses. the new system delivers a high-throughput of data and transformation & serves as a pluggable solution for allstate’s internal developers.  i developed a framework using kstreams & java for event driven solutions in allstate applications and other data sources in various file formats.
updated kafka connector with kerberos password authentication.
created plan to use kafka-connect on speed-testing.
complex kstreams data processing using windows & streams dsl.
github used for managing the project lifecycle
technologies:  hadoop, kafka, kstreams, java, intellij, python
nlp ai application for medical
chemical pricing strategy
data on consumption, sales, and unit costs helps companies also evaluate the inventory of available products. by analyzing profitability, market forecasting, and raw material availability, companies can make better decisions about the products and grades they offer. this information can help companies unearth new markets for existing or new products, too.
expert in migrating streaming or static rdbms data into hadoop cluster from dynamically-generated files using flume and sqoop.
http://www.argylejournal.com/chief-information-officer/chief-it-architect-at-texas-instruments-talks-about-tis-big-data-journey/
january 2013 - january 2014
amazon redshift, amazon aurora, amazon rds, dynamodb, apache cassandra, apache hbase, mapr-db, mongodb, oracle, sql server, db2, sybase, rdbms, ms access, postgresql, mysql, nosql
84.51° cincinnati, oh
the purpose of this project involved the design and maintenance of a scheduling and automation tool to enable the orchestration of etl and data science at scale within the cloud.
the problem was lack of functionality within the enterprise cloud platform to run automated jobs.  we needed to enable the data scientists to run schedulable and repeatable jobs on a massive scale both in terms of population and compute, in a google cloud/microsoft azure environment.  to solve the problem, i created a custom solution by way of a web application using apache airflow.
the agreed upon tech-stack for the automation tool consisted of: python flask web application, sqlalchemy db, postgresql db, apache airflow, uwsgi server, nginx server, flask application, dockers, kubernetes, helm chart, jfrog artifact repository, teamcity ci/cd.
used teamcity ci/cd and jfrog artifact repository to create artifacts as docker containers for the application, postgres instance, and airflow teamcity will build and run the test suite to make sure the application has and passes the proper amount of test coverage.
customized a fork of apache airflow to meet all system and user requirements.
iot, big data and analytics platform development
configured apache zeppelin binaries/conf for spark web clients; integrated zeppelin daemon with spark master node.
developed pipelines to fetch data from sql, nosql and hybrid databases on the cloud.
by using python libraries numpy, scipy, scikit-learn, pandas analyzed large datasets and developed graphs.
aggressive monitoring of partitioning versus topic production via jmx interface(s); developed kafka standalone poc's with the confluent schema registry, rest proxy, kafka connectors for redshift and hdfs(hadoop)
perdue farms, salisbury, md
worked on a hortonworks hadoop cloud platform developing pipelines and performance tuning.
moved some data from google cloud platform (gcp) to aws s3.
transformations using mapreduce, hive to load data into hdfs.
elk, iam, cloud formation
the purpose of the project was to migrate all data from one data lake to another and replicate or improve the environment used to view and interact with said data. this would be done by automating ingestion from whichever data sources were used in the original data lake. solved multiple problems arose across several pipelines, ranging from permissions errors to data inconsistencies, but these problems were resolved with careful analysis and communication between clients, vendors, and persons of contact.
prepared sql scripts to load ingested data in apache hive and amazon redshift using aws glue as an external metastore.
automated resulting scripts using aws data pipeline and shell scripting to ensure daily execution in production.
monitored past projects for outages or other issues using amazon sns notifications.
met with project lead and other team members to ascertain the best method of solving challenges.
troubleshooted finished pipelines as needed if sns messages signal failure.
utilized a fail-fast method for non-critical issues in development to quickly try proposed solutions.
clients - financial services, telecommunications, banking to name few.
source tools
spark, hive, udfs, and hbase
✓ involved in coding, reviewing the codes, unit testing, functional
✓ scheduled the jobs using crontab.
✓ designed and developed reporting tables in redshift.
✓ involved in running the scalding jobs on clouodera hadoop (cdh
✓ worked with cdh5.1.3, yarn, spark 1.0, sparksql, postgres 9.3, hive
✓ involved in loading data from linux file system to hdfs.
✓ proficiency with modern natural language processing and general
✓ use of shared preferences.
✓ good knowledge on agile methodology and the scrum process.
nodes, cluster monitoring
qa analyst, automation
specifications document.
✓ executed test cases and procedures on different application build
tracking, defect triaging and defect closure.
[pic][pic][pic]
cloud technologies
agile processes, problem solution skills, mentoring, requirement gathering
sql, hiveql, impala. pig
cerner, kansas city, mo
secured applications with kerberos and used kerberos for authentication with a ethereum blockchain cryptography.
configured collection realtime data from embedded systems using apache storm.
batch processing and realtime processing of iot data
worked with deep knowledge in incremental imports, partitioning and bucketing concepts in hive needed for optimization.
•	imported data using sqoop to load data from mysql and oracle to hdfs on regular basis.
developed disaster recovery plans for entire operation.
worked with management to determine infrastructure security strategy.
pro-actively identified potential security issues and implemented preventive measures.
successfully migrated corporate colo resources to microsoft's azure could solution. designed azure environment utilizing cisco asav, vnets, security groups, and 3rd party virtual appliances and software to meet virtual infrastructure requirements.
set-up and configured the following servers: windows nt 4.0 migrated to 2000 then exchange 5.5 migration to exchange 2003 server, windows 2000/2003 terminal (citrix).
setup and configured iis; web sense; in house norton anti-virus 8.1 corporate edition server; checkpoint firewall; sonic wall hardware firewall; vpn technology between multiple sonic walls; and cisco concentrator (vpn3000).
certified in information technology infrastructure library 2011: itil v3
