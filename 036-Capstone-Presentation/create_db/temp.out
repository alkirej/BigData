[info] Loading project definition from /media/sf_jeff/git/BigData/036-Capstone-Presentation/create_db/project
[info] Loading settings for project create_db from build.sbt ...
[info] Set current project to create_db (in build file:/media/sf_jeff/git/BigData/036-Capstone-Presentation/create_db/)
[info] Compiling 1 Scala source to /media/sf_jeff/git/BigData/036-Capstone-Presentation/create_db/target/scala-2.11/classes ...
[info] Done compiling.
[info] running net.alkire.task36.createDb.CreateDb 
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
20/02/29 10:07:29 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
20/02/29 10:07:29 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
20/02/29 10:07:30 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
20/02/29 10:07:30 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
20/02/29 10:07:31 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
20/02/29 10:07:31 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
20/02/29 10:07:31 INFO Query: Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
20/02/29 10:07:31 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
+------------+
|databaseName|
+------------+
|    capstone|
|      covers|
|     default|
+------------+

+------------+
|databaseName|
+------------+
|    capstone|
|      covers|
|     default|
+------------+

++
||
++
++

org.apache.hadoop.fs.HarFileSystem
org.apache.spark.network.protocol.MessageEncoder
org.apache.spark.storage.DefaultTopologyMapper
io.netty.channel.ChannelInitializer
org.apache.hadoop.security.SecurityUtil
io.netty.channel.socket.nio.NioServerSocketChannel
hive.metastore
org.apache.hadoop.hdfs.server.namenode.NameNode
org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection
io.netty.channel.AbstractChannel
org.apache.spark.network.server.OneForOneStreamManager
org.spark_project.jetty.server.handler.gzip.GzipHttpOutputInterceptor
org.apache.hadoop.hive.ql.exec.FunctionRegistry
org.apache.hadoop.security.Groups
org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator
org.apache.hadoop.net.unix.DomainSocketWatcher
org.apache.hadoop.ipc.Server
io.netty.buffer.ByteBufUtil
io.netty.util.ResourceLeakDetector
org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory
/stages
org.apache.hadoop.util.VersionInfo
org.apache.spark.ContextCleaner
org.apache.hadoop.hive.conf.SystemVariables
io.netty.bootstrap.ServerBootstrap
org.apache.hadoop.util.NativeCodeLoader
org.apache.hadoop.hdfs.HAUtil
org.apache.hadoop.hive.metastore.ObjectStore
org.apache.hadoop.security.Credentials
/SQL
org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint
org.spark_project.jetty.server.handler.ContextHandler
org.apache.spark.SecurityManager
org.spark_project.jetty.util.component.AbstractLifeCycle
org.apache.hadoop.conf.Configuration.deprecation
io.netty.util.NetUtil
org.apache.hadoop.hive.metastore.MetaStoreInit
BlockStateChange
org.apache.spark.executor.Executor
org.apache.hadoop.hive.ql.session.SessionState
org
org.apache.spark.network.client.TransportClientFactory
org.spark_project.jetty.server.handler.AbstractHandler
org.apache.spark.sql.hive.client.HiveClientImpl
com.jolbox.bonecp.PreparedStatementHandle
org.apache.hadoop.ipc.ProtobufRpcEngine
org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef
org.apache.spark.storage.DiskBlockManager
org.spark_project.jetty.http.MimeTypes
org.apache.spark.util.Utils
/SQL/execution/json
org.apache.spark.ui.JettyUtils
io.netty.channel.AbstractChannelHandlerContext
org.apache.hadoop.conf.Configuration
io.netty.util.internal.logging.InternalLoggerFactory
org.apache.spark.storage.BlockManager
org.apache.hive.common.util.HiveVersionInfo
org.apache.spark.sql.internal.SharedState
com.jolbox.bonecp.CustomThreadFactory
DataNucleus.Datastore.Schema
com.jolbox.bonecp.PoolWatchThread
org.spark_project.jetty.util.resource.Resource
org.apache.hadoop.hdfs.DFSUtil
io.netty.channel.ChannelOutboundBuffer
org.apache.hadoop.metrics2.lib.Interns
org.apache.spark.network.netty.NettyBlockTransferService
/api
org.apache.spark.ui.SparkUI
/stages/stage/kill
org.apache.hadoop.hdfs.NameNodeProxies
org.apache.spark.storage.BlockManagerMaster
org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection
hive.log
DataNucleus.Transaction
org.apache.hadoop.security.token.Token
/jobs/json
org.apache.hadoop.hive.conf.HiveConf
org.apache.hadoop.hive.metastore.AggregateStatsCache
org.apache.hadoop.hdfs.DFSClient
org.apache.hadoop.metrics2.lib.MutableMetricsFactory
org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedPartitioner
/stages/pool
org.spark_project.jetty.util.resource.URLResource
com.jolbox.bonecp.StatementHandle
org.spark_project.jetty.util.resource.JarResource
org.apache.hadoop.ipc.Client
org.spark_project.jetty.util.log
org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$2
org.spark_project.jetty.servlet.ServletHolder
/executors/threadDump/json
org.spark_project.jetty.servlet.Holder
org.apache.spark.sql.hive.HiveUtils
/stages/json
org.apache.hadoop.hive.metastore.HiveAlterHandler
org.apache.spark.network.protocol.MessageDecoder
org.apache.hadoop.util.PerformanceAdvisory
org.spark_project.jetty.server.AbstractConnector
org.spark_project.jetty.server.handler.AllowSymLinkAliasChecker
org.apache.hadoop.hive.ql.QueryPlan
org.apache.spark.HeartbeatReceiver
/jobs
io.netty.channel.nio.NioEventLoop
/stages/stage
org.apache.hadoop.metrics2.impl.MetricsSystemImpl
org.apache.spark.SparkContext
org.apache.hadoop.security.ShellBasedUnixGroupsMapping
/jobs/job/kill
/environment/json
org.apache.spark.network.TransportContext
SecurityLogger.org.apache.hadoop.ipc.Server
hive.metastore.hivemetastoressimpl
/stages/stage/json
io.netty.buffer.PooledByteBufAllocator
org.apache.hadoop.net.NetUtils
io.netty.channel.MultithreadEventLoopGroup
org.apache.hadoop.hdfs.PeerCache
org.apache.hadoop.security.UserGroupInformation
org.apache.hadoop.security.authentication.util.KerberosName
com.jolbox.bonecp.BoneCPConfig
org.apache.spark.sql.hive.client.IsolatedClientLoader
DataNucleus.Cache
org.apache.spark.sql.execution.SparkSqlParser
org.spark_project.jetty.util.thread.strategy.ExecutingExecutionStrategy
org.spark_project.jetty.util.DecoratedObjectFactory
org.apache.hadoop.hive.metastore.RetryingMetaStoreClient
org.apache.hadoop.hive.ql.exec.Task
io.netty.util.concurrent.AbstractEventExecutor
io.netty.util.internal.CleanerJava6
org.spark_project.jetty.server.handler.gzip.GzipHandler
org.spark_project.jetty.util.DeprecationWarning
org.apache.hadoop.hive.metastore.HiveMetaStore
org.spark_project.jetty.http.HttpGenerator
org.spark_project.jetty.util.thread.QueuedThreadPool
org.spark_project.jetty.servlet.ServletContextHandler
org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences
org.apache.spark.network.util.JavaUtils
com.jolbox.bonecp.ConnectionTesterThread
DataNucleus.MetaData
org.spark_project.jetty
org.spark_project.jetty.server.handler.ErrorHandler
org.apache.hadoop.hive.metastore.MetaStoreDirectSql
mapreduce.Counters
/storage/rdd
DependencyResolver
org.apache.hadoop.hdfs.web.WebHdfsFileSystem
org.apache.hadoop.hive.ql.optimizer.ppr.PartitionExpressionForMetastore
hive.metastore.warehouse
hive.ql.metadata.Hive
org.apache.spark.sql.catalyst.expressions.codegen.package$ExpressionCanonicalizer
org.spark_project.jetty.io.SelectorManager
DataNucleus.Datastore.Native
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil
DataNucleus.Lifecycle
/jobs/job/json
org.apache.hadoop.mapred.JobConf
org.apache.hadoop.hdfs.StateChange
/static
/storage
io.netty.util.concurrent.DefaultPromise
org.apache.spark.storage.BlockManagerMasterEndpoint
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache
DataNucleus.Datastore.Retrieve
/jobs/job
org.apache.hadoop.io.retry.RetryPolicies
org.spark_project.jetty.io.ManagedSelector
org.apache.hadoop.metrics2.lib.MetricsSourceBuilder
io.netty.util.internal.SystemPropertyUtil
org.apache.hadoop.hive.metastore.HiveMetaStore.audit
/static/sql
org.spark_project.jetty.servlet.ServletHandler
org.apache.hadoop.fs.FileSystem
org.apache.spark.network.server.TransportServer
org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume
/storage/rdd/json
org.apache.hadoop.util.ShutdownHookManager
org.spark_project.jetty.http.PreEncodedHttpField
org.apache.hadoop.fs.permission.FsPermission
/metrics/json
/storage/json
org.apache.hadoop.fs.ftp.FTPFileSystem
org.spark_project.jetty.util.Jetty
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient
org.spark_project.jetty.util.thread.ExecutionStrategy$Factory
io.netty.util.concurrent.GlobalEventExecutor
org.apache.hadoop.hdfs.util.ByteArrayManager
org.apache.spark.MapOutputTrackerMasterEndpoint
org.spark_project.jetty.util.component.ContainerLifeCycle
com.jolbox.bonecp.ConnectionHandle
org.spark_project.jetty.servlet.BaseHolder
org.apache.hadoop.hdfs.protocol.CachePoolInfo
DataNucleus.Persistence
org.apache.hadoop.ipc.RPC
DataNucleus.Query
DataNucleus.ValueGeneration
org.apache.hadoop.util.Shell
org.apache.hadoop.fs.FileUtil
org.spark_project.jetty.servlet.DefaultServlet
io.netty.util.internal.PlatformDependent0
org.apache.hadoop.hdfs.BlockReaderLocal
org.spark_project.jetty.http.HttpFields
com.jolbox.bonecp.ConnectionPartition
org.apache.spark.SparkEnv
com.jolbox.bonecp.BoneCP
org.apache.spark.storage.memory.MemoryStore
com.jolbox.bonecp.BoneCPDataSource
org.spark_project.jetty.server.Server
DataNucleus.General
/executors/json
/executors
org.apache.spark.repl.Main
io.netty.util.internal.MacAddressUtil
org.apache.hadoop.hdfs.ClientContext
parquet.CorruptStatistics
org.apache.hadoop.hive.metastore.RetryingHMSHandler
org.apache.hadoop.hive.metastore.Deadline
/stages/pool/json
org.apache.hadoop.io.retry.RetryInvocationHandler
org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1
DataNucleus.Datastore
org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback
io.netty.util.internal.PlatformDependent
io.netty.util.internal.InternalThreadLocalMap
org.apache.hadoop.hive.common.JavaUtils
akka
org.apache.hive.common.HiveCompat
org.apache.spark.util.ShutdownHookManager
/environment
org.apache.spark.repl.SparkILoop$SparkILoopInterpreter
org.spark_project.jetty.util.resource.JarFileResource
/
org.apache.hadoop.io.nativeio.NativeIO
io.netty.channel.DefaultChannelPipeline
/executors/threadDump
org.apache.hadoop.io.retry.RetryUtils
/SQL/execution
org.spark_project.jetty.util.StringUtil
DataNucleus.Datastore.Persist
DataNucleus.JDO
org.spark_project.jetty.server.handler.ContextHandlerCollection
io.netty.util.concurrent.SingleThreadEventExecutor
DataNucleus.Connection
org.apache.spark.repl.SparkIMain$exprTyper
org.apache.parquet.CorruptStatistics
org.apache.spark.network.server.RpcHandler$OneWayRpcCallback
/SQL/json
io.netty.channel.nio.AbstractNioChannel
io.netty.util.concurrent.DefaultPromise.rejectedExecution
io.netty.channel.DefaultChannelId
org.apache.hadoop.hive.shims.HadoopShimsSecure
2020-02-29 10:07:32,537 shutdown-hooks-run-all ERROR No Log4j 2 configuration file found. Using default configuration (logging only errors to the console), or user programmatically provided configurations. Set system property 'log4j2.debug' to show Log4j 2 internal initialization logging. See https://logging.apache.org/log4j/2.x/manual/configuration.html for instructions on how to configure Log4j 2
